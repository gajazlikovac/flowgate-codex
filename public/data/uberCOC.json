{
  "step_0": {
    "description": "2025 Guide Section 3: Data Centre Utilisation, Management and Planning (Practices 3.1.1, and 3.2.1 through 3.2.10)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_DC_Mgmt_Planning"
    ],
    "critical_questions": [],
    "questions": {
      "question_1": {
        "id": 1,
        "old_question_text": null,
        "questionText": "Has the organization established an approval board or equivalent cross-functional group, including representatives from Senior Management, IT, M&E Engineering, Applications/Software, and Procurement, which must approve any significant data centre decision to ensure impacts are understood and effective solutions are reached (e.g., defining standard IT hardware lists considering M&E implications)?",
        "simplifiedQuestion": "Do you have a cross-departmental group approving significant data centre decisions?",
        "reference": "2025 CoC Guide 3.1.1",
        "priority": "High (Value: 5, Expected: Entire Data Centre)",
        "stepSection": "step_0",
        "recommendationTitle": "Group Involvement",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx"
          ],
          "description": "Terms of Reference for the approval board, meeting minutes showing decision-making processes, examples of approved decisions."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx",
        "documentDescriptor": "Mandatory: Documentation of the approval board (e.g., charter, Terms of Reference) and records of its activities (e.g., meeting minutes, decision logs) demonstrating cross-functional involvement in significant data centre decisions.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of board documentation and decision records. Interviews with board members and stakeholders involved in recent significant decisions.",
        "nonconformity": {
          "major": "No cross-functional approval board or equivalent process for significant decisions.",
          "minor": "Approval board exists but lacks representation from key disciplines or its involvement in decisions is inconsistent."
        },
        "majorNonconformity": "Absence of a formal, functioning cross-functional group for approving significant data centre decisions.",
        "minorNonconformity": "The approval group is not consistently involved, lacks key representatives, or its decision-making process is poorly documented.",
        "whatThisMeans": "Your organization needs a formal group with members from all key departments (IT, Facilities, Procurement, Management, etc.) to review and approve major data centre changes. This ensures everyone understands the impact of decisions and helps find the best solutions.",
        "complianceRequires": "- Establishment of an approval board with representatives from Senior Management, IT, M&E Engineering, Applications/Software, and Procurement.\n- Requirement for group approval for any significant decision.\n- Ensuring impacts of decisions are properly understood by all parties.\n- Functional equivalence to a change advisory board (CAB) but with a broader scope covering strategic DC decisions.",
        "expertTip": "Ensure the approval group's remit clearly defines 'significant decision' to avoid ambiguity. Document the process for bringing items to this group and how decisions are recorded and communicated. Evidence of this group actively influencing decisions for overall efficiency is key.",
        "recommendedEvidence": "- Terms of Reference or Charter for the approval group.\n- List of members and their disciplines.\n- Meeting minutes showing diverse representation and decisions made.\n- Examples of decisions influenced by this group (e.g., hardware standardization, new technology adoption).",
        "recommendedActions": "- Formally establish the cross-functional approval group.\n- Define its scope, responsibilities, and decision-making process.\n- Integrate this group's approval into relevant change management and project management workflows.\n- Ensure senior management sponsorship and participation.",
        "sampleResponseTemplate": "Our organization has established a [Group Name, e.g., Data Centre Steering Committee] as per our [Policy Document Name]. This group includes representatives from [List Disciplines] and is responsible for approving significant decisions such as [Example Decision Types]. Its last meeting was on [Date] where [Example Topic] was discussed.",
        "humanInputAnswer": "Yes, we have a Data Centre Strategy Board (DCSB) established in 2023, with documented Terms of Reference. It includes VPs from IT Operations, Facilities Management, Global Procurement, a representative from the CIO's office, and Application Development leadership. All capital expenditures over \u20ac50,000 or changes impacting data centre resilience or capacity require DCSB approval. For example, the recent decision to adopt a new liquid cooling technology for our HPC cluster was approved by the DCSB after reviewing technical, financial, and operational impact assessments from all represented disciplines.",
        "llmGeneratedAnswer": "Yes, the organization has formally constituted a 'Data Centre Governance Council' (DCGC) effective Q1 2025, as outlined in the 'DCGC Charter & Operating Procedures' document. This council comprises senior representatives from IT Operations, Mechanical & Electrical Engineering, Software Development, Corporate Procurement, and a delegate from the Executive Management team. The DCGC's mandate requires its formal approval for all strategic data centre initiatives, including but not limited to adoption of new infrastructure technologies, changes to standard hardware lists, and significant capacity expansion projects. Decisions are logged, and impact assessments from each discipline are prerequisites for submission to the council.",
        "badExampleAnswer": "IT and facilities managers discuss important changes. We don't have a formal board for this."
      },
      "question_2": {
        "id": 2,
        "old_question_text": null,
        "questionText": "Has the organization carried out an audit of existing equipment to maximize any unused existing capability, ensuring that all areas of optimization, consolidation, and aggregation are identified prior to new material investment, particularly for IT equipment where impact severity is related to refresh frequency? Is it understood that this consideration of embodied environmental impact is key before new purchases?",
        "simplifiedQuestion": "Do you audit and optimize existing equipment for unused capacity and embodied impact before buying new hardware?",
        "reference": "2025 CoC Guide 3.2.1",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_0",
        "recommendationTitle": "Consider the embodied environmental impact of installed devices",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "xlsx",
            "docx"
          ],
          "description": "Audit reports of existing equipment utilization, process for identifying optimization/consolidation opportunities before new investment, considerations of embodied energy/environmental impact in procurement decisions."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, xlsx, docx",
        "documentDescriptor": "Mandatory: Documentation showing audits of existing equipment capacity and utilization, process for prioritizing optimization/consolidation (considering embodied impact) before new purchases, especially for IT equipment.",
        "verification_method": [
          "Document Review",
          "Process Verification"
        ],
        "verificationMethods": "Review of equipment audit reports, utilization data, procurement decision processes, and evidence of considering embodied environmental impact.",
        "nonconformity": {
          "major": "No process to audit/optimize existing equipment or consider embodied impact before new investment; frequent unnecessary purchases.",
          "minor": "Ad-hoc audits or optimization; new investments made without consistently evaluating existing asset utilization or embodied impact."
        },
        "majorNonconformity": "Systematic failure to assess and optimize existing equipment capabilities or consider their embodied environmental impact before new material investments, particularly for IT equipment.",
        "minorNonconformity": "Inconsistent or incomplete audits of existing equipment; embodied impact is not a significant factor in procurement decisions, or optimization opportunities are sometimes missed before new purchases.",
        "whatThisMeans": "Before buying new equipment (especially servers, storage, or network gear), your organization needs to thoroughly check its existing assets. The aim is to see if any unused capacity can be leveraged, or if current equipment can be optimized or consolidated to meet new demands. This includes considering the 'embodied environmental impact' \u2013 the resources and energy used to manufacture and transport new devices. Avoiding unnecessary purchases reduces your overall environmental footprint.",
        "complianceRequires": "- Implement a process to audit existing equipment (IT, M&E) for unused capacity, optimization potential, and existing embodied environmental impact.\n- Prioritize optimization, consolidation, and aggregation of existing assets before new material investment.\n- Focus particularly on IT equipment due to its refresh frequency and significant embodied energy.\n- Document this pre-investment assessment process, including how embodied impact is considered.",
        "expertTip": "Incorporate a 'reuse, optimize, then buy' hierarchy into your procurement workflow. Before approving a purchase request, require documentation demonstrating that existing assets have been evaluated and that the new purchase represents the most sustainable option after considering the full lifecycle, including embodied energy of new vs. extending life of old.",
        "recommendedEvidence": "- Standard procedure for auditing existing equipment and considering embodied impact before new purchases.\n- Examples of recent equipment audits showing utilization analysis and embodied energy considerations.\n- Decision records where new purchases were deferred, modified, or selected based on lifecycle environmental impact, including embodied energy.\n- Asset management reports indicating utilization levels and age of equipment.",
        "recommendedActions": "- Develop and implement a formal pre-procurement review process that includes auditing existing assets and evaluating the embodied environmental impact of potential new purchases.\n- Train procurement and technical teams on assessing embodied energy and lifecycle impacts.\n- Utilize DCIM or asset management tools to track utilization, age, and embodied energy data (where available) for existing assets.\n- Set targets for extending the life of existing assets and utilizing existing capacity before new investment.",
        "sampleResponseTemplate": "Our organization's 'Sustainable Procurement Policy' ([Policy Name]) mandates an 'Existing Asset & Embodied Impact Review' before new IT equipment investment. This involves [describe steps]. In QX [Year], this led to [specific example of avoiding/modifying purchase due to these considerations].",
        "humanInputAnswer": "Our 'IT Asset Lifecycle Management Procedure (IT-ALMP-002)' requires a mandatory 'Existing Capability Audit' before any new server procurement. This audit assesses current server utilization and explores consolidation opportunities to maximize use of existing hardware and defer new purchases, thus avoiding the embodied carbon of new equipment. For Q1 2025, this audit identified capacity to absorb 15% of new project requirements on existing virtualized infrastructure, deferring a planned hardware purchase.",
        "llmGeneratedAnswer": "Yes, prior to any new material investment, particularly for IT equipment, our organization conducts a 'Pre-Acquisition Resource Optimization Audit' as per 'Procurement Policy PP-ENV-003'. This audit, documented for each major procurement cycle (e.g., 'ServerRefresh_Q12025_OptimizationAudit.pdf'), identifies unused existing capabilities through detailed utilization analysis and explores all avenues for optimization, consolidation, or aggregation. This process explicitly considers the embodied environmental impact of new devices versus extending the life or improving the utilization of existing assets. For example, the Q1 2025 server refresh plan was reduced by 10% based on identified consolidation opportunities.",
        "badExampleAnswer": "We buy new servers when we need more capacity or when the old ones are out of warranty. We don't really audit for unused capacity specifically."
      },
      "question_3": {
        "id": 3,
        "old_question_text": null,
        "questionText": "Does the organization select and deploy mechanical and electrical (M&E) equipment that does not itself require additional cooling beyond standard office room cooling in normal operation, with the stated exception of some UPS batteries which may need lower temperatures to preserve performance, reliability, and operational lifetime?",
        "simplifiedQuestion": "Is your M&E equipment selected to avoid needing its own special cooling (apart from some UPS batteries)?",
        "reference": "2025 CoC Guide 3.2.2",
        "priority": "Medium (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_0",
        "recommendationTitle": "Mechanical and electrical equipment environmental operating ranges",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "xlsx",
            "docx"
          ],
          "description": "Procurement policies for M&E equipment, specifications of installed M&E equipment, design documents for M&E plant rooms, and specific documentation for UPS battery environmental requirements if applicable."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, xlsx, docx",
        "documentDescriptor": "Mandatory: M&E equipment selection policies/criteria, equipment datasheets confirming operating temperature ranges, and documentation for any specific cooling provided to UPS batteries.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of M&E equipment specifications and design documents. Inspection of M&E plant rooms.",
        "nonconformity": {
          "major": "Significant M&E equipment (excluding UPS batteries) is deployed that requires cooling beyond office ambient conditions, increasing overall cooling load.",
          "minor": "Some non-critical M&E equipment requires cooling, or UPS battery cooling is not managed according to manufacturer's recommendations."
        },
        "majorNonconformity": "Deployment of M&E equipment that unnecessarily requires dedicated cooling (beyond office ambient), contributing significantly to the data centre's cooling load.",
        "minorNonconformity": "Minor M&E components require non-standard cooling, or documentation for UPS battery environmental conditions is unclear or not followed.",
        "whatThisMeans": "Your supporting mechanical and electrical equipment (like switchgear, transformers, most UPS components) should be chosen so it can operate in normal room temperatures without needing its own dedicated data centre-grade cooling. This reduces overall energy use. Some UPS batteries are an exception and might need cooler conditions as specified by the manufacturer.",
        "complianceRequires": "- Select mechanical and electrical (M&E) equipment designed to operate in standard office room temperatures (or non-specially cooled plant rooms) under normal conditions.\n- Avoid M&E equipment that adds to the specialized cooling load of the data centre.\n- Document and adhere to specific temperature requirements for UPS batteries if they need to be kept cooler to preserve performance, reliability, and lifespan.\n- Ensure M&E equipment choices do not negatively impact overall data centre energy efficiency.",
        "expertTip": "When designing new facilities or undertaking major retrofits, physically locate M&E equipment that does not require precise environmental control outside of the main data halls or specially cooled areas. This segregation simplifies cooling design and reduces energy waste.",
        "recommendedEvidence": "- Procurement policy or standard for M&E equipment specifying environmental operating range requirements.\n- Datasheets/specifications for installed M&E equipment showing their ambient operating ranges.\n- Design documents for plant rooms housing M&E equipment.\n- If UPS batteries require specific cooling, provide manufacturer recommendations and evidence of adherence.",
        "recommendedActions": "- Update procurement policies to specify M&E equipment capable of operating without dedicated cooling (except for specified UPS batteries).\n- During design/retrofit, plan for M&E equipment placement in areas not requiring precision cooling.\n- Regularly review M&E plant for any equipment that might be unnecessarily drawing on cooling resources.\n- Ensure UPS battery environments are maintained as per manufacturer guidelines for longevity and performance, separate from general IT cooling if necessary.",
        "sampleResponseTemplate": "Our M&E equipment, including switchgear and transformers, is specified to operate within [Temperature Range, e.g., 0-40\u00b0C] without dedicated cooling, as per our [Procurement Policy Document Name]. UPS units [are/are not] in specially conditioned spaces. For our UPS batteries, we adhere to manufacturer guidelines by maintaining them at [Temperature/Condition] as documented in [Battery Maintenance Plan/Specification Sheet].",
        "humanInputAnswer": "Our standard for new M&E equipment, as per 'M&E Procurement Standard DC-ME-004', requires an operational range of 5-35\u00b0C without direct precision cooling. All switchgear and main distribution frames are located in non-environmentally controlled electrical rooms. Our modular UPS systems are designed to operate at these ambient plant room temperatures. However, our VRLA battery strings for the primary UPS are housed in a dedicated battery room maintained at 22\u00b0C \u00b12\u00b0C, as recommended by the battery manufacturer (see 'Battery System Specification XYZ-123') to maximize service life.",
        "llmGeneratedAnswer": "Yes, the organization's 'M&E Equipment Selection Policy (MEP-002)' mandates that all newly deployed mechanical and electrical infrastructure must operate efficiently within a standard ambient temperature range of 15\u00b0C to 30\u00b0C without requiring supplemental cooling. This applies to equipment such as switchgear, transformers, and the power components of UPS systems. For UPS batteries (specifically our lithium-ion type), we follow the manufacturer's environmental guidelines which specify a narrower operational window for optimal lifespan; these are located in a segregated area with independent temperature control, documented in 'UPS Battery Room Environmental Protocol BATT-001'.",
        "badExampleAnswer": "Most of our electrical gear is in the data hall with the servers so it stays cool. We haven't had any issues with it."
      },
      "question_4": {
        "id": 4,
        "old_question_text": null,
        "questionText": "Has the organization, if a Colocation or Managed Service provider, implemented charging models and tariffs that actively encourage the adoption of best practices and improve energy efficiency among its customers? For Enterprise operators, does the organization ensure that the true and full costs of providing data centre services are thoroughly understood, accurately calculated, and properly reported internally to relevant business units or stakeholders?",
        "simplifiedQuestion": "Do your data centre service charging or internal cost models promote energy efficiency?",
        "reference": "2025 CoC Guide 3.2.3",
        "priority": "Medium (Value: 3, Expected: Optional Practice)",
        "stepSection": "step_0",
        "recommendationTitle": "Service Charging Models",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Documentation of service charging models, tariffs (for providers) or internal data centre service cost allocation and reporting methodologies (for enterprises)."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory: For Colocation/MSP, provide charging models/tariff sheets showing efficiency incentives. For Enterprise operators, provide documentation of internal DC service cost understanding and reporting mechanisms.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of charging models, tariff structures, internal cost accounting for DC services, and interviews with finance/management.",
        "nonconformity": {
          "major": "Charging models actively discourage energy efficiency or complete lack of internal cost transparency for DC services.",
          "minor": "Charging models are neutral to energy efficiency, or internal cost understanding is superficial/incomplete."
        },
        "majorNonconformity": "Absence of charging models that incentivize energy efficiency (providers) or no system for understanding/reporting true DC service costs (enterprise).",
        "minorNonconformity": "Existing charging models have no discernible impact on customer efficiency behavior; internal cost reporting is ad-hoc or incomplete.",
        "whatThisMeans": "If you provide data centre services to others, your pricing should encourage customers to be energy efficient. If you operate an enterprise data centre, your organization needs to accurately track and report the full costs of these services to business units, so they understand the financial impact of their IT demands and are motivated to use resources efficiently.",
        "complianceRequires": "- Colocation/Managed Service Providers: Implement charging models and tariffs that encourage the use of best practices and improve energy efficiency.\n- Enterprise Operators: Ensure the true cost of data centre services (including energy, infrastructure, etc.) is fully understood and properly reported to internal consumers.\n- Transparency in how energy costs are factored into charges or allocations.\n- Mechanisms to show customers/users the financial benefits of their efficiency efforts.",
        "expertTip": "For providers, consider tiered pricing based on customer PUE, power density, or adoption of specific best practices. For enterprise operators, allocating actual energy costs (not just averaged overheads) to business units based on their IT consumption can create powerful incentives for demand-side efficiency.",
        "recommendedEvidence": "- Sample service contracts or tariff sheets detailing efficiency incentives (for providers).\n- Internal chargeback/showback reports for data centre services (for enterprises).\n- Methodology for calculating 'true cost' of DC services, including energy components.\n- Communication materials explaining efficiency-linked charging or cost reporting to customers/users.",
        "recommendedActions": "- Providers: Develop or revise charging models to include clear incentives for customer energy efficiency.\n- Enterprises: Implement a robust methodology for tracking and allocating the full costs of DC services to internal users.\n- Both: Ensure transparency in how energy consumption impacts costs or charges.\n- Both: Regularly review and update models to reflect current energy costs and efficiency opportunities.",
        "sampleResponseTemplate": "As a [Provider/Enterprise Operator], our organization addresses service charging and cost transparency by [describe your model, e.g., 'implementing tariffs that reward lower PUE racks' or 'allocating actual power and cooling costs to business units based on their IT load']. This is documented in [Document Name] and encourages efficiency by [explain mechanism].",
        "humanInputAnswer": "As a colocation provider, our 'Green Tier Tariff', introduced in Q2 2024 and detailed in our customer portal, offers a 5% rebate on power charges for clients who maintain an average rack power density within 70-80% of their contracted capacity and can demonstrate IT equipment inlet temperatures consistently above 24\u00b0C. This incentivizes efficient use of space and power, and supports our higher ambient temperature operations.",
        "llmGeneratedAnswer": "Yes, our enterprise IT department has implemented a 'Resource Consumption Showback Model' (documented in FIN-IT-007). This model details the fully burdened cost of data centre resources, including amortized capital for space, power, cooling infrastructure, and the direct energy costs associated with IT load. Monthly reports are provided to each business unit, illustrating their consumption and associated costs, with specific metrics on energy use per virtual machine and per application. This transparency has encouraged business units to actively participate in server consolidation and application optimization initiatives.",
        "badExampleAnswer": "We bill our clients a flat fee per rack plus metered power. Internally, IT costs are just part of the overall corporate budget."
      },
      "question_5": {
        "id": 5,
        "old_question_text": null,
        "questionText": "Has the organization introduced a plan for Life Cycle Assessment (LCA) in accordance with EU guidelines and internationally standardised methodologies (e.g., ISO 14040, ISO 14044, EN 15978 'Sustainability of construction works - assessment of environmental performance of buildings calculation method'), aiming to reduce the overall carbon footprint and improve sustainability, particularly for new builds or retrofits?",
        "simplifiedQuestion": "Do you have a plan for conducting Life Cycle Assessments (LCAs) for your data centre, especially for new projects?",
        "reference": "2025 CoC Guide 3.2.4",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_0",
        "recommendationTitle": "Life Cycle Assessment",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx"
          ],
          "description": "Documented plan for Life Cycle Assessment (LCA) aligned with specified standards. If LCAs have been conducted, provide summary reports or findings."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds/retrofits: A plan for LCA. Optional but good: Examples of LCAs conducted and how findings influenced decisions.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of the LCA plan, its alignment with standards (ISO 14040/14044, EN 15978), and any completed LCA reports. Interview relevant staff regarding LCA implementation.",
        "nonconformity": {
          "major": "No LCA plan for new builds/major retrofits despite the expectation.",
          "minor": "LCA plan exists but is not aligned with recognized standards or is not consistently applied to relevant projects."
        },
        "majorNonconformity": "Absence of a documented plan to conduct Life Cycle Assessments for new data centre constructions or significant retrofits.",
        "minorNonconformity": "An LCA plan exists but lacks alignment with ISO 14040/14044 or EN 15978, or its application is inconsistent for new projects.",
        "whatThisMeans": "For new data centres or major upgrades, your organization should plan to assess the total environmental impact over their entire lifecycle \u2013 from sourcing materials for construction, through operational energy use, to decommissioning and disposal of equipment. This 'Life Cycle Assessment' helps in making more sustainable choices beyond just operational energy efficiency.",
        "complianceRequires": "- Develop and document a plan for conducting Life Cycle Assessments (LCA) for new builds or significant retrofits.\n- Align the LCA methodology with EU guidelines and international standards like ISO 14040, ISO 14044, and EN 15978.\n- The LCA should aim to understand and help reduce the overall carbon footprint and improve the sustainability of the data centre.\n- Acknowledge that LCA focuses on broader environmental performance, complementary to direct energy efficiency measures (refer to CLC TS 50600-5-1 for more sustainability practices).",
        "expertTip": "When starting with LCA, focus on the phases and components with the highest potential environmental impact for a data centre, such as the embodied carbon in building materials (concrete, steel), major M&E equipment (generators, UPS, chillers), and the IT equipment itself (servers, storage). This targeted approach can make the initial LCAs more manageable.",
        "recommendedEvidence": "- A documented 'LCA Implementation Plan' or similar.\n- References to ISO 14040/14044 or EN 15978 in the plan.\n- Scope definition for LCAs (e.g., new builds from [Year] onwards).\n- Examples of LCA studies conducted for data centre projects, if any.\n- How LCA findings are intended to be used in decision-making.",
        "recommendedActions": "- Develop a formal plan for incorporating LCA into new build and major retrofit projects.\n- Identify or train personnel on LCA methodologies or engage external experts.\n- Define the scope and boundaries for typical data centre LCAs (e.g., cradle-to-grave).\n- Integrate LCA findings into the design and procurement decision-making processes.",
        "sampleResponseTemplate": "Our organization has a documented 'Data Centre Life Cycle Assessment Plan' ([Document ID]), which aligns with [ISO 14040/14044 or EN 15978]. For our new [Project Name] facility, an LCA is currently underway/planned, focusing on [key areas]. The aim is to inform [specific decisions] to reduce overall environmental impact.",
        "humanInputAnswer": "We have developed an 'LCA Framework for New Data Centre Projects' (LCA-FRM-001), aligned with ISO 14044 and EN 15978. This framework is mandatory for all new builds initiated after January 2024. For our 'Project Alpha' data centre, currently in design, we are conducting an LCA focusing on embodied carbon of construction materials and the lifecycle impact of primary cooling and power systems. The preliminary findings have already influenced our selection of low-carbon concrete alternatives.",
        "llmGeneratedAnswer": "Yes, a formal Life Cycle Assessment (LCA) plan, titled 'Data Centre Sustainability & LCA Protocol DC-LCA-2025', has been established. This plan is aligned with ISO 14040 and ISO 14044 methodologies and incorporates guidance from EN 15978 for assessing the environmental performance of building construction. The plan mandates an LCA for any new data centre construction or major retrofit project exceeding \u20ac5M in capital expenditure. The primary objectives are to quantify and minimize the embodied carbon of materials, evaluate the lifecycle environmental impact of critical M&E and IT equipment, and improve overall sustainability beyond operational energy efficiency. This protocol is currently being applied to the planned 'EU-West Hub' expansion project.",
        "badExampleAnswer": "We consider environmental impact when building, but we don't do formal LCAs as they seem very complex."
      },
      "question_6": {
        "id": 6,
        "old_question_text": null,
        "questionText": "Has the organization introduced a plan for Environmental Management in accordance with emerging EU guidelines and internationally standardised methodologies, such as ISO 14001, and considered appointing a cross-functional Environmental Sustainability Manager?",
        "simplifiedQuestion": "Do you have a formal plan for environmental management (e.g., aligned with ISO 14001) and potentially a dedicated manager?",
        "reference": "2025 CoC Guide 3.2.5",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_0",
        "recommendationTitle": "Environmental Management",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx"
          ],
          "description": "Documented plan for Environmental Management, details of alignment with standards like ISO 14001, and information on the Environmental Sustainability Manager role if established."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx",
        "documentDescriptor": "Mandatory: A documented plan for Environmental Management. If pursuing/achieved ISO 14001, provide relevant documentation. Details of the Environmental Sustainability Manager role if it exists.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of the environmental management plan, any ISO 14001 certification or related documents, and interview with the person(s) responsible for environmental management.",
        "nonconformity": {
          "major": "No documented plan or systematic approach to environmental management for the data centre.",
          "minor": "An informal environmental management approach exists, or the plan is not aligned with recognized standards or lacks dedicated oversight."
        },
        "majorNonconformity": "Absence of a plan for Environmental Management aligned with EU guidelines or international standards like ISO 14001.",
        "minorNonconformity": "A plan exists but is not comprehensive, lacks clear objectives, or the role of an Environmental Sustainability Manager has been considered but not effectively implemented or resourced.",
        "whatThisMeans": "Your organization should have a structured plan to manage its overall environmental impact, not just energy. This could involve following established frameworks like ISO 14001. Appointing a dedicated manager to oversee these efforts is also a good practice.",
        "complianceRequires": "- Introduce a documented plan for Environmental Management.\n- Align this plan with relevant EU guidelines and internationally recognized standards (e.g., ISO 14001).\n- Define environmental objectives and targets.\n- Establish processes to identify and manage environmental aspects and impacts.\n- Consider appointing a cross-functional Environmental Sustainability Manager or assigning clear responsibilities for this initiative.",
        "expertTip": "Even if you're not aiming for full ISO 14001 certification, using its structure (e.g., identifying environmental aspects, legal requirements, setting objectives, monitoring, and review) can provide a robust framework for your environmental management plan.",
        "recommendedEvidence": "- The Environmental Management Plan document.\n- ISO 14001 certificate (if applicable) or documents showing alignment progress.\n- Job description or role definition for the Environmental Sustainability Manager (if appointed).\n- Minutes from meetings where environmental management is discussed.",
        "recommendedActions": "- Develop or formalize your Environmental Management Plan.\n- Conduct an initial environmental review to identify key aspects and impacts.\n- Set clear environmental objectives and targets.\n- Define roles and responsibilities for environmental management, considering a dedicated manager.\n- Implement procedures for monitoring and measuring environmental performance.",
        "sampleResponseTemplate": "Our 'Environmental Management Plan [Year]' ([Document ID]) outlines our approach, which is aligned with [ISO 14001 principles/EU Guidelines]. We [have appointed/are in the process of appointing/have assigned responsibility to X for the role of] an Environmental Sustainability Manager. Key objectives include [Example Objective 1] and [Example Objective 2].",
        "humanInputAnswer": "We implemented our Environmental Management System (EMS) in 2023, which is certified to ISO 14001:2015 (Certificate #XYZ123). The EMS plan is documented in our 'EMS Manual v2.0'. Sarah Miller was appointed as our Environmental Sustainability Manager in Q3 2023, reporting to the Director of Operations, and she leads a cross-functional sustainability team.",
        "llmGeneratedAnswer": "Yes, the organization has an established Environmental Management Plan (EMP Ref: EMP-DC-2025) which is structured in accordance with ISO 14001 guidelines. While we are working towards full certification, the plan includes defined environmental objectives, targets, monitoring protocols, and management review processes. The role of Environmental Sustainability Lead has been assigned to the Head of Facilities, who coordinates a cross-functional working group that meets quarterly to oversee the EMP's implementation and progress.",
        "badExampleAnswer": "We recycle our office paper and try to be mindful of the environment."
      },
      "question_7": {
        "id": 7,
        "old_question_text": null,
        "questionText": "Has the organization introduced a plan for Energy Management in accordance with emerging EU guidelines and internationally standardised methodologies, such as ISO 50001, and considered appointing a cross-functional Energy Manager? Is the EU Code of Conduct used to underpin expectations and reporting for data centres in relation to ISO 50001?",
        "simplifiedQuestion": "Do you have a formal Energy Management plan (e.g., ISO 50001), an Energy Manager, and do you use the EU CoC to support it?",
        "reference": "2025 CoC Guide 3.2.6",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_0",
        "recommendationTitle": "Energy Management",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx"
          ],
          "description": "Documented plan for Energy Management (e.g., towards ISO 50001), evidence of how EU CoC supports this for the DC, and information on the Energy Manager role if established."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx",
        "documentDescriptor": "Mandatory: A documented plan for Energy Management. If pursuing/achieved ISO 50001, provide relevant documentation and how the EU CoC is integrated. Details of the Energy Manager role if it exists.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of the energy management plan, any ISO 50001 certification or related documents, evidence of CoC integration, and interview with the Energy Manager or responsible person.",
        "nonconformity": {
          "major": "No documented plan or systematic approach to energy management for the data centre.",
          "minor": "An informal energy management approach exists, the plan is not aligned with recognized standards, the CoC is not effectively used, or there's no dedicated oversight."
        },
        "majorNonconformity": "Absence of a plan for Energy Management aligned with EU guidelines or international standards like ISO 50001.",
        "minorNonconformity": "An Energy Management plan exists but is not comprehensive, lacks clear objectives, the link to EU CoC for DC specifics is missing, or the role of an Energy Manager has not been effectively implemented.",
        "whatThisMeans": "Your organization needs a structured system to continually improve energy performance. This involves creating an Energy Management plan, potentially following standards like ISO 50001. The EU Code of Conduct for Data Centres should be used as a detailed guide for the data centre parts of this plan. Appointing an Energy Manager to lead these efforts is also recommended.",
        "complianceRequires": "- Introduce a documented plan for Energy Management.\n- Align this plan with EU guidelines and internationally standardized methodologies (e.g., ISO 50001).\n- Specifically use the EU Code of Conduct on Data Centre Energy Efficiency to support and provide detail for the data centre aspects within the broader Energy Management plan/system.\n- Establish processes for setting energy objectives, monitoring energy performance, and implementing improvements.\n- Consider appointing a cross-functional Energy Manager or assigning clear responsibilities for this initiative.",
        "expertTip": "When implementing an ISO 50001 based Energy Management System, use the EU Code of Conduct's Best Practice list as a specific checklist for identifying energy performance improvement opportunities and operational controls relevant to your data centre. This makes the CoC a practical tool within your EnMS.",
        "recommendedEvidence": "- The Energy Management Plan document.\n- ISO 50001 certificate (if applicable) or documents showing alignment progress.\n- Documentation explicitly stating how the EU CoC for Data Centres is used in conjunction with the Energy Management plan (e.g., as a technical reference, for setting objectives, for reporting).\n- Job description or role definition for the Energy Manager (if appointed).\n- Minutes from energy management review meetings.",
        "recommendedActions": "- Develop or formalize your Energy Management Plan.\n- If pursuing ISO 50001, integrate the EU CoC as a core reference for data centre operations.\n- Define clear energy performance indicators (EnPIs) relevant to your data centre.\n- Establish a process for regular energy reviews and identification of improvement opportunities.\n- Assign clear roles and responsibilities, considering a dedicated Energy Manager.",
        "sampleResponseTemplate": "Our 'Energy Management System Plan [Year]' ([Document ID]) is aligned with [ISO 50001 principles/EU Guidelines]. We utilize the EU Code of Conduct for Data Centres as a primary reference for setting our data centre energy objectives and identifying best practices, as detailed in section [X] of our plan. [Name/Role] acts as our Energy Manager. Key data centre energy objectives include [Example Objective 1].",
        "humanInputAnswer": "We are certified to ISO 50001:2018 (Cert #ENMS456). Our EnMS manual, section 4.4.3, explicitly references the EU Code of Conduct for Data Centres as the guiding framework for identifying significant energy uses (SEUs) and improvement opportunities within our data centre facilities. John Doe, our Chief Engineer, is the appointed Energy Manager and chairs our cross-departmental Energy Team.",
        "llmGeneratedAnswer": "Yes, our organization has established an Energy Management Plan (EMP-DC-2025-Rev2) that conforms to ISO 50001 principles. Section 1.5 of this plan, 'Data Centre Specific Energy Management', explicitly states that the EU Code of Conduct on Data Centre Energy Efficiency (2025 Guidelines) serves as the primary technical framework for defining data centre energy performance indicators, identifying best practices for implementation, and structuring our internal energy reporting related to data centre operations. The plan also outlines the responsibilities of the 'Data Centre Energy Efficiency Lead', a role currently fulfilled by the Senior Facilities Manager, who coordinates with IT and Operations for this initiative.",
        "badExampleAnswer": "We monitor our energy bills and try to save energy. We are aware of ISO 50001 but haven't formally adopted it."
      },
      "question_8": {
        "id": 8,
        "old_question_text": null,
        "questionText": "Has the organization ensured that Asset Management for both IT and mechanical and electrical assets is implemented and controlled according to a standard and accepted methodology, such as ISO 55000, to underpin effective energy management by understanding the numbers, types, and purposes of deployed assets?",
        "simplifiedQuestion": "Do you have a formal Asset Management system (e.g., aligned with ISO 55000) covering DC assets that supports energy management?",
        "reference": "2025 CoC Guide 3.2.7",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_0",
        "recommendationTitle": "Asset Management",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "xlsx",
            "docx"
          ],
          "description": "Documentation of the Asset Management system (e.g., policy, procedures, excerpts from asset register showing IT and M&E assets) and its alignment with a standard methodology like ISO 55000. Evidence of how asset data supports energy management."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, xlsx, docx",
        "documentDescriptor": "Mandatory: Documentation of the Asset Management system covering IT and M&E assets, its methodology (e.g., alignment with ISO 55000), and examples of how asset data informs energy management.",
        "verification_method": [
          "Document Review",
          "Process Verification"
        ],
        "verificationMethods": "Review of asset management policies, procedures, and system records (e.g., asset register). Interviews with staff responsible for asset management and energy management to understand the linkage.",
        "nonconformity": {
          "major": "No systematic asset management for critical IT and M&E data centre assets, hindering effective energy management.",
          "minor": "Asset management processes are informal, incomplete (e.g., cover only IT or only M&E), or not effectively linked to energy management objectives."
        },
        "majorNonconformity": "Absence of a controlled Asset Management system for IT and M&E assets, or failure to use asset information for energy management purposes.",
        "minorNonconformity": "Asset Management system is partially implemented, does not follow a recognized standard, or its data is not effectively utilized to support energy management decisions.",
        "whatThisMeans": "Your organization needs a systematic way to manage all your data centre assets \u2013 both IT equipment (servers, storage, network) and the supporting M&E plant (UPS, chillers, etc.). This system, ideally following a standard like ISO 55000, helps you know what assets you have, their condition, and their purpose, which is fundamental for making informed energy management decisions.",
        "complianceRequires": "- Implement a controlled Asset Management system for both IT and M&E assets.\n- Follow a standard and accepted methodology for asset management (e.g., principles of ISO 55000).\n- Maintain an accurate understanding of the numbers, types, and purposes of all assets deployed in the data centre.\n- Ensure that this asset information is used to underpin and support effective energy management strategies and decisions.",
        "expertTip": "Ensure your asset register includes energy-relevant data for each asset, such as nameplate power consumption, typical operational power, age, efficiency rating (e.g., 80 PLUS for PSUs, SEER/COP for cooling units), and maintenance history related to efficiency. This data transforms the asset register into a powerful tool for energy analysis and planning.",
        "recommendedEvidence": "- Asset Management policy and procedures documentation.\n- Evidence of alignment with a standard methodology (e.g., ISO 55000 principles).\n- Excerpts from the asset register showing coverage of IT and M&E assets with relevant attributes.\n- Examples or case studies where asset management data was used to inform energy efficiency initiatives or decisions (e.g., equipment refresh planning, identifying underutilized assets).",
        "recommendedActions": "- Develop or enhance your Asset Management system to cover all DC assets comprehensively.\n- Align processes with a recognized standard like ISO 55000.\n- Ensure the asset inventory includes energy-related attributes.\n- Integrate asset data with energy monitoring systems and energy management decision-making processes.\n- Regularly audit and update asset information.",
        "sampleResponseTemplate": "Our Asset Management System, documented in '[AMS Policy Document ID]', covers all IT and M&E assets and is aligned with [ISO 55000 principles]. We track [examples of asset data, e.g., age, capacity, energy ratings] which informs our energy management by [explain how, e.g., 'identifying inefficient assets for replacement'].",
        "humanInputAnswer": "We have implemented an Asset Management System based on ISO 55000 principles, as detailed in our 'Enterprise Asset Management Framework'. This system includes a centralized CMDB that tracks all IT assets (servers, storage, network) and critical M&E infrastructure (UPS, chillers, generators). Each asset record includes specifications, purchase date, warranty information, maintenance history, and, where applicable, energy efficiency ratings and measured power consumption. This data directly supports our energy management program by identifying underutilized or inefficient assets for consolidation or refresh.",
        "llmGeneratedAnswer": "Yes, the organization has implemented a comprehensive Asset Management system (AMS) aligned with ISO 55000 principles, covering both IT equipment and Mechanical & Electrical (M&E) infrastructure. This is detailed in our 'Asset Management Policy & Procedures Manual (AMP-001)'. The system maintains a live inventory of all assets, including their specifications, operational status, age, and energy performance characteristics (e.g., nameplate efficiency, typical consumption). This comprehensive asset understanding is a foundational input for our energy audits, equipment refresh strategies, and capacity planning, ensuring that energy management decisions are based on accurate asset data.",
        "badExampleAnswer": "We have spreadsheets that list most of our servers and some of the big M&E equipment. The IT team knows what servers they have."
      },
      "question_9": {
        "id": 9,
        "old_question_text": null,
        "questionText": "Does the organization consider, record, and report on the proportion of energy used by the data centre that comes from renewable/sustainable sources, potentially using standardised metrics such as Renewable Energy Factor (REF) from EN 50600-4-3 or ISO/IEC 30134-3?",
        "simplifiedQuestion": "Do you track and report your data centre's use of renewable/sustainable energy?",
        "reference": "2025 CoC Guide 3.2.8",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre - consideration; reporting expected to become mandatory)",
        "stepSection": "step_0",
        "recommendationTitle": "Sustainable energy usage",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "xlsx",
            "docx"
          ],
          "description": "Documentation of how renewable/sustainable energy usage is considered, recorded, and reported. REF calculations if performed."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, xlsx, docx",
        "documentDescriptor": "Mandatory: Evidence of tracking and reporting the proportion of renewable/sustainable energy used. If REF (EN 50600-4-3) is calculated, provide methodology and results. Guarantees of Origin or equivalent.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of energy procurement records, renewable energy certificates, sustainability reports, and REF calculation methodology if used.",
        "nonconformity": {
          "major": "No consideration or tracking of renewable energy sources for the data centre.",
          "minor": "Renewable energy use is considered but not systematically recorded or reported; REF not calculated or inaccurately calculated if claimed."
        },
        "majorNonconformity": "Complete absence of tracking and reporting on the proportion of renewable/sustainable energy used by the data centre.",
        "minorNonconformity": "Renewable energy use is considered but not systematically recorded/reported, or the REF calculation (if performed) is not compliant with EN 50600-4-3/ISO/IEC 30134-3 (e.g., double counting, incorrect inclusion/exclusion of on-site generation).",
        "whatThisMeans": "Your organization should track how much of the energy your data centre uses comes from renewable sources (like solar, wind). This should be recorded and reported. Using a standard metric like the Renewable Energy Factor (REF) can help, but it's important to understand how it's calculated (e.g., it includes purchased renewable energy with guarantees and on-site production, but excludes on-site renewable energy sold back to the grid).",
        "complianceRequires": "- Consider, record, and report the proportion of data centre energy from renewable/sustainable sources against total consumption.\n- Understand and potentially use standardized metrics like Renewable Energy Factor (REF) as per EN 50600-4-3 or ISO/IEC 30134-3.\n- For REF calculation: include renewable energy purchased from utilities (with Guarantees of Origin) and renewable energy produced and consumed on-site.\n- For REF calculation: exclude any on-site renewable energy that is sold to the grid.\n- Be aware of differences between REF and other metrics like ETSI EN 305 200-3-1 KPIREN (which excludes grid energy and energy not consumed on-site).",
        "expertTip": "When reporting renewable energy usage, ensure transparency about the instruments used (e.g., Guarantees of Origin, RECs, PPAs) and clearly state the methodology (e.g., market-based vs. location-based for emissions, REF calculation method). This builds credibility and allows for accurate comparisons.",
        "recommendedEvidence": "- Energy bills and contracts showing procurement of certified renewable energy.\n- Guarantees of Origin (GOs) or Renewable Energy Certificates (RECs) that have been retired for the data centre's consumption.\n- Records of on-site renewable energy generation and consumption.\n- Documented methodology for calculating the percentage of renewable energy or REF.\n- Sustainability reports or internal reports showing renewable energy performance.",
        "recommendedActions": "- Establish a system for tracking all energy sources, distinguishing renewable/sustainable sources.\n- Procure renewable energy with appropriate certification (e.g., GOs).\n- If calculating REF, ensure adherence to EN 50600-4-3 / ISO/IEC 30134-3 standards.\n- Clearly document and report the renewable energy fraction regularly.\n- Set targets for increasing renewable energy usage.",
        "sampleResponseTemplate": "Our data centre uses [X]% renewable energy, sourced via [Methods, e.g., on-site solar, PPAs, RECs]. We track this using [Method/Standard] and report it in our [Report Name]. Our REF is [Value if calculated].",
        "humanInputAnswer": "For 2024, 65% of our data centre's electricity consumption was sourced from renewables. This comprises 15% from our on-site solar PV array (all consumed on-site) and 50% from purchased wind energy, verified by Guarantees of Origin retired in our name (Certificate #ABCDEF). We calculate and report our Renewable Energy Factor (REF) quarterly according to EN 50600-4-3; for Q4 2024, it was 0.65. This is detailed in our 'Quarterly Sustainability Report Q4-2024'.",
        "llmGeneratedAnswer": "Yes, the organization actively tracks and reports its sustainable energy usage. For the reporting year 2024, 72% of the total energy consumed by our primary data centre was derived from renewable sources. This figure is calculated in accordance with the Renewable Energy Factor (REF) methodology outlined in EN 50600-4-3. Our sources include a direct Power Purchase Agreement (PPA) for wind energy, supported by Guarantees of Origin (GOs), and on-site solar generation which is entirely consumed by the facility. All calculations and supporting documentation are compiled in our annual 'Renewable Energy Performance Report - DC01-2024'.",
        "badExampleAnswer": "Our electricity provider says some of their energy comes from renewables, so we probably use some."
      },
      "question_10": {
        "id": 10,
        "old_question_text": null,
        "questionText": "Has the organization implemented monitoring and reporting for the energy consumption of devices powered by IT cabling (e.g., Power over Ethernet devices such as telephony handsets, cameras, sensors, LED lights)? Is the risk of this power being drawn directly from network switches (constituting 'IT Load') considered in energy usage calculations like DCiE or PUE, especially if power is delivered outside the data centre's physical boundary?",
        "simplifiedQuestion": "Do you monitor and correctly account for energy used by devices powered via IT cabling (e.g., PoE)?",
        "reference": "2025 CoC Guide 3.2.9",
        "priority": "Low (Value: 1, Expected: Optional Practice)",
        "stepSection": "step_0",
        "recommendationTitle": "Powering of devices via the IT cabling",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "xlsx",
            "csv"
          ],
          "description": "Documentation of monitoring and accounting for energy consumed by devices powered via IT cabling, and its impact on DCiE/PUE, if material and monitored."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, xlsx, csv",
        "documentDescriptor": "Optional: Reports on energy consumption by IT-cabling-powered devices; Methodology for including/excluding this power in IT Load for PUE calculations if this is significant and tracked.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of any monitoring reports for devices powered by IT cabling and examination of how this energy is treated in overall IT load and PUE calculations, if applicable.",
        "nonconformity": {
          "major": null,
          "minor": "Energy from IT cabling-powered devices is significant but not monitored or accounted for, potentially skewing PUE (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to consider or account for the energy consumption of devices powered by IT cabling where this consumption is material to overall IT load figures, leading to inaccurate efficiency metrics.",
        "whatThisMeans": "You should be aware of devices (like IP phones, cameras, sensors) powered directly by your network switches via IT cables. This power is part of your IT energy load. If this is a significant amount and not tracked, especially if these devices are outside the main data centre area, your PUE or DCiE calculations could be misleading.",
        "complianceRequires": "- Monitor energy consumption by devices powered via IT cabling, particularly if this load is material.\n- Report on this usage where significant or if it impacts efficiency calculations.\n- Understand that this power contributes to the 'IT Load' component of PUE/DCiE.\n- Critically assess if PUE/DCiE metrics give false indications if such devices are powered and their energy usage (especially if outside the DC boundary) is not correctly accounted for.",
        "expertTip": "Modern network switches can often report PoE power draw per port or aggregated for the switch. Utilize these features to understand the magnitude of this load. If significant power is being delivered to devices outside the measured data centre space for PUE, ensure this is correctly handled in your PUE calculation methodology to maintain accuracy.",
        "recommendedEvidence": "- Network switch reports detailing PoE power consumption (if monitored).\n- Documentation outlining how energy for IT cabling-powered devices is treated in IT load calculations for PUE/DCiE.\n- An inventory or assessment of devices powered this way and their locations (inside/outside data centre space).",
        "recommendedActions": "- Assess the number and type of devices powered via IT cabling.\n- If the load is potentially significant, investigate monitoring capabilities of your network switches.\n- Review your PUE/DCiE calculation methodology to ensure this IT load is accurately defined and measured, distinguishing power delivered inside versus outside the DC if necessary.",
        "sampleResponseTemplate": "We have assessed devices powered by IT cabling. [Describe extent, e.g., 'Primarily IP phones in adjacent offices and security cameras within the DC']. We [monitor this via X / estimate this to be Y kWh]. For PUE calculations, this is [how it's handled, e.g., 'PoE to office phones is excluded from IT Load'].",
        "humanInputAnswer": "Our network switches (Cisco Catalyst 9300 series) provide aggregated PoE power consumption data. We estimate that approximately 5kW is constantly supplied via PoE to devices such as WAPs and security cameras within our data halls. This 5kW is included in our total IT equipment energy measurement for PUE calculations. We do not power devices outside the data centre halls via these switches.",
        "llmGeneratedAnswer": "Yes, energy consumption by devices powered via IT cabling is considered. Our network management system monitors the aggregate Power over Ethernet (PoE) load on all distribution switches. This load, primarily for internal security cameras and wireless access points, is logged monthly. As per our 'PUE Calculation Policy (DC-PUE-001)', this PoE load is factored into the total IT equipment energy (IT_EQ_Energy) denominator for PUE calculations, as these devices are integral to the data centre's operation and are located within its physical boundary. Currently, this load represents less than 0.5% of total IT energy.",
        "badExampleAnswer": "We use PoE for our office phones, but that's run from the office network switches, not the data centre ones."
      },
      "question_11": {
        "id": 11,
        "old_question_text": null,
        "questionText": "Has the organization considered the impact of mobile or shifting workloads on data centre design and operation, particularly concerning how increasingly fluctuating workloads and the ability to migrate workloads between sites might change required equipment resilience, reliability levels, and the management of installed power and cooling infrastructure from both capacity and energy efficiency perspectives?",
        "simplifiedQuestion": "Do you consider how mobile/shifting workloads affect your data centre's energy use, design, and operation?",
        "reference": "2025 CoC Guide 3.2.10",
        "priority": "Medium (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_0",
        "recommendationTitle": "Impact of mobile / shifting workloads",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "pptx"
          ],
          "description": "Documentation (e.g., design considerations, operational procedures, capacity planning reports) showing how the impact of mobile/shifting workloads is considered."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, pptx",
        "documentDescriptor": "Optional: Design documents, capacity planning models, or operational strategies that demonstrate consideration for fluctuating and mobile IT workloads and their impact on infrastructure efficiency and resilience.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of design philosophy, capacity planning documents, and interviews with IT and facilities management regarding strategies for managing variable workloads.",
        "nonconformity": {
          "major": null,
          "minor": "No documented consideration of mobile/shifting workload impacts on energy efficiency or infrastructure design, if this is relevant to the facility's operations (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If relevant workload types are present and this practice is adopted: Lack of formal consideration in design or operational strategy on how mobile/shifting workloads impact power and cooling capacity management and energy efficiency.",
        "whatThisMeans": "Modern IT workloads can fluctuate significantly and even move between data centres. Your organization should think about how these dynamic loads affect the design of your data centre (e.g., resilience needs) and how you manage your power and cooling systems to stay efficient and effective under these changing conditions.",
        "complianceRequires": "- Consider the type of workload(s) supported, both during design and operation.\n- Analyze how fluctuating or mobile workloads impact: \n    - Required equipment resilience and reliability levels at a site.\n    - Power infrastructure management (capacity and efficiency).\n    - Cooling infrastructure management (capacity and efficiency).\n- Develop strategies to manage infrastructure effectively under these dynamic load conditions.",
        "expertTip": "For facilities supporting highly variable or mobile workloads, implement modular power and cooling infrastructure that can scale efficiently. Also, consider software-defined power and workload orchestration tools that can dynamically adjust infrastructure resources or shift workloads to optimize for energy efficiency based on real-time demand.",
        "recommendedEvidence": "- Design documents that discuss workload variability and its impact on infrastructure choices.\n- Capacity planning reports that model shifting workloads.\n- Operational procedures for managing power and cooling during significant workload migrations or fluctuations.\n- Examples of how infrastructure (e.g., cooling setpoints, power distribution) is adjusted for varying loads.",
        "recommendedActions": "- During data centre design or refresh, explicitly assess the expected nature of IT workloads (stable, fluctuating, mobile).\n- Choose power and cooling infrastructure with good part-load efficiency and scalability if workloads are expected to be dynamic.\n- Implement monitoring and analytics to understand workload patterns and their impact on energy consumption.\n- Develop operational strategies to adjust infrastructure settings in response to shifting workloads to maintain efficiency.",
        "sampleResponseTemplate": "Our organization considers the impact of mobile/shifting workloads by [describe approach, e.g., 'designing for modularity in power/cooling', 'using workload management tools']. This is particularly relevant for our [Example Workload Type] services. Our strategy is documented in [Document Name].",
        "humanInputAnswer": "For our cloud platform services, which experience significant daily and seasonal load variations, our design incorporates modular Schneider Symmetra UPS systems and Stulz CyberRow in-row cooling that scales with IT deployment. Our 'Workload Management Policy DC-WLM-003' details how we use VMware vRealize to monitor and dynamically allocate resources, and how cooling capacity is managed in zones based on predicted and actual IT load, ensuring part-load efficiency.",
        "llmGeneratedAnswer": "Yes, consideration of mobile and shifting workloads is integral to our data centre strategy, as outlined in 'DC_Design_Standard_Rev4: Section 6 - Workload Adaptability'. We recognize that traditional steady-state power draws are evolving. Our new deployments incorporate scalable power modules and variable-capacity cooling systems designed to maintain high efficiency across a wider operational range (20-80% load). Furthermore, our capacity planning models now incorporate workload mobility patterns to forecast potential shifts in resource demand across sites, influencing resilience planning and proactive infrastructure adjustments to optimize energy efficiency.",
        "badExampleAnswer": "Our servers are always on, so workload shifts don't really change much for our power and cooling."
      }
    }
  },
  "step_1": {
    "description": "2025 Guide Section 3: Data Centre Utilisation, Management and Planning (Practices 3.2.11 through 3.3.5 - covering second part of Section 3.2 and all of Section 3.3)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_DC_Mgmt_Planning"
    ],
    "critical_questions": [],
    "questions": {
      "question_12": {
        "id": 12,
        "old_question_text": null,
        "questionText": "Has the organization given consideration to energy supplies from alternative and sustainable energy sources (including fuel cells, wind power, photo-voltaic, shared local-generation using biomass/bio-fuels, etc.) as potentially lower carbon footprint, economically attractive, and reliable alternatives to traditional utility electricity supplies, noting this practice aims to reduce overall carbon footprint and improve sustainability rather than directly providing energy efficiency within the data centre itself?",
        "simplifiedQuestion": "Have you considered using alternative/sustainable energy sources (fuel cells, wind, solar, biomass) for your data centre?",
        "reference": "2025 CoC Guide 3.2.11",
        "priority": "Low (Value: 1, Expected: Optional Practice)",
        "stepSection": "step_1",
        "recommendationTitle": "Alternative power generation technologies",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Feasibility studies, evaluations, or strategy documents related to the consideration or implementation of alternative and sustainable energy sources."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: Documentation of feasibility studies, pilot projects, or implementation plans for alternative power generation technologies like fuel cells, wind, solar, biomass, etc.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of any documentation related to the assessment or adoption of alternative energy sources. Interviews with personnel involved in energy procurement and sustainability strategy.",
        "nonconformity": {
          "major": null,
          "minor": "No consideration given to alternative or sustainable energy sources where opportunities may exist (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Superficial consideration without proper technical or economic feasibility analysis of potentially viable alternative energy sources.",
        "whatThisMeans": "Your organization should explore the possibility of powering your data centre with alternative and sustainable energy sources like fuel cells, wind, solar, or biomass. While this might not make the data centre itself more energy-efficient internally, it can significantly reduce its overall carbon footprint and improve sustainability, and could offer economic or reliability benefits.",
        "complianceRequires": "- Give formal consideration to alternative and sustainable energy supplies.\n- Evaluate technologies such as fuel cells, wind power, photo-voltaic (solar), shared local-generation using biomass/bio-fuels.\n- Assess these alternatives based on their potential to provide a lower carbon footprint, economic attractiveness, and reliability compared to conventional utility supplies.\n- Recognize that the primary aim of this practice is overall carbon footprint reduction and sustainability, not necessarily direct internal data centre energy efficiency gains.",
        "expertTip": "When evaluating alternative energy sources, consider a portfolio approach. Combining on-site generation (like solar PV or fuel cells for baseload/backup) with off-site procurement (like Power Purchase Agreements for wind or solar) can provide a balanced solution for cost, reliability, and carbon reduction. Also, investigate local community energy projects or shared generation schemes.",
        "recommendedEvidence": "- Feasibility studies or white papers evaluating various alternative energy technologies for the data centre.\n- Business cases or proposals for implementing specific alternative energy solutions.\n- Contracts or agreements related to the procurement of alternative energy (e.g., PPAs).\n- If implemented, schematics and performance data of on-site alternative energy generation systems.",
        "recommendedActions": "- Conduct a formal assessment of locally available and economically viable alternative and sustainable energy sources.\n- Evaluate the technical feasibility of integrating these sources with the data centre's power infrastructure.\n- Analyze the lifecycle carbon impact and economic benefits/costs of adoption.\n- If viable, develop an implementation plan or incorporate into long-term energy strategy.",
        "sampleResponseTemplate": "Our organization has considered alternative energy sources as documented in our '[Feasibility Study Name/Energy Strategy Document]'. We have evaluated [List technologies, e.g., solar PV, fuel cells] and found [brief outcome, e.g., 'on-site solar PV to be viable and plan to install X kWp by YYYY' or 'currently investigating PPAs for wind energy']. Our focus is on [reducing carbon footprint/improving sustainability].",
        "humanInputAnswer": "We conducted a 'Sustainable Power Alternatives Study' in 2024 (SPA-2024-Final.pdf). This evaluated on-site solar PV, hydrogen fuel cells, and local biomass co-generation. While on-site solar was deemed viable for supplementing ~5% of our load (Phase 1 installation planned for 2026), fuel cells were found to be currently too expensive for primary power. We are actively monitoring developments in green hydrogen fuel cell technology for future consideration as part of our Net Zero 2040 roadmap.",
        "llmGeneratedAnswer": "Yes, the organization's 'Long-Term Energy Strategy (LTES-2025)' includes a dedicated section (Section 4.3) on 'Alternative and Sustainable Power Sources'. This section documents our ongoing evaluation of fuel cells (specifically solid oxide and PEM types), on-site solar photovoltaic arrays, and potential for participation in local wind power projects. While direct implementation has not yet occurred due to current local economic viability and site constraints, feasibility studies are refreshed biennially. The primary driver for these evaluations is our corporate goal to reduce Scope 2 emissions by 40% by 2030.",
        "badExampleAnswer": "We use grid power. Alternative energy is too expensive for us right now."
      },
      "question_13": {
        "id": 13,
        "old_question_text": null,
        "questionText": "Has the organization ensured that air quality within the data centre is monitored and managed to prevent damage to critical IT and cooling equipment from particulates or corrosive elements, thereby maintaining performance, energy efficiency, and reliability? Does this management inform the choice of air filters, their replacement schedule, and the frequency of routine technical cleaning (including underfloor and ceiling voids if applicable), with filter choices informed by ISO 16890-1 and air quality/cleanliness potentially referencing ISO 14644-8 Class 8 and gaseous contaminants ANSI/ISA 71.04-2013?",
        "simplifiedQuestion": "Do you monitor and manage air quality (dust, corrosive gases) in your data centre to protect equipment?",
        "reference": "2025 CoC Guide 3.2.12",
        "priority": "Medium (Value: 2, Expected: Entire Data Centre)",
        "stepSection": "step_1",
        "recommendationTitle": "Monitor and manage air quality",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Air quality monitoring plan, sensor data/reports, filter specifications (ISO 16890-1), replacement schedules, technical cleaning procedures and logs, and any reports referencing ISO 14644-8 or ANSI/ISA 71.04-2013."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory: Documentation of air quality monitoring procedures, filter selection criteria (ideally referencing ISO 16890-1), filter replacement logs, and technical cleaning schedules. Records of air quality measurements and any compliance reports (e.g., against ISO 14644-8 or ANSI/ISA 71.04).",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "Data Analysis"
        ],
        "verificationMethods": "Review of air quality management plan, filter specs, cleaning logs, and air quality monitoring data. Inspection of air handling units, filters, and general cleanliness of technical spaces.",
        "nonconformity": {
          "major": "No air quality monitoring or management in place, leading to visible contamination or evidence of equipment damage from particulates/corrosion.",
          "minor": "Informal air quality management, inadequate filter selection/maintenance, or incomplete cleaning routines; lack of adherence to relevant ISO/ANSI standards if claimed."
        },
        "majorNonconformity": "Absence of any systematic air quality monitoring or management program, resulting in potential risk to equipment from contamination.",
        "minorNonconformity": "Air quality management practices are ad-hoc; filter choices are not based on standards like ISO 16890-1, or cleaning/filter replacement is inconsistent. Monitoring data, if collected, is not acted upon.",
        "whatThisMeans": "Your data centre's air needs to be clean to prevent dust and corrosive gases from damaging your sensitive IT and cooling equipment. This involves monitoring air quality, using appropriate filters (chosen based on standards like ISO 16890-1), changing them regularly, and performing routine technical cleaning of the data centre spaces, including underfloor or ceiling plenums if used for airflow.",
        "complianceRequires": "- Monitor and manage air quality to protect IT and cooling equipment.\n- Prevent damage from particulates and corrosive elements to maintain performance, energy efficiency, and reliability.\n- Use air quality management to inform the choice of filters (referencing ISO 16890-1) and their replacement schedule.\n- Implement a routine technical cleaning program (including underfloor/ceiling voids if applicable).\n- Consider referencing ISO 14644-8 Class 8 for particulate cleanliness and ANSI/ISA 71.04-2013 for gaseous contaminants.",
        "expertTip": "Implement a particle counting and gaseous contaminant monitoring program, even if periodic, to establish a baseline and identify trends. This data can justify investment in better filtration or more frequent cleaning, and help diagnose sources of contamination (e.g., external air, construction activities, outgassing from materials).",
        "recommendedEvidence": "- Air quality management plan/procedure.\n- Specifications for air filters used, referencing ISO 16890-1 if possible.\n- Filter replacement logs and schedule.\n- Technical cleaning program documentation and logs.\n- Results from any air quality monitoring (particulate counts, gaseous contaminant levels).\n- Reports or certifications referencing ISO 14644-8 or ANSI/ISA 71.04-2013, if available.",
        "recommendedActions": "- Establish an air quality monitoring program (can be periodic).\n- Select air filters based on environmental conditions and equipment sensitivity, guided by ISO 16890-1.\n- Implement a scheduled filter replacement and technical cleaning program.\n- If issues are suspected or high-reliability is critical, conduct specific particulate and gaseous contaminant testing against relevant standards.",
        "sampleResponseTemplate": "We monitor air quality by [Method, e.g., 'periodic particulate counts and assessment of corrosive gases'] and manage it through [Actions, e.g., 'use of ISO 16890-1 compliant ePM1 50% filters changed quarterly, and annual deep cleaning of data halls']. Our last particulate count was [Value] which aligns with [Standard, e.g., ISO 14644-8 Class 8].",
        "humanInputAnswer": "Our 'Air Quality Management Protocol DC-AQM-002' mandates MERV 13 (equivalent to ePM1 50-65%) filters on all AHUs serving data halls, changed quarterly. We conduct annual particulate testing to ISO 14644-8 Class 8 standards and biennial gaseous contaminant testing (ISA 71.04 G1 severity level) via a third-party lab. Underfloor plenums are cleaned every 2 years. Last particulate test (June 2024) confirmed Class 8 compliance.",
        "llmGeneratedAnswer": "Yes, air quality is actively monitored and managed. Our 'Environmental Control SOP EC-004' details our air quality program. This includes:\n- Continuous particulate monitoring (PM2.5, PM10) in all data halls.\n- Use of ISO 16890-1 compliant filters (ePM1 70%) with replacement based on differential pressure triggers and a maximum 6-month interval.\n- Annual technical cleaning of data halls, including underfloor voids.\n- Biennial testing for gaseous contaminants against ANSI/ISA 71.04-2013 G1 severity by an accredited lab (last report: 'AQ_Test_Report_2024_Q2').\nOur air filtration strategy aims to meet or exceed ISO 14644-8 Class 8 cleanliness.",
        "badExampleAnswer": "We change the air filters when they look dirty. The data centre is pretty clean."
      },
      "question_14": {
        "id": 14,
        "old_question_text": null,
        "questionText": "Has the organization established and enforced a policy to consider data centre technical areas and plant rooms as industrial spaces, designed, built, and operated with the primary objective of delivering high availability IT services reliably and efficiently, rather than for human comfort (beyond statutory health and safety requirements)? Does this policy ensure that energy efficiency in technical spaces is not compromised by human comfort needs, and that these areas (excluding dedicated office spaces within the building) primarily control make-up air for pressurization against contaminants rather than for seated human comfort?",
        "simplifiedQuestion": "Are your data centre technical areas treated as industrial spaces focused on IT equipment needs, not human comfort?",
        "reference": "2025 CoC Guide 3.2.13",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_1",
        "recommendationTitle": "Consider technical areas of data centres as industrial space",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx"
          ],
          "description": "Policy documents defining technical spaces as industrial. Design guidelines for data centre technical areas. HVAC design philosophy for technical vs. office spaces within the data centre building."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx",
        "documentDescriptor": "Mandatory: Policy document or design standard classifying data centre technical areas as industrial spaces. HVAC design showing separate considerations for technical spaces versus dedicated office areas. Site plans differentiating these zones.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of policies, design standards, and HVAC specifications. Physical inspection of data centre technical areas to observe conditions and separation from office spaces.",
        "nonconformity": {
          "major": "Data centre technical areas are consistently managed for human comfort levels (e.g., office-like temperatures/humidity) at the expense of energy efficiency.",
          "minor": "Policy exists but is not consistently applied, or some technical areas are unnecessarily conditioned for human comfort without clear justification."
        },
        "majorNonconformity": "Systematic operation of data centre technical areas to human comfort standards (e.g., typical office temperatures and HVAC settings) significantly impacting energy efficiency, without specific justification for such conditions.",
        "minorNonconformity": "Lack of a clear policy differentiating environmental conditions for technical spaces versus office areas, or instances where technical spaces are over-conditioned beyond equipment and pressurization needs.",
        "whatThisMeans": "The parts of your data centre that house IT equipment, power systems, and cooling machinery should be treated like industrial zones, not offices. This means their environment (temperature, humidity, air) should be optimized for the equipment's reliable and efficient operation, not for people to sit and work comfortably. Dedicated office spaces within the building can have comfort-focused conditions, but not the tech areas.",
        "complianceRequires": "- Classify data centre technical areas and plant rooms as industrial spaces.\n- Design, build, and operate these spaces with the primary objective of delivering high-availability IT services reliably and efficiently.\n- Ensure energy efficiency is not compromised by requirements for human comfort beyond statutory health and safety needs.\n- For technical areas (not dedicated offices): control make-up air volumes primarily for pressurization (to avoid ingress of particles/contaminants) rather than for seated human comfort.\n- Technical areas housing operational IT or supporting M&E should not contain desks or workstations for regular office work.",
        "expertTip": "Clearly demarcate 'technical industrial spaces' from any 'dedicated office spaces' within the data centre building in your design documents and signage. Ensure HVAC systems are zoned separately, with different control philosophies and setpoints for these distinct areas to prevent energy waste from over-cooling technical zones for human comfort.",
        "recommendedEvidence": "- Policy document stating that technical areas are industrial spaces.\n- Design guidelines or standards for data centre technical areas specifying environmental parameters optimized for equipment.\n- HVAC design documents showing separate zoning and control strategies for technical areas vs. office spaces.\n- Photographic evidence or site plans showing no permanent workstations in technical equipment areas.",
        "recommendedActions": "- Develop and formally adopt a policy classifying data centre technical areas as industrial spaces.\n- Review and update HVAC design and operational setpoints for technical areas to prioritize equipment needs and energy efficiency over human comfort (while meeting H&S laws).\n- Ensure any new builds or retrofits adhere to this classification and design principle.\n- Relocate any non-essential human occupancy/workstations out of technical areas.",
        "sampleResponseTemplate": "Our 'Data Centre Operational Standards Policy' (Doc ID: DC-OPS-STD-002) explicitly defines all server halls, M&E plant rooms, and electrical switch rooms as 'Industrial Technical Spaces'. Environmental conditions in these zones are maintained solely for equipment reliability and efficiency, with make-up air managed for positive pressurization. Dedicated office areas within the facility have separate HVAC systems.",
        "humanInputAnswer": "Our company policy, 'DC Environment Standard DC-ENV-001', clearly defines data halls and M&E plant rooms as 'controlled industrial environments.' HVAC in these zones is optimized for ASHRAE A1 Allowable ranges for IT equipment, not human comfort. For example, our data halls operate at 25\u00b0C. Only essential maintenance staff access these areas; no permanent workstations are located there. Adjacent office spaces have separate comfort cooling systems.",
        "llmGeneratedAnswer": "Yes, the organization's 'Data Centre Design and Operations Standard (DCDOS-2025)' formally classifies all technical areas (including data halls, UPS rooms, and chiller plants) as 'Industrial Process Environments'. Section 3.2 of DCDOS-2025 specifies that these areas are to be designed and operated primarily for the reliability and energy efficiency of the housed equipment, with human access limited to operational and maintenance tasks. Environmental controls prioritize equipment specifications and air pressurization to prevent contaminant ingress, rather than human comfort levels, except where mandated by local Health & Safety regulations. Dedicated office spaces within the facility are served by separate HVAC systems optimized for human occupancy.",
        "badExampleAnswer": "Our data centre is kept cool and comfortable for the staff who need to work in there with the servers."
      },
      "question_15": {
        "id": 15,
        "old_question_text": null,
        "questionText": "Has the organization ensured that high quality, accurate, and current documentation such as O&M manuals, As-Built records, commissioning records, schematics, and single-line diagrams are readily available for all installed infrastructure and equipment? Is there a process to update this documentation whenever settings are changed or equipment is added, replaced, or modified, and are historical records maintained? Is effective commissioning and delivery of detailed, accurate documentation a key part of project handovers, potentially referencing EN 50600-3-1?",
        "simplifiedQuestion": "Do you have up-to-date and accurate technical documents (manuals, drawings, schematics) for all your data centre infrastructure?",
        "reference": "2025 CoC Guide 3.2.14",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_1",
        "recommendationTitle": "Site Documentation",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "Examples of O&M manuals, as-built drawings, commissioning records, schematics. Documentation of the process for managing and updating these records. Project handover procedures emphasizing documentation delivery."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Mandatory: Accessible and current O&M manuals, as-built drawings, commissioning reports, single-line diagrams, and schematics. Evidence of a process for updating documentation after changes and for project handovers.",
        "verification_method": [
          "Document Review",
          "Process Verification"
        ],
        "verificationMethods": "Review of sample site documentation (O&M manuals, as-builts, schematics) for completeness and currency. Examination of change management processes and project handover procedures related to documentation updates.",
        "nonconformity": {
          "major": "Critical site documentation (e.g., as-builts, single-line diagrams) is missing, significantly outdated, or inaccessible.",
          "minor": "Some documentation is outdated, incomplete, or processes for updates and project handover are not consistently followed."
        },
        "majorNonconformity": "Absence of, or significantly inaccurate/outdated, critical site documentation such as O&M manuals, as-built records, or schematics for major infrastructure.",
        "minorNonconformity": "Documentation exists but is not consistently updated after changes, or the project handover process for documentation is not robustly implemented. Historical records are not well maintained.",
        "whatThisMeans": "Your organization must maintain a complete and up-to-date set of technical documents for all data centre systems. This includes manuals from manufacturers, 'as-built' drawings showing exactly how things were installed, records from when systems were commissioned, electrical single-line diagrams, and other schematics. These documents are essential for operating efficiently, troubleshooting, and making future changes correctly. They must be updated whenever changes occur.",
        "complianceRequires": "- Maintain high quality, accurate O&M manuals, as-built records, commissioning records, schematics, and single-line diagrams for all installed infrastructure and equipment.\n- Ensure these documents are readily available to relevant personnel.\n- Implement a process to update documentation whenever settings are changed, or equipment is added, replaced, or modified.\n- Maintain historical records of documentation and changes.\n- Make effective commissioning and the delivery of detailed, accurate documentation a key requirement of any project handover process (referencing EN 50600-3-1 can be useful).",
        "expertTip": "Implement a centralized Document Management System (DMS) with version control and a formal check-in/check-out process for all site documentation. Link documentation updates to your Change Management process, making it a mandatory step before a change is considered complete.",
        "recommendedEvidence": "- A central repository (e.g., DMS) for site documentation.\n- Examples of current O&M manuals, as-built drawings, schematics, and commissioning records.\n- Change management procedures that include steps for updating relevant documentation.\n- Project handover checklists or procedures that specify documentation deliverables.\n- Evidence of recent documentation updates related to system changes or additions.",
        "recommendedActions": "- Conduct an audit of existing site documentation to identify gaps or outdated materials.\n- Digitize paper documents where feasible and implement a centralized DMS.\n- Establish a clear process and assign responsibilities for updating documentation when changes occur.\n- Integrate documentation updates into project management and change management workflows.\n- Ensure project close-out includes handover and verification of all required documentation.",
        "sampleResponseTemplate": "We maintain comprehensive site documentation, including [list key document types like O&M manuals, as-builts, schematics], in our [DMS Name or location]. Our Change Management Process (Ref: CMP-001) mandates updates to relevant documentation, verified before change closure. For new projects, documentation handover is a key milestone as per our Project Management Framework, referencing EN 50600-3-1 guidelines.",
        "humanInputAnswer": "All our critical facility documentation, including O&M manuals, as-built CAD drawings (updated after every major project), commissioning reports from the last 5 years, and electrical single-line diagrams, are stored in our SharePoint-based Document Control System (DCS). Our 'MOC Policy DC-MOC-005' requires that any modification to equipment or settings triggers a documentation review and update task, tracked in our Jira system. Historical versions are archived in the DCS.",
        "llmGeneratedAnswer": "Yes, the organization maintains a comprehensive library of site documentation managed through our 'Enterprise Document Management System (EDMS) - SiteDocs Module'. This includes:\n- Up-to-date O&M manuals for all critical M&E and IT infrastructure.\n- Verified As-Built drawings and schematics, revised post any significant modification (as per 'Change Management Procedure CHG-003, Section 4.7').\n- Complete commissioning records for all new systems and major upgrades, following EN 50600-3-1 for handover processes.\n- Electrical single-line diagrams, P&IDs for mechanical systems, and network architecture diagrams.\nOur EDMS enforces version control, and a formal 'Documentation Update Protocol' ensures records are amended within 14 days of any equipment change or system reconfiguration. Historical records are archived indefinitely.",
        "badExampleAnswer": "We have manuals for most equipment, usually kept in the plant room or with the engineering team. We try to update drawings when big changes happen."
      },
      "question_16": {
        "id": 16,
        "old_question_text": null,
        "questionText": "Has the organization ensured that the Data Centre Manager and any appointed Energy Manager possess relevant and appropriate training for their energy efficiency roles, including an in-depth appreciation of Best Practices (such as this Code of Conduct)? Is there a broader training methodology to ensure both IT and Facilities personnel are aware of Data Centre energy efficiency Best Practices, to promote common understanding, remove silos, and develop suitably qualified and experienced data centre personnel essential for efficient operation?",
        "simplifiedQuestion": "Are your data centre managers and staff adequately trained in data centre energy efficiency and best practices?",
        "reference": "2025 CoC Guide 3.2.15",
        "priority": "High (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_1",
        "recommendationTitle": "Training and Development",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Training plans, curriculum for DC Manager/Energy Manager, records of training completed, materials from energy efficiency training modules, overall staff training methodology for DC energy efficiency."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory: Documented training program/methodology for data centre staff on energy efficiency. Evidence of specific training for DC/Energy Managers on CoC Best Practices. Training records.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of training plans, course materials, and attendance records. Interviews with DC Manager, Energy Manager, and samples of IT and Facilities staff to assess awareness of energy efficiency best practices.",
        "nonconformity": {
          "major": "No specific energy efficiency training for key roles (DC Manager, Energy Manager) or a general lack of awareness of best practices among relevant staff.",
          "minor": "Training is ad-hoc, not comprehensive, or doesn't adequately cover relevant CoC best practices or promote cross-functional understanding."
        },
        "majorNonconformity": "Absence of formal training on data centre energy efficiency best practices for key management roles (DC Manager, Energy Manager) and relevant IT/Facilities staff.",
        "minorNonconformity": "Training on energy efficiency is informal, lacks structure, does not adequately cover CoC best practices, or fails to foster cross-disciplinary understanding between IT and Facilities.",
        "whatThisMeans": "Key staff like your Data Centre Manager and any Energy Manager need specific training on energy efficiency and best practices like those in the EU Code of Conduct. Beyond that, all IT and Facilities staff involved with the data centre should also be trained on energy efficiency to ensure everyone understands their role, can work together effectively, and helps operate the data centre efficiently.",
        "complianceRequires": "- Ensure the Data Centre Manager and any appointed Energy Manager have relevant training in energy efficiency and an in-depth appreciation of CoC Best Practices.\n- Implement an overall training methodology for both IT and Facilities personnel regarding data centre energy efficiency best practices.\n- Aim to remove silos and promote a common understanding of operations and management related to energy efficiency.\n- Develop suitably qualified and experienced data centre personnel through this training.\n- Recognize that properly trained personnel are essential for efficient data centre operation.",
        "expertTip": "Incorporate practical, hands-on elements into your energy efficiency training. Use your own facility's data and systems as case studies. Consider cross-departmental workshops where IT and Facilities teams can discuss challenges and solutions together, fostering better collaboration.",
        "recommendedEvidence": "- Training records for DC Manager and Energy Manager showing energy efficiency / CoC Best Practice training.\n- Overall data centre staff training plan that includes energy efficiency modules.\n- Examples of training materials used for IT and Facilities staff on energy efficiency.\n- Description of how cross-functional understanding (IT/Facilities) is promoted through training.",
        "recommendedActions": "- Identify training needs for DC Managers, Energy Managers, IT staff, and Facilities staff related to energy efficiency and CoC Best Practices.\n- Source or develop appropriate training modules.\n- Implement a regular training schedule and track completion.\n- Foster a culture of continuous learning and knowledge sharing on energy efficiency.",
        "sampleResponseTemplate": "Our Data Centre Manager [Name] and Energy Lead [Name] have completed [Specific Training, e.g., 'Certified Data Centre Energy Professional' and 'EU CoC Best Practices Workshop'] on [Dates]. Our 'DC Staff Development Program' includes mandatory energy efficiency modules (Ref: TRN-DCEE-01) for all IT and Facilities personnel, reviewed annually. These modules aim to [explain goal, e.g., 'improve cross-functional understanding of PUE optimization'].",
        "humanInputAnswer": "Our DC Operations Manager completed the 'Certified Data Centre Energy Practitioner' course in 2023. All our data centre facilities and IT operations staff undergo an annual mandatory online training module covering the EU Code of Conduct Best Practices and our internal energy efficiency policies. We also hold bi-annual joint workshops for IT and Facilities teams focusing on specific efficiency initiatives like airflow management and PUE optimization, as documented in our 'Training Plan HRP-DC-TRN-2024'.",
        "llmGeneratedAnswer": "Yes, a comprehensive training and development program is in place. The Data Centre Operations Manager and the designated Energy Efficiency Champion have both completed accredited 'Data Centre Energy Efficiency Management' certifications which explicitly cover the EU Code of Conduct Best Practices (certificates available: Cert_DCMgr_EE2024, Cert_EnergyChamp_EE2024). Furthermore, our 'Continuous Learning Framework for Technical Staff (CLF-TS-001)' mandates annual refresher modules on 'Data Centre Energy Best Practices' for all IT infrastructure and Facilities engineering personnel. This framework includes cross-functional workshops to ensure shared understanding and break down operational silos, fostering collaborative approaches to energy saving initiatives. Training records are maintained in our LMS.",
        "badExampleAnswer": "Our managers are very experienced. Staff learn about energy saving on the job."
      },
      "question_17": {
        "id": 17,
        "old_question_text": null,
        "questionText": "Has the organization continually evaluated the use of its energy storage and consumption patterns to support Smart Grid interactions, including 'Behind the Meter' support, considering solutions that employ a broad range of information technology resources for potential reduction in electricity waste and energy costs? Is the organization aware of and examining guidance from the European Commission and standards like EN 50600-5-1 related to Smart Grids as referenced by the Energy Efficiency Directive (EED)?",
        "simplifiedQuestion": "Are you evaluating how your data centre can interact with Smart Grids to improve energy efficiency and reduce costs?",
        "reference": "2025 CoC Guide 3.2.16",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_1",
        "recommendationTitle": "Smart Grid",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "pptx"
          ],
          "description": "Documentation of evaluations or feasibility studies regarding Smart Grid interaction, energy storage use for grid support, and 'Behind the Meter' strategies. References to EED, EN 50600-5-1 considerations."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, pptx",
        "documentDescriptor": "Optional: Feasibility studies, strategy documents, or pilot project reports on data centre interaction with Smart Grids, including demand response, energy storage for grid services, and alignment with relevant directives/standards (EED, EN 50600-5-1).",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of any evaluations or strategies for Smart Grid participation. Interviews with personnel responsible for energy strategy and infrastructure planning.",
        "nonconformity": {
          "major": null,
          "minor": "No consideration or evaluation of Smart Grid interaction opportunities, if practice is adopted and relevant local Smart Grid infrastructure exists."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Superficial evaluation of Smart Grid capabilities without considering 'Behind the Meter' support, relevant EU guidance, or standards like EN 50600-5-1.",
        "whatThisMeans": "Your organization should look into how your data centre can work with the 'Smart Grid'. This means using technology to manage your energy use flexibly, potentially using energy storage, to help reduce electricity waste, lower energy costs, and support the stability of the electricity grid. This includes considering 'Behind the Meter' solutions (on your side of the electricity meter) and being aware of EU guidelines and standards in this area.",
        "complianceRequires": "- Continuously evaluate the use of energy storage and data centre energy usage patterns to support Smart Grid interactions.\n- Consider solutions that use information technology to potentially reduce electricity waste and energy costs.\n- Include evaluation of 'Behind the Meter' support capabilities.\n- Be aware of and examine guidance and requirements being developed by the European Commission concerning Smart Grids, as referenced by the Energy Efficiency Directive (EED).\n- Consider relevant standards like EN 50600-5-1 which incorporates practices for Smart Grid interaction.",
        "expertTip": "Start by engaging with your utility provider to understand available Smart Grid programs, dynamic tariffs, and demand response opportunities. Evaluate how your existing assets (like UPS batteries or backup generators, if regulations permit) could participate, or if dedicated energy storage could provide a good return by offering grid services or enabling peak shaving.",
        "recommendedEvidence": "- Reports or studies evaluating Smart Grid interaction opportunities.\n- Documentation of energy storage systems and their potential for grid support.\n- Evidence of participation in demand response programs or other Smart Grid initiatives.\n- Internal discussions or strategy documents referencing EED, EN 50600-5-1 in the context of Smart Grids.",
        "recommendedActions": "- Research and understand local Smart Grid initiatives and utility programs.\n- Assess the data centre's load flexibility and potential for demand response or ancillary services.\n- Evaluate the feasibility of deploying energy storage for 'Behind the Meter' grid support.\n- Stay informed about evolving EU directives, Commission guidance, and relevant standards (e.g., EN 50600 series) related to Smart Grids.",
        "sampleResponseTemplate": "Our organization is evaluating Smart Grid interaction as part of our [Energy Strategy Document Name]. We have [conducted a feasibility study / are participating in a pilot program] regarding [specific Smart Grid aspect, e.g., demand response, battery storage for grid services]. We are referencing [EED guidance / EN 50600-5-1] in these evaluations.",
        "humanInputAnswer": "We are actively evaluating Smart Grid participation. Our 'Energy Innovation Team' completed a preliminary study in Q4 2024 (Doc: 'SmartGrid_Feasibility_Study_2024.pdf') focusing on 'Behind the Meter' battery storage for peak shaving and potential ancillary service provision. We are also in discussion with our utility provider about their upcoming dynamic tariff structures, referencing the latest EED interpretation notes from the Commission and EN 50600-5-1 for best practices.",
        "llmGeneratedAnswer": "Yes, our organization's 'Future Energy Initiatives Report (FEI-2025)' details our assessment of leveraging our 2MWh battery energy storage system (BESS) for 'Behind the Meter' grid support services, including frequency regulation and demand response. This evaluation considers the technical requirements specified in EN 50600-5-1 and aligns with objectives outlined in the EU Energy Efficiency Directive. We are currently in Phase 1, which involves discussions with our local Distribution System Operator (DSO) about participation in their pilot Smart Grid programs.",
        "badExampleAnswer": "We are not connected to a Smart Grid. We just use power from the utility."
      },
      "question_18": {
        "id": 18,
        "old_question_text": null,
        "questionText": "Has the organization ensured that for new builds or colocation purchases, only the level of resilience (and therefore availability) actually justified by clearly documented business requirements and impact analysis is built or purchased? Is there a clear understanding that 2N infrastructures are frequently unnecessary, and that increased resilience for critical services might be more efficiently obtained by splitting IT platforms across multiple sites with application-level resilience against individual site loss?",
        "simplifiedQuestion": "Do you build or buy only the level of data centre resilience truly needed by your business, avoiding over-provisioning like unnecessary 2N setups?",
        "reference": "2025 CoC Guide 3.3.1",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_1",
        "recommendationTitle": "Build resilience to business requirements",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Business impact analysis (BIA) documents, data centre design requirements based on BIA, documentation justifying the chosen resilience level (e.g., for a new build or colocation contract). Strategy documents for multi-site application resilience if applicable."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds/colocation procurement: Business requirements documentation, Business Impact Analysis (BIA), and design/procurement specifications that clearly link the chosen resilience level to these justified needs, avoiding default over-provisioning (e.g., automatic 2N).",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of business impact analyses, data centre design documents, colocation RFPs/contracts. Interviews with business continuity planners and data centre designers/procurers.",
        "nonconformity": {
          "major": "New builds or significant colocation agreements default to high resilience levels (e.g., 2N) without documented business justification or impact analysis.",
          "minor": "Business requirements for resilience are informally defined or the justification for the chosen resilience level is weak or poorly documented."
        },
        "majorNonconformity": "For new builds or colocation purchases, deploying high levels of infrastructure resilience (e.g., 2N) without a formal business impact analysis or documented justification linking resilience to specific business requirements.",
        "minorNonconformity": "Resilience levels are chosen based on general assumptions rather than detailed business requirements, or the option of application-level resilience across multiple sites is not adequately considered as an alternative to high site-level resilience.",
        "whatThisMeans": "When building a new data centre or choosing a colocation provider, don't automatically go for the highest level of redundancy (like 2N, meaning everything is duplicated). Instead, first figure out what level of availability your business *really* needs based on a proper risk and business impact analysis. Sometimes, making your applications resilient across different sites is a more efficient way to achieve high availability than overbuilding a single site.",
        "complianceRequires": "- For new builds or colocation purchases, ensure the level of resilience and availability is justified by documented business requirements and a business impact analysis (BIA).\n- Avoid defaulting to high-resilience designs like 2N if not strictly necessary.\n- Consider if increased resilience for critical services can be achieved more efficiently through IT platform distribution across multiple sites and application-level resilience, rather than solely relying on high infrastructure resilience at a single site.",
        "expertTip": "Challenge requests for 2N infrastructure by default. Require a detailed BIA and a cost-benefit analysis that explicitly compares the cost and energy impact of 2N site resilience versus distributed application resilience across less resilient (and less expensive/energy-intensive) sites or zones.",
        "recommendedEvidence": "- Business Impact Analysis (BIA) reports for critical services.\n- Data centre design specifications for new builds clearly stating the target resilience level and its justification based on business requirements.\n- Colocation service agreements or RFPs specifying required resilience levels and the basis for these requirements.\n- Documentation of any analysis comparing single-site high resilience vs. multi-site application resilience strategies.",
        "recommendedActions": "- Implement a formal process requiring a BIA before defining resilience requirements for new data centres or colocation services.\n- Educate business stakeholders on the cost and energy implications of different resilience levels.\n- Explore and document strategies for application-level resilience as a means to achieve service availability goals without over-provisioning site infrastructure.\n- Ensure procurement processes for colocation evaluate providers based on the ability to offer resilience levels matched to actual needs, not just highest availability.",
        "sampleResponseTemplate": "For our new [Project Name/Colocation Contract], the resilience level was determined by a Business Impact Analysis conducted on [Date] (Ref: [BIA Document ID]). This analysis indicated a requirement for [Specific Resilience Level, e.g., N+1 for power, N for cooling], which is reflected in our design/contract. We consciously avoided a 2N design because [Reason, e.g., 'critical applications utilize multi-site failover'].",
        "humanInputAnswer": "Our 'New Data Centre Design Policy (DCDP-001)' mandates that resilience tiers for new builds are directly derived from a formal Business Impact Analysis. For our recently commissioned 'West Wing Expansion', the BIA (Ref: BIA-WW-2023) identified that only 30% of workloads required Tier III equivalent uptime. Consequently, we designed a mixed-resilience facility, avoiding a blanket 2N approach, and deployed critical applications with geo-redundancy across our primary and DR sites.",
        "llmGeneratedAnswer": "Yes, the organization's 'Data Centre Resilience Standard (DCRS-2025)' dictates that all new data centre builds and significant colocation service procurements must be preceded by a comprehensive Business Impact Analysis (BIA) and a formal definition of Business Requirements for Availability (BRA). The DCRS explicitly states that resilience levels (e.g., N, N+1, 2N) must be directly justified by these documented requirements, discouraging default specification of 2N infrastructure. For instance, the 'Project Phoenix BIA & BRA Report (PP-BIA-2024)' informed the N+1 electrical and N mechanical resilience design for the new facility, leveraging application-level redundancy across existing sites for Tier 1 services.",
        "badExampleAnswer": "We always build 2N data centres for maximum reliability because uptime is critical for our business."
      },
      "question_19": {
        "id": 19,
        "old_question_text": null,
        "questionText": "Has the organization, for new builds or retrofits, considered designing and building a single data centre facility to provide multiple, distinct levels of power and cooling resilience to different floor areas or zones within the facility (e.g., offering optional 'grey' power feeds without UPS or generator backup for less critical loads, alongside highly resilient zones for critical IT systems)?",
        "simplifiedQuestion": "For new/retrofitted data centres, do you design for different resilience levels in different areas to match varied needs?",
        "reference": "2025 CoC Guide 3.3.2",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_1",
        "recommendationTitle": "Consider multiple levels of resilience",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Design documents for new builds/retrofits showing zones with different resilience levels. Specifications for power and cooling systems in these different zones. Colocation service offerings detailing varied resilience tiers, if applicable."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits: Design documentation illustrating how the data centre provides multiple levels of power and cooling resilience to different areas/zones, or service offerings from colocation providers detailing such options.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of design documents (floor plans, M&E schematics showing zoned resilience). For colocation providers, review of service level offerings. Site inspection to verify physical implementation of zoned resilience.",
        "nonconformity": {
          "major": "New build/retrofit applies a monolithic high-resilience design across the entire facility, even where varying business needs would permit lower, more efficient resilience in some zones.",
          "minor": "Consideration of multiple resilience levels was superficial, or implementation lacks clear differentiation and optimization for the different zones."
        },
        "majorNonconformity": "For new builds or retrofits, failure to consider or implement designs that offer multiple, distinct levels of power and cooling resilience tailored to different areas, leading to potential over-provisioning for less critical functions.",
        "minorNonconformity": "The concept of multiple resilience levels is acknowledged but not practically implemented in new designs, or the differentiation between zones is minimal and does not yield significant efficiency benefits.",
        "whatThisMeans": "When designing a new data centre or making major upgrades, think about creating different zones with varying levels of power and cooling protection. Not all IT equipment needs the highest (and most expensive/energy-intensive) level of resilience. For example, some areas might have basic power, while others have full UPS and generator backup. This allows you to match the infrastructure cost and energy use to the actual needs of the IT systems in each zone.",
        "complianceRequires": "- For new builds or retrofits, consider building a single data centre to provide multiple levels of power and cooling resilience to different floor areas/zones.\n- Evaluate if different parts of the data centre can operate with different infrastructure resilience (e.g., N, N+1, 2N for power; varying cooling redundancy).\n- This approach can be offered by colocation providers (e.g., optional 'grey' power feeds without UPS/generator backup for non-critical test/dev environments alongside highly resilient production zones).",
        "expertTip": "Clearly map business application criticality to different resilience zones within your data centre design. Use this mapping to justify providing lower (and more energy-efficient) resilience levels for non-critical applications, development/test environments, or batch processing workloads, reserving higher resilience for truly critical systems.",
        "recommendedEvidence": "- Design documents for new data centres showing zoned resilience architectures (e.g., floor plans with demarcated zones, M&E single-line diagrams illustrating different power paths).\n- Colocation provider service brochures or contracts offering tiered resilience options.\n- Specifications for power and cooling infrastructure for different resilience zones.\n- Cost-benefit analysis supporting the decision for a multi-resilience level design.",
        "recommendedActions": "- During the design phase of new data centres or major retrofits, perform an analysis of varying resilience needs across different IT services and plan zones accordingly.\n- Engage with M&E designers to develop efficient ways to deliver multiple resilience levels within a single facility.\n- Colocation providers: Develop service offerings that cater to a spectrum of resilience requirements, clearly articulating the cost and energy implications of each tier.",
        "sampleResponseTemplate": "Our new [Data Centre Name/Project Phase], designed in [Year] and commissioned [Date], incorporates multiple resilience zones. For example, [Zone A] provides [Resilience Level X, e.g., N+1 power and cooling] for critical production systems, while [Zone B] offers [Resilience Level Y, e.g., utility power with basic cooling] for development labs. This is documented in our [Design Specification Document ID].",
        "humanInputAnswer": "Our 'Project Titan' data centre, completed in Q1 2025, was designed with three distinct resilience zones. Zone 1 (Production Critical) is a 2N electrical and N+1 mechanical design. Zone 2 (Production Standard) is N+1 electrical and N+1 mechanical. Zone 3 (Test & Dev) utilizes conditioned utility power ('grey' feeds) with N cooling. This tiered approach, detailed in 'Titan Design Basis Document TDB-001', allowed us to optimize capital and operational costs based on actual business requirements for different workloads.",
        "llmGeneratedAnswer": "Yes, our 'Aurora Data Centre Facility', constructed in 2024, was designed with a zoned resilience architecture. As detailed in the 'Aurora Design & Engineering Specification (ADES-2023)', distinct areas of the data floor are provisioned with varying levels of power and cooling resilience. For instance, Hall A, housing critical financial transaction systems, is built to a 2N electrical and 2(N+1) cooling specification. Conversely, Hall C, designated for development and staging environments, is provided with N+1 electrical (UPS-backed) and N+1 cooling, with an option for 'grey' utility-only power feeds for specific non-critical test benches. This approach avoids universal over-provisioning and aligns infrastructure cost and energy use more closely with the criticality of the hosted IT services.",
        "badExampleAnswer": "All our data centres are built to be highly resilient (2N) because we cannot afford any downtime for any system."
      },
      "question_20": {
        "id": 20,
        "old_question_text": null,
        "questionText": "Has the organization implemented lean provisioning of power and cooling capacity for new builds or retrofits, ensuring that excess capacity is avoided by planning for modular (scalable) expansion and building out this capacity in a rolling program of deployments (e.g., for a maximum of 18 months of data floor capacity at a time)? Does this approach aim to match the technology 'generation' of IT equipment with supporting M&E infrastructure, improving efficiency and responsiveness to business needs?",
        "simplifiedQuestion": "For new/retrofitted data centres, do you avoid overbuilding power/cooling by deploying it in modular stages as needed (e.g., 18-month outlook)?",
        "reference": "2025 CoC Guide 3.3.3",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_1",
        "recommendationTitle": "Lean provisioning of power and cooling for a maximum of 18 months of data floor capacity",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Data centre master plan showing modular design and phased build-out strategy. Capacity planning documents linking M&E deployment to IT demand forecasts (e.g., 18-month rolling forecast). Specifications for modular M&E components."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Documentation of a modular/scalable design approach with phased deployment of power and cooling capacity aligned with demand forecasts (e.g., 18-month capacity outlook).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of data centre design master plans, capacity planning methodology, and M&E deployment schedules. Site inspection to observe evidence of modular infrastructure and phased build-out.",
        "nonconformity": {
          "major": "New builds/retrofits deploy full day-one power and cooling capacity for the entire facility shell, leading to significant fixed losses and inefficient operation at low initial IT loads.",
          "minor": "Modular design is considered but deployment phases are excessively long (e.g., >24 months of capacity deployed at once), or M&E technology is not well-matched with IT refresh cycles."
        },
        "majorNonconformity": "For new builds or retrofits, deploying power and cooling infrastructure for the full ultimate capacity of the facility from day one, rather than using a modular, phased approach.",
        "minorNonconformity": "A modular approach is used, but capacity deployments significantly exceed an 18-month demand forecast, or there's a mismatch between the 'generation' of M&E and IT equipment being deployed.",
        "whatThisMeans": "When building or upgrading a data centre, instead of installing all the power and cooling equipment for the entire facility's ultimate capacity right from the start, you should plan for it to be modular and scalable. This means you build out capacity in stages (e.g., only enough for the next 18 months of expected IT growth). This avoids having lots of expensive equipment running inefficiently at low loads and allows you to use newer, more efficient M&E technology as you expand, matching it with the IT equipment of that time.",
        "complianceRequires": "- Plan data centres for modular (scalable) expansion of power and cooling capacity.\n- Avoid provisioning excess power and cooling capacity from day one, which drives substantial fixed losses.\n- Build out capacity in a rolling program of deployments, ideally matching a forecast of no more than 18 months of data floor capacity needs at a time.\n- Aim to match the technology 'generation' of the deployed IT equipment with that of the supporting M&E infrastructure to improve both efficiency and responsiveness.",
        "expertTip": "Use a 'right-sized' modular approach not just for large M&E plant like chillers or UPS systems, but also for distribution within the data hall (e.g., modular busway, row-based cooling). This allows capacity to be added incrementally, closely matching demand and maximizing operational efficiency at all stages of the data centre's lifecycle.",
        "recommendedEvidence": "- Data centre master plan or design documents illustrating modular design and phased deployment of M&E capacity.\n- Capacity planning methodology and forecasts (e.g., 18-month rolling IT demand forecast).\n- Procurement records showing phased purchase and installation of M&E equipment.\n- Specifications for modular power and cooling components used.",
        "recommendedActions": "- Adopt a modular and scalable design philosophy for all new data centre builds and major retrofits.\n- Develop accurate IT demand forecasting (e.g., for 12-18 month horizons) to guide phased M&E capacity deployment.\n- Select M&E equipment that supports modularity and efficient operation at varying load levels.\n- Align M&E deployment schedules with IT refresh and expansion plans.",
        "sampleResponseTemplate": "Our new data centre, [Facility Name], employs a modular design for power and cooling, as detailed in our [Master Plan Document ID]. We deploy M&E capacity in phases, targeting an [e.g., 18-month] IT load forecast. The current phase, completed [Date], supports [X kW] of IT load, with the next phase planned for [Date] based on current growth projections. This allows us to utilize the latest M&E technology aligned with our IT deployments.",
        "humanInputAnswer": "Our 'Data Centre Build Standard DC-STD-BLD-007' mandates a modular, 'just-in-time' approach for power and cooling deployment in all new facilities. For our 'North Campus Phase 2' build, we deployed initial power and cooling infrastructure to support 500kW of IT load, representing our projected 18-month requirement. Subsequent 250kW modules of power (modular UPS) and cooling (in-row coolers) will be added as demand materializes, with provisions made in the initial shell construction for this expansion.",
        "llmGeneratedAnswer": "Yes, our data centre expansion strategy, outlined in the 'DC Capacity Roadmap 2025-2030', explicitly adopts lean provisioning. New data hall fit-outs are designed with modular power and cooling infrastructure, deployed in phases. Each phase is sized to accommodate a maximum of 18 months of projected IT load growth, based on our 'IT Demand Forecast Model'. For example, Data Hall 4, commissioned in Q1 2025, was equipped with 1MW of power and cooling capacity, with an additional 1MW module scheduled for Q3 2026. This approach minimizes initial capital outlay, reduces fixed energy losses from underutilized M&E plant, and allows us to deploy newer generation, more efficient M&E equipment in subsequent phases, aligning with IT hardware refresh cycles.",
        "badExampleAnswer": "We built our data centre with enough power and cooling to support its full capacity for the next 10 years, so we don't have to worry about upgrades for a long time."
      },
      "question_21": {
        "id": 21,
        "old_question_text": null,
        "questionText": "Has the organization ensured that for new builds or retrofits, all areas of the data centre (including power and cooling infrastructure) are designed to maximise energy efficiency under partial fill/partial load conditions and variable IT electrical and cooling loads? Does this include consideration for the dynamic response of the infrastructure to changing loads, such as using appropriately controlled Variable Frequency (or speed) Drives for pumps, fans, and compressors?",
        "simplifiedQuestion": "For new/retrofitted data centres, is your infrastructure designed to be energy efficient even when not fully loaded or when IT loads fluctuate?",
        "reference": "2025 CoC Guide 3.3.4",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_1",
        "recommendationTitle": "Design infrastructure to maximise part load efficiency",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Design documents for M&E systems showing part-load efficiency considerations. Specifications for equipment with variable speed drives (pumps, fans, compressors). Control system philosophy detailing how dynamic loads are managed for efficiency."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits: Design documentation detailing how power and cooling infrastructure is optimized for part-load efficiency and variable IT loads. This should include specifications for variable speed drives and control strategies for dynamic load response.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Review"
        ],
        "verificationMethods": "Review of M&E design documents, equipment specifications (e.g., VFDs), and control system logic. Inspection of installed equipment and review of operational data showing performance under varying loads.",
        "nonconformity": {
          "major": "New build/retrofit M&E infrastructure is designed primarily for peak load efficiency with little or no consideration for part-load or variable load efficiency, leading to significant inefficiencies during typical operation.",
          "minor": "Some consideration for part-load efficiency exists, but it's not systematically applied across all major systems, or control strategies for dynamic loads are not optimized."
        },
        "majorNonconformity": "For new builds or retrofits, infrastructure design primarily focuses on full-load efficiency without adequate measures to ensure efficient operation under common partial load or variable IT load conditions.",
        "minorNonconformity": "Part-load efficiency is considered for some components (e.g., UPS) but not holistically across the power and cooling chain, or variable speed drives are installed but not optimally controlled for dynamic IT loads.",
        "whatThisMeans": "When designing a new data centre or doing a major upgrade, it's crucial that your power and cooling systems are efficient not just when they're running at full capacity, but also when they're only partially loaded or when the IT workload changes frequently. This often involves using equipment like pumps, fans, and compressors that can adjust their speed (using Variable Frequency Drives - VFDs) and have smart controls to respond to these dynamic conditions efficiently.",
        "complianceRequires": "- Design all areas of the data centre to maximize energy efficiency under partial fill/partial load conditions.\n- Ensure efficiency is maintained under variable IT electrical and cooling loads.\n- This is in addition to modular provisioning (3.3.3) and should consider the dynamic response of infrastructure.\n- Implement appropriately controlled Variable Frequency (or speed) Drives (VFDs/VSDs) for components like pumps, fans, and compressors where load varies.",
        "expertTip": "Specify M&E equipment with high efficiency across a broad range of its operational capacity (e.g., 20% to 100% load), not just at its peak design point. For systems with multiple components (like a chiller plant with multiple chillers and pumps), ensure the control system is designed to stage equipment effectively to keep operating units in their most efficient range.",
        "recommendedEvidence": "- Design documents explicitly stating part-load efficiency targets and strategies.\n- Specifications for M&E equipment showing efficiency curves across different load levels.\n- List and specifications of installed VFDs/VSDs on pumps, fans, compressors.\n- Control system sequence of operations detailing how variable loads are managed and how VFDs are controlled.\n- Commissioning reports verifying part-load performance of systems.",
        "recommendedActions": "- During the design phase, specify equipment with high part-load efficiency ratings.\n- Incorporate VFDs/VSDs for major motors in variable load applications (fans, pumps, compressors).\n- Develop and implement advanced control strategies that optimize system performance across varying IT loads.\n- Regularly review operational data to ensure systems are effectively adapting to load changes and maintaining efficiency.",
        "sampleResponseTemplate": "Our new [Facility Wing/Data Hall] was designed to maximize part-load efficiency. Key M&E systems like [Chillers/Pumps/CRACs] use variable speed drives, as specified in [Design Document ID]. Our BMS control strategy (Ref: [Control Doc ID]) dynamically adjusts [e.g., fan speeds, pump speeds, chiller staging] based on real-time IT load and environmental conditions to maintain optimal efficiency across varying loads.",
        "humanInputAnswer": "Our 'DC M&E Design Standard ME-DS-002' mandates that all new cooling and power systems must demonstrate high efficiency down to 20% of their rated load. For our 'Alpha Site Expansion', all CRAH fans, chilled water pumps, and chiller compressors are equipped with VFDs. The BMS control sequences, documented in 'Alpha_BMS_Controls_v1.2', utilize predictive load following and dynamic staging to ensure equipment operates in its peak efficiency band, even under fluctuating IT demands.",
        "llmGeneratedAnswer": "Yes, for our 'Phoenix Project' retrofit completed in 2024, a primary design objective was maximizing part-load efficiency. All cooling distribution pumps and CRAC unit fans were specified with Variable Frequency Drives (VFDs), as detailed in 'Phoenix M&E Specification PME-SPEC-001'. The control system, outlined in 'Phoenix BMS Operational Logic PBL-003', employs demand-based staging for chillers and VFD control based on differential pressure and temperature feedback to ensure that both power and cooling infrastructure dynamically adjust to variable IT loads (typically ranging from 30% to 75% of design capacity) while maintaining high operational efficiency.",
        "badExampleAnswer": "Our systems are designed for our maximum expected IT load, so they are very efficient when we are busy."
      },
      "question_22": {
        "id": 22,
        "old_question_text": null,
        "questionText": "Has the organization, for new builds or retrofits, utilized appropriate levels of resilience within the data centre IT equipment, software, and network levels to achieve the service resilience and availability required by business demands, thereby avoiding the energy inefficiencies associated with installing excessively high levels of infrastructure resilience (e.g., multiple redundant M&E units that reduce overall energy efficiency)?",
        "simplifiedQuestion": "For new/retrofitted data centres, do you use appropriate resilience at IT/software/network levels to meet business needs, rather than just overbuilding infrastructure resilience?",
        "reference": "2025 CoC Guide 3.3.5",
        "priority": "High (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_1",
        "recommendationTitle": "Consider appropriate levels of resilience",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx"
          ],
          "description": "Resilience strategy documents, IT architecture designs showing software/application/network level resilience. Justification for the chosen balance between infrastructure resilience and IT/software/network resilience."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Documentation outlining the overall resilience strategy, showing how IT, software, and network-level resilience contribute to service availability, and how this influences the design of infrastructure resilience to avoid over-provisioning.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of overall resilience strategy, BIA, IT architecture documents, and M&E design documents. Interviews with IT architects, business continuity planners, and data centre designers.",
        "nonconformity": {
          "major": "New build/retrofit defaults to high infrastructure resilience (e.g., 2N M&E) without considering or leveraging IT/software/network resilience capabilities to achieve service availability, leading to significant energy inefficiency.",
          "minor": "Some consideration is given to IT/software/network resilience, but infrastructure resilience levels are still higher than demonstrably needed for the achieved service availability, or the balance is not well-documented."
        },
        "majorNonconformity": "For new builds or retrofits, primarily relying on high levels of redundant M&E infrastructure for service availability without sufficiently leveraging IT equipment, software, and network-level resilience strategies, leading to avoidable energy inefficiencies.",
        "minorNonconformity": "While IT/software/network resilience is utilized to some extent, the chosen M&E infrastructure resilience level is still excessive for the actual business requirements, or the strategy for balancing these different layers of resilience is not clearly documented or optimized for energy efficiency.",
        "whatThisMeans": "When designing new data centres or upgrading existing ones, instead of just building in lots of backup power and cooling equipment (which can be inefficient), your organization should also focus on making the IT equipment itself, the software, and the network more resilient. This means designing systems that can handle failures at the IT layer, which can help you achieve the required service uptime without needing as much redundant, energy-consuming infrastructure.",
        "complianceRequires": "- Utilize appropriate levels of resilience at multiple layers: data centre infrastructure (M&E), IT equipment, software, and network levels.\n- Ensure the combination of these resilience layers achieves the service resilience and availability required by documented business demands.\n- Avoid installing unnecessarily high levels of infrastructure resilience (e.g., multiple redundant M&E units running inefficiently at low loads) if service availability can be met through more efficient IT-layer resilience strategies.\n- This practice aims to reduce overall energy inefficiency caused by over-provisioned redundant infrastructure.",
        "expertTip": "Promote an 'active-active' or 'N+1 active' design philosophy at the IT application and platform level where feasible. Architecting services to run concurrently across multiple servers or even sites, with automated failover, can often provide higher service availability with more energy-efficient underlying infrastructure (e.g., N+1 power and cooling) compared to relying on passive standby 2N M&E systems.",
        "recommendedEvidence": "- Overall service availability and resilience strategy documents.\n- Business Impact Analysis defining RTO/RPO for key services.\n- IT architecture diagrams and descriptions showing how IT equipment, software (e.g., clustering, load balancing, database replication), and network resilience contribute to service availability.\n- M&E design documents for new builds/retrofits, with justification for the selected infrastructure resilience level in context of IT-layer resilience.\n- Analysis comparing the energy efficiency of different resilience strategies (e.g., 2N infrastructure vs. N+1 infrastructure with application-level resilience).",
        "recommendedActions": "- Integrate IT architects, application owners, and infrastructure designers in resilience planning from the outset.\n- Formally evaluate the contribution of IT/software/network resilience capabilities when determining infrastructure resilience requirements.\n- Prioritize designs that achieve business availability requirements through the most energy-efficient combination of infrastructure and IT-layer resilience.\n- Document the rationale for selected resilience levels across all layers.",
        "sampleResponseTemplate": "Our resilience strategy for new deployments, documented in '[Strategy Document ID]', focuses on achieving service availability through a combination of infrastructure and IT-level resilience. For [Application X], we utilize [e.g., N+1 infrastructure] coupled with [e.g., software-defined load balancing and database clustering across multiple compute nodes]. This approach, detailed in '[App X Architecture Doc]', meets our RTO/RPO of [Values] more efficiently than relying solely on 2N infrastructure.",
        "humanInputAnswer": "Our 'Service Resilience Framework (SRF-003)' for new applications mandates a design approach that leverages IT-level resilience (e.g., application clustering, stateless services, geo-replication for data) first, to achieve business RTO/RPO targets. This allows us to deploy these services on our 'Standard Resilience Platform' (N+1 power and cooling infrastructure), which is significantly more energy-efficient than our legacy 'High Resilience Platform' (2N). Only applications with specific, unmeetable dependencies on physical infrastructure resilience are considered for the 2N platform, with documented justification.",
        "llmGeneratedAnswer": "Yes, our 'Resilience Design Standard (RDS-2024)' mandates a holistic approach to service availability for all new and retrofitted environments. Section 3.1 of RDS-2024 emphasizes achieving business-defined RTOs/RPOs through an optimal mix of infrastructure, IT hardware, software, and network resilience. For example, our new 'Customer Portal Platform' leverages Kubernetes for application-level fault tolerance and active-active database replication across two N+1 power zones, allowing the supporting infrastructure to be designed with N+1 mechanical cooling rather than a full 2N. This reduced the number of redundant cooling units by two, improving overall plant efficiency by an estimated 8% compared to a traditional 2N M&E design for the same service availability target.",
        "badExampleAnswer": "We always use 2N for everything in our new data centres because our customers expect maximum uptime, and we can't risk IT or software failures causing an outage."
      }
    }
  },
  "step_2": {
    "description": "2025 Guide Section 4: IT Equipment and Services (Practices 4.1.1 through 4.1.10 - Selection and Deployment of New IT Equipment, Part 1)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_IT_Equip_SVC_1"
    ],
    "critical_questions": [],
    "questions": {
      "question_23": {
        "id": 23,
        "old_question_text": null,
        "questionText": "Has the organization made the energy efficiency performance of new IT devices a high priority decision factor in the tender process? Is this demonstrated through the use of recognized metrics like SERT (Standard Performance Evaluation Corporation's Server Efficiency Rating Tool) or SPECPower, or through specific user metrics aligned with the target environment (including service level or reliability components)? Is the power consumption of the device at its expected utilization or applied workload considered, in addition to peak performance per Watt figures, referencing standards like ISO/IEC 30134-4 (ITEEsv) and ETSI EN 303 470?",
        "simplifiedQuestion": "When buying new IT equipment, is energy efficiency a major deciding factor, using standard tests and considering real-world usage?",
        "reference": "2025 CoC Guide 4.1.1",
        "priority": "High (Value: 5, Expected: New IT Equipment)",
        "stepSection": "step_2",
        "recommendationTitle": "IT hardware - Power",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Procurement policies for IT hardware, tender evaluation criteria, examples of tender documents specifying energy efficiency requirements (SERT, SPECPower, ITEEsv), and analysis of IT device power consumption at expected workloads."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT equipment procurement: Procurement policies, tender documents, and evaluation matrices showing that energy efficiency (e.g., SERT/SPECPower scores, ITEEsv, ETSI EN 303 470) is a high-priority selection criterion, including consideration of power use at expected utilization levels.",
        "verification_method": [
          "Document Review",
          "Process Verification"
        ],
        "verificationMethods": "Review of IT procurement policies, tender evaluation criteria, and recent tender examples. Interviews with procurement and IT staff involved in hardware selection.",
        "nonconformity": {
          "major": "Energy efficiency is not a significant or high-priority criterion in the IT hardware tender and selection process for new equipment.",
          "minor": "Energy efficiency is considered, but not as a high priority, or standard metrics/real-world workload consumption are not consistently evaluated."
        },
        "majorNonconformity": "Failure to include energy efficiency performance (using recognized metrics like SERT/SPECPower or ITEEsv) as a high-priority decision factor in the tender process for new IT equipment.",
        "minorNonconformity": "Energy efficiency is considered but given low weighting in procurement decisions, or evaluation focuses solely on peak performance per Watt without considering power consumption at expected operational utilization levels.",
        "whatThisMeans": "When your organization buys new IT equipment (like servers), its energy efficiency should be a very important factor in the decision. This means using standard industry tests (like SERT or SPECPower) or your own tests that reflect how you'll actually use the equipment. You need to look at power use for your typical workloads, not just the best-case \"power per watt\" figures, and be aware of standards like ITEEsv and ETSI EN 303 470.",
        "complianceRequires": "- Include energy efficiency performance of IT devices as a high priority decision factor in tender processes for new IT equipment.\n- Utilize recognized metrics such as SERT (SPEC Server Efficiency Rating Tool) or SPECPower, or specific user-defined metrics relevant to the target operational environment (which may include service level or reliability aspects).\n- Consider power consumption at the *expected utilization level or applied workload*, not just peak performance per Watt figures.\n- Reference relevant standards such as ISO/IEC 30134-4 (ITEEsv - IT Equipment Energy Efficiency for servers) and ETSI EN 303 470 (Energy Efficiency measurement for servers).",
        "expertTip": "Develop a scoring matrix for IT procurement that assigns a significant weight (e.g., 20-30%) to energy efficiency metrics. Ensure that an apples-to-apples comparison of energy consumption is made based on *your* anticipated workloads and utilization profiles, not just vendor-supplied peak efficiency data.",
        "recommendedEvidence": "- IT procurement policy explicitly stating energy efficiency as a high-priority criterion.\n- Tender documents (RFPs) for recent IT purchases showing requirements for SERT/SPECPower scores, ITEEsv data, or other energy efficiency metrics.\n- Evaluation spreadsheets or reports from recent procurements demonstrating how energy efficiency was weighted and factored into the selection decision.\n- Documentation of how expected utilization levels were considered in evaluating power consumption.",
        "recommendedActions": "- Update IT procurement policies to mandate energy efficiency as a key selection criterion with significant weighting.\n- Specify required energy efficiency metrics (SERT, SPECPower, ITEEsv) in all relevant tenders for new IT equipment.\n- Train procurement and IT staff on evaluating and comparing energy efficiency data from vendors.\n- Develop internal methods for estimating power consumption under expected operational workloads if standardized metrics are insufficient.",
        "sampleResponseTemplate": "Our IT procurement policy ([Policy ID]) mandates energy efficiency as a high-priority (weighted at [X]%) selection factor for all new servers, referencing [SERT/SPECPower/ITEEsv] scores. For our recent [Server Type] purchase, we evaluated bids based on [Metric] at an expected [Y]% utilization, as detailed in '[Evaluation Report ID]'.",
        "humanInputAnswer": "As per our 'Sustainable IT Procurement Policy (ITP-004)', energy efficiency is a mandatory high-priority criterion, accounting for 25% of the total score in server tenders. We require vendors to provide SERT scores and power consumption data at 20%, 50%, and 80% utilization levels. For our Q1 2025 server refresh, the winning bid demonstrated a 15% better SERT score and 10% lower projected operational power consumption compared to the next best offer. ITEEsv data is also requested.",
        "llmGeneratedAnswer": "Yes, energy efficiency is a primary decision factor in our IT hardware procurement process, as stipulated in the 'IT Hardware Acquisition Standard (ITAS-2025)'. Section 3.2 requires that all server tenders include SPEC SERT results and data compliant with ISO/IEC 30134-4 (ITEEsv). Furthermore, our evaluation methodology, detailed in 'Tender Evaluation Protocol (TEP-005)', assigns a 30% weighting to energy efficiency, which includes an assessment of power consumption at projected typical workloads (40-60% utilization) in addition to peak performance/watt figures. For example, our recent procurement of virtualization hosts (Ref: PO-2025-042) prioritized solutions with superior SERT scores and lower consumption profiles under simulated mixed-workload conditions.",
        "badExampleAnswer": "We mainly look at performance and price when buying new servers, but we ask for Energy Star ratings if possible."
      },
      "question_24": {
        "id": 24,
        "old_question_text": null,
        "questionText": "For new IT hardware, if equipment cannot be procured that meets the expected operating temperature and humidity range of ASHRAE Class A2 (as per Practice 4.1.3), does the organization ensure that any purchased equipment supports at least ASHRAE Class A1? Is such equipment with more restrictive environmental needs (legacy operating range) installed in a separate, appropriately controlled area of the data floor to facilitate segregation, as described in Practices 5.1.11, 5.3.4, and 5.3.5, especially in cases where older technology equipment must be procured due to compatibility or specific application validation requirements?",
        "simplifiedQuestion": "If new IT gear can't handle wider A2 ASHRAE ranges, do you ensure it meets at least A1 and install it in a separate, suitable area?",
        "reference": "2025 CoC Guide 4.1.2",
        "priority": "High (Value: 4, Expected: New IT Equipment)",
        "stepSection": "step_2",
        "recommendationTitle": "New IT hardware - Restricted (legacy) operating temperature and humidity range",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Procurement specifications for new IT hardware indicating accepted ASHRAE class. Design documents or operational procedures for segregated environmental zones if A1 (but not A2) equipment is deployed. Justification for procuring equipment not meeting A2."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory (if applicable): IT hardware specifications showing required ASHRAE Class (A1 minimum if A2 not met). Documentation of segregated zones for such equipment (Practices 5.1.11, 5.3.4, 5.3.5). Justification for not meeting A2.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of IT procurement specifications, datasheets for newly acquired hardware, and data centre design/zoning plans. Inspection of any segregated areas for equipment with restricted environmental ranges.",
        "nonconformity": {
          "major": "New IT hardware is procured that does not meet at least ASHRAE Class A1, or A1 equipment is deployed without appropriate segregation and environmental control, compromising overall data centre efficiency.",
          "minor": "New IT hardware meeting only A1 (not A2) is procured without clear justification or optimally designed segregated zones."
        },
        "majorNonconformity": "Procurement of new IT hardware that does not meet minimum ASHRAE Class A1 environmental specifications, or deployment of such A1-only equipment within general A2 (or wider) operational zones without segregation, thereby dictating tighter environmental controls for the entire area.",
        "minorNonconformity": "New IT hardware supporting only ASHRAE Class A1 is deployed, but the segregated area is not optimally designed or controlled, or justification for not selecting A2-compliant equipment is weak.",
        "whatThisMeans": "When buying new IT equipment, it should ideally operate within the ASHRAE Class A2 temperature and humidity ranges, allowing for more energy-efficient data centre environments. If you absolutely must buy new equipment that can't meet A2, it must at least meet Class A1. This more sensitive equipment should then be put in a separate part of your data centre with its own environmental controls so it doesn't force the whole data centre to run less efficiently.",
        "complianceRequires": "- For new IT hardware, prioritize equipment that allows operation within ASHRAE Class A2 (as per Practice 4.1.3).\n- If A2-compliant equipment cannot be procured (e.g., due to compatibility, specific application validation for older tech), ensure that purchased equipment supports a minimum of ASHRAE Class A1.\n- Install any new equipment supporting only ASHRAE Class A1 (or other restrictive ranges) in a separate area of the data floor.\n- This segregation should facilitate tighter environmental controls specifically for this equipment, as detailed in Practices 5.1.11 (Equipment segregation), 5.3.4 (Separate environmental zones - new build), and 5.3.5 (Separate environmental zones - colocation/MSP).\n- Document justification for procuring equipment not meeting the A2 standard.",
        "expertTip": "Maintain a clear inventory of IT equipment environmental specifications. When planning new deployments or hardware refreshes, use this inventory to identify equipment with restrictive ranges. Proactively design or allocate segregated zones with appropriate, independent environmental controls to house such equipment, preventing it from limiting the operational flexibility and efficiency of the broader data centre environment.",
        "recommendedEvidence": "- IT procurement policy specifying minimum acceptable ASHRAE environmental classes for new hardware.\n- Datasheets for recently procured IT equipment indicating their supported ASHRAE class.\n- Data centre floor plans or design documents showing designated zones for equipment with restricted (e.g., A1-only) environmental requirements, if applicable.\n- Documentation justifying the procurement of any new equipment that does not meet ASHRAE Class A2.",
        "recommendedActions": "- Update IT procurement standards to mandate ASHRAE Class A2 as the default requirement for new equipment.\n- Establish a clear exception process with justification requirements for procuring equipment that only meets Class A1.\n- If such equipment is procured, integrate its placement into data centre zoning plans, ensuring it's housed in an area with appropriate, isolated environmental controls.\n- Regularly review the necessity for maintaining equipment with restricted environmental ranges.",
        "sampleResponseTemplate": "Our standard for new IT hardware is ASHRAE Class A2. In rare cases where specific legacy application compatibility requires equipment only meeting Class A1 (e.g., [System XYZ]), it is deployed in our 'Legacy Support Zone' (Data Hall 3, Zone B), which has independent environmental controls set to [Specific A1 Range]. Justification for such procurement is documented via [Exception Process Document ID].",
        "humanInputAnswer": "Our default procurement standard (IT-PROC-002) requires all new servers and storage to be ASHRAE A2 compliant. For a recent specialized telemetry system that was only available with an A1 environmental rating (System Model ABC-123), we obtained a formal waiver (WVR-2025-007) and installed it within our 'Specialized Equipment Pod'. This pod has dedicated, tighter environmental controls (18-22\u00b0C, 40-55% RH) separate from our main data halls which operate at higher A2-range temperatures.",
        "llmGeneratedAnswer": "Yes, our IT procurement policy (Ref: IT-HW-STD-2025, Section 4.1) mandates ASHRAE Class A2 as the minimum acceptable environmental operating range for all new IT hardware. In instances where mission-critical legacy applications necessitate the procurement of specific hardware not meeting A2 (but certified for at least A1, e.g., certain specialized controllers), such equipment is deployed into a dedicated 'Restricted Environment Zone' (REZ). The REZ (detailed in 'DC Floor Plan Rev 7.2, Zone D') is physically segregated and equipped with independent environmental controls tailored to A1 specifications, ensuring it does not compromise the wider A2-optimized environment of the main data halls, consistent with CoC practices 5.1.11 and 5.3.4.",
        "badExampleAnswer": "We buy the equipment needed for the job. If it needs special conditions, we adjust the room cooling."
      },
      "question_25": {
        "id": 25,
        "old_question_text": null,
        "questionText": "Has the organization made the operating temperature and humidity ranges at the air intake of new IT equipment high priority decision factors in the tender process? Is it required that this equipment can operate and be within warranty for the full ASHRAE Class A2 allowable temperature and humidity range? Are vendors required to publish (not simply make available on request) any restriction to operating hours within this range for any model that restricts warranty to less than continuous operation within the A2 allowable range? (Exclusions for legacy/A1 equipment and liquid-cooled devices are addressed in Practices 4.1.2 and 4.1.14 respectively).",
        "simplifiedQuestion": "When buying new IT gear, do you prioritize and require it to safely operate within the full ASHRAE A2 temperature/humidity range, with clear vendor publication of any limitations?",
        "reference": "2025 CoC Guide 4.1.3",
        "priority": "High (Value: 5, Expected: New IT Equipment)",
        "stepSection": "step_2",
        "recommendationTitle": "New IT hardware - Expected operating temperature and humidity range",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "IT procurement policies, tender documents specifying ASHRAE A2 requirements, vendor documentation/datasheets confirming A2 compliance and any warranty limitations, internal server hardware standards."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT equipment: Procurement documents (RFPs, policies) explicitly requiring new IT hardware to operate within the full ASHRAE Class A2 allowable range under warranty. Evidence of vendor publication of any operational hour restrictions within this range.",
        "verification_method": [
          "Document Review",
          "Vendor Documentation Review"
        ],
        "verificationMethods": "Review of IT procurement policies and recent tender documents for ASHRAE A2 requirements. Examination of vendor datasheets and warranty statements for selected new IT equipment.",
        "nonconformity": {
          "major": "New IT equipment is routinely procured without the requirement for operation within the full ASHRAE Class A2 range, or vendors do not publish restrictions on continuous operation within this range.",
          "minor": "ASHRAE A2 requirement is stated but not consistently enforced as a high priority, or vendor transparency on operational restrictions within A2 is lacking."
        },
        "majorNonconformity": "Failure to mandate ASHRAE Class A2 operational capability (or higher) as a high-priority requirement for new IT equipment, or accepting equipment with unpublished/unclear warranty restrictions within the A2 range.",
        "minorNonconformity": "The requirement for ASHRAE Class A2 is present but not consistently weighted as a high priority in procurement, or the process for verifying vendor publication of operational restrictions is weak.",
        "whatThisMeans": "When you buy new IT equipment, it's critical that it can reliably operate within the full ASHRAE Class A2 temperature and humidity range (which allows for more energy-efficient data centre cooling). This should be a top priority in your purchasing decisions. Vendors must also be upfront and publish any limitations if their equipment cannot run continuously within this A2 range without affecting the warranty. (This doesn't apply to genuinely unavoidable legacy-compatible A1 gear or direct liquid-cooled equipment, which have their own rules).",
        "complianceRequires": "- Make the operating temperature and humidity ranges at the air intake of new IT equipment high-priority decision factors in tenders.\n- Require new IT equipment to be able to operate and be within warranty for the *full* ASHRAE Class A2 allowable temperature and humidity range.\n- Mandate that vendors *publish* (not just provide on request) any restrictions on operating hours within the A2 range if their equipment's warranty is limited to less than continuous operation within that range.\n- This practice applies broadly, with specific exclusions for already defined restrictive/legacy needs (Practice 4.1.2) and direct liquid-cooled IT devices (Practice 4.1.14).",
        "expertTip": "In your RFPs for new IT equipment, explicitly require vendors to state their equipment's continuous operational capability across the entire ASHRAE A2 allowable range and to provide direct links to published documentation confirming this and detailing any warranty implications or time-limited operational restrictions within that range. Make compliance with this a heavily weighted evaluation criterion.",
        "recommendedEvidence": "- IT procurement policy specifying ASHRAE Class A2 operational range as a high-priority requirement.\n- Standard RFP/tender language for IT equipment explicitly requiring A2 compliance and vendor publication of any operational restrictions within A2.\n- Examples of vendor responses or datasheets for recently procured equipment clearly stating A2 compliance and any limitations.\n- Internal server/storage hardware standards specifying A2 operational requirements.",
        "recommendedActions": "- Update IT procurement policies and tender templates to mandate full ASHRAE Class A2 operational capability (under warranty, continuous operation) as a high-priority criterion.\n- Develop a checklist item for procurement evaluations to verify vendor publication of any A2 operational restrictions.\n- Train procurement staff on the importance of this requirement and how to verify vendor compliance.\n- Engage with preferred IT vendors to communicate this expectation clearly for future hardware generations.",
        "sampleResponseTemplate": "Our standard IT procurement specification ([Doc ID: IT-HW-SPEC-A2]) mandates that all new servers, storage, and network equipment must be warranted for continuous operation within the full ASHRAE Class A2 allowable environmental range. Tender evaluations heavily prioritize vendors who explicitly publish this capability. For example, our Q1 2025 server purchase (Vendor X, Model Y) documentation confirms full A2 compliance without operational hour restrictions (see attached datasheet excerpt).",
        "humanInputAnswer": "Our 'Server Hardware Standard Procurement Policy (SHSPP-2024)' mandates that all new server purchases must be capable of continuous, warranted operation across the entire ASHRAE Class A2 allowable temperature and humidity range. Our RFPs explicitly require vendors to confirm this and provide links to publicly accessible documentation detailing any operational de-rating or time limitations within the A2 envelope. For our primary server vendor, their [Vendor Model Series] datasheets confirm continuous A2 operation, which was a key factor in their selection.",
        "llmGeneratedAnswer": "Yes, the organization's 'IT Equipment Procurement Standard (IT-PROC-STD-003, Rev. 2025)' explicitly designates the ability of new IT hardware to operate continuously within the full ASHRAE Class A2 allowable environmental range (as defined in the current ASHRAE Thermal Guidelines) as a 'High Priority' selection criterion. Section 4.1.3 of this standard requires that tender submissions include vendor-published documentation confirming such capability and explicitly stating any warranty limitations or operational hour restrictions if continuous operation across the entire A2 range is not supported. This requirement is verified during the technical evaluation phase of all new IT hardware acquisitions, excluding specialized equipment covered under practices 4.1.2 or 4.1.14.",
        "badExampleAnswer": "We expect our new servers to work in our data centre environment, which is generally kept cool. We haven't specifically asked about ASHRAE A2 for all equipment."
      },
      "question_26": {
        "id": 26,
        "old_question_text": null,
        "questionText": "When procuring new IT equipment, does the organization consider as a high priority decision factor equipment that can operate under a wider range of intake temperature and humidity, such as that defined in ASHRAE Class A4 (broadly equivalent to ETSI EN 300 019-1-3 Class 3.1), to potentially eliminate the capital cost of providing mechanical cooling capability in some hotter climate regions? Are vendors requested to supply the actual supported environmental range if their equipment exceeds standard classes like A4?",
        "simplifiedQuestion": "When buying new IT gear, do you prioritize equipment that can handle wider temperature/humidity ranges (e.g., ASHRAE A4) to reduce cooling needs?",
        "reference": "2025 CoC Guide 4.1.4",
        "priority": "Medium (Value: 3, Expected: Optional Practice)",
        "stepSection": "step_2",
        "recommendationTitle": "New IT hardware - Extended operating temperature and humidity range",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "IT procurement policies that include consideration of extended environmental ranges (e.g., ASHRAE A4). Vendor datasheets for equipment selected based on extended range capabilities. Analysis or justification for selecting such equipment."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: IT procurement policies showing preference for extended environmental range equipment (e.g., ASHRAE A4). Datasheets of equipment selected for these capabilities. Documentation of any realized energy/cost savings.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of IT procurement policies and specific tender evaluations where extended environmental range was a factor. Interviews with IT procurement and data centre design staff.",
        "nonconformity": {
          "major": null,
          "minor": "Opportunities to utilize IT equipment with extended environmental ranges (e.g., A4) are not considered in procurement, even if climate conditions might allow for reduced mechanical cooling (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If relevant conditions exist and the practice is adopted: Failure to actively consider or prioritize IT hardware capable of wider environmental ranges (like ASHRAE A4) in procurement, thus missing potential opex/capex savings on cooling.",
        "whatThisMeans": "When buying new IT equipment, your organization should consider if it can operate in even wider temperature and humidity ranges, like those defined by ASHRAE Class A4. Using such equipment might allow you to reduce or even eliminate expensive mechanical cooling systems in certain climates, saving significant energy and cost. You should also ask vendors for the exact supported ranges if they are better than standard classes.",
        "complianceRequires": "- Include the operating temperature and humidity ranges at the air intake of new equipment as high-priority decision factors in the tender process.\n- Specifically consider equipment that operates under a wider range of intake temperature and humidity, such as defined in ASHRAE Class A4 (or ETSI EN 300 019-1-3 Class 3.1).\n- Recognize that this extended range can allow operators to eliminate the capital cost of mechanical cooling in some hotter climates.\n- Request vendors to provide the actual supported environmental range for their equipment, especially if it exceeds standard classes, to identify opportunities for additional energy or cost savings.",
        "expertTip": "For facilities in suitable climates, procuring IT equipment compliant with ASHRAE A3 or A4 can significantly reduce or eliminate reliance on mechanical refrigeration, leading to substantial PUE improvements and operational cost savings. Ensure your facility design can leverage these wider ranges (e.g., through robust air filtration for direct air cooling).",
        "recommendedEvidence": "- IT procurement policies that reference ASHRAE A3/A4 or similar extended environmental ranges as desirable or required criteria.\n- Records of tender evaluations where vendors' responses regarding extended operating ranges were considered.\n- Datasheets of IT equipment procured that is certified for A3/A4 operation.\n- Design documents or operational data showing how the facility leverages wider IT equipment environmental tolerances to save cooling energy.",
        "recommendedActions": "- Assess the climatic conditions of your data centre locations to determine the potential benefits of using IT equipment with extended environmental ranges.\n- Update IT procurement specifications to include evaluation criteria for ASHRAE A3/A4 compliance.\n- Engage with IT equipment vendors to understand their product roadmaps for hardware supporting wider environmental tolerances.\n- If adopting wider ranges, ensure facility infrastructure (e.g., air filtration, humidity control) is adequate.",
        "sampleResponseTemplate": "Our IT procurement strategy includes evaluating equipment for extended environmental range capability, referencing ASHRAE A4 where climate permits. For our [Location/Project], we selected [Equipment Model] specifically for its [A3/A4] compliance, allowing us to [describe benefit, e.g., 'operate with 100% free air cooling for X% of the year']. Vendor documentation confirming this range is [Document Reference].",
        "humanInputAnswer": "For our edge data centre deployments in warmer climates, our 'Edge IT Procurement Standard EDG-PROC-001' mandates ASHRAE A4 compliance as a primary selection criterion. We recently deployed 'Vendor X Model Y' servers that are A4 rated, enabling us to use direct air economization with minimal mechanical cooling backup. This is projected to save 40% in cooling energy costs for these sites compared to A2-only equipment. We always request specific vendor datasheets detailing environmental operating envelopes.",
        "llmGeneratedAnswer": "Yes, our organization actively considers IT hardware with extended operating temperature and humidity ranges. Our 'Global IT Hardware Procurement Policy (GHP-005, Section 6.2)' states a preference for equipment capable of operating within ASHRAE Class A3 or A4 envelopes, where local climatic conditions and facility design permit leveraging such capabilities for energy savings. For instance, in our recent refresh for the 'Southern Europe Hub', we prioritized servers compliant with ASHRAE A3, which allowed for an increase in our standard operating temperature by 2\u00b0C, contributing to an estimated 5% reduction in cooling energy. Vendor datasheets confirming these extended ranges are appended to all relevant procurement evaluations.",
        "badExampleAnswer": "We buy standard servers; they usually work fine with our current cooling setpoints."
      },
      "question_27": {
        "id": 27,
        "old_question_text": null,
        "questionText": "Does the organization select and deploy new IT equipment at the designed power density (per cabinet or per square meter) of the data centre to avoid running the cooling system outside its designed parameters? Is it understood that increasing power density beyond design can create cooling and airflow management problems, reducing both capacity and efficiency, and that power and cooling are considered capacity constraints in addition to physical space (referencing Practice 5.5.6 on Cooling unit sizing)?",
        "simplifiedQuestion": "Do you deploy new IT gear matching your data centre's designed power density to keep cooling efficient?",
        "reference": "2025 CoC Guide 4.1.5",
        "priority": "Medium (Value: 3, Expected: New IT Equipment)",
        "stepSection": "step_2",
        "recommendationTitle": "Select IT equipment suitable for the data centre power density and cooling delivery capabilities",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Data centre design specifications detailing designed power density. IT deployment guidelines or standards that address matching equipment to design density. Capacity planning documents considering power, cooling, and space constraints."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT equipment: Data centre design documents specifying power density limits per cabinet/area. IT deployment procedures or checklists that verify new equipment aligns with these designed densities. Capacity management reports.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "Data Analysis"
        ],
        "verificationMethods": "Review of data centre design specifications for power density, IT deployment guidelines, and capacity management processes. Inspection of cabinet power densities and comparison with design limits. Review of cooling system performance in relation to IT load distribution.",
        "nonconformity": {
          "major": "New IT equipment is frequently deployed exceeding designed power densities, leading to localized hotspots, cooling system strain, and reduced efficiency.",
          "minor": "Power density considerations are informal, or occasional deployments exceed design limits without proper mitigation, impacting cooling efficiency in those areas."
        },
        "majorNonconformity": "Systematic deployment of new IT equipment that significantly exceeds the designed power density of data centre cabinets or zones, leading to compromised cooling system performance and energy inefficiency.",
        "minorNonconformity": "Lack of formal processes to match new IT equipment deployment with designed power densities, or instances where density limits are exceeded without adequate airflow and cooling adjustments.",
        "whatThisMeans": "Your data centre is designed to handle a certain amount of power (and thus heat) per cabinet or area. When you install new IT equipment, make sure its power needs fit within these design limits. If you try to pack in too much high-power gear, your cooling system might struggle, become inefficient, and you could run out of cooling capacity even if you have physical space.",
        "complianceRequires": "- Select and deploy new IT equipment that aligns with the designed power density (per cabinet or per m\u00b2) of the data centre.\n- Avoid deploying IT equipment in a way that causes the cooling system to operate outside its design parameters.\n- Recognize that increasing power density beyond original design can create cooling and airflow management problems, reducing overall capacity and efficiency.\n- Consider power and cooling as critical capacity constraints, alongside physical space, (referencing Practice 5.5.6: Cooling unit sizing and selection).",
        "expertTip": "Implement a capacity planning process that tracks available power, cooling, and space at a granular level (e.g., per cabinet or zone). Before deploying new IT equipment, verify that the intended location has sufficient capacity for all three resources. Use DCIM tools to visualize and manage these capacities.",
        "recommendedEvidence": "- Data centre design documentation specifying designed power densities (e.g., kW per cabinet, W/m\u00b2).\n- IT deployment checklist or procedure that includes verification of power density compliance.\n- Capacity management reports showing tracking of power, cooling, and space utilization against design limits.\n- Examples of how power density considerations influenced recent IT deployment decisions.",
        "recommendedActions": "- Clearly document the designed power density for different areas/zones of your data centre.\n- Integrate power density checks into your IT equipment request, approval, and deployment processes.\n- Train IT and facilities staff on the importance of adhering to designed power densities.\n- If higher densities are required, conduct a thorough assessment of cooling capacity and airflow management, and implement necessary upgrades before deployment.",
        "sampleResponseTemplate": "Our data centre is designed for an average power density of [X kW per cabinet / Y W/m\u00b2], as specified in our [Design Document ID]. Our 'IT Equipment Deployment Standard' (Ref: IT-DEP-002) requires verification that new hardware installations do not exceed this density without a formal review and mitigation plan. We use [DCIM Tool/Spreadsheet] to track cabinet-level power and ensure compliance.",
        "humanInputAnswer": "Our 'Data Centre Design & Operations Manual (DCDOM-Rev3)' specifies a maximum power density of 8kW per standard cabinet. All new IT equipment requests are reviewed against this limit by our Capacity Management team using our Nlyte DCIM system. If a proposed deployment exceeds this, it triggers a mandatory cooling and airflow assessment. For example, a recent high-density GPU server deployment required specific airflow management adjustments (e.g., higher CFM floor tiles) and was co-located in a pod with additional cooling capacity.",
        "llmGeneratedAnswer": "Yes, our organization adheres to a strict power density management protocol. The 'Data Centre Build Standard (DCBS-2023), Annex B' defines specific power density limits for each data hall zone (e.g., Zone A: 7kW/cabinet, Zone B: 12kW/cabinet). Before any new IT equipment is deployed, its power draw is assessed against the designated zone's capacity using our DCIM tool (Vertiv Environet). This ensures that deployments remain within the cooling system's designed operational parameters, preventing hotspots and inefficient cooling. This process considers power and cooling as fundamental capacity constraints, aligning with Practice 5.5.6 regarding cooling unit sizing.",
        "badExampleAnswer": "We fit as many servers as we can in the racks. If it gets too hot in one area, we add more cooling or turn the temperature down."
      },
      "question_28": {
        "id": 28,
        "old_question_text": null,
        "questionText": "When selecting new IT equipment, does the organization require vendors to supply, at a minimum, the total system power consumption for a range of temperatures covering the full allowable inlet temperature range for that equipment, specifically at 100% load on a specified recognized benchmark (e.g., Linpack, SERT, SPECPower)? Is this data required for 5\u00b0C or smaller steps of inlet temperature? Does this comply with the EU Eco Design Directive and Lot 9 amendments for servers and storage? Is it also recommended and requested that vendors provide similar data for 0% and 50% load conditions to allow selection of equipment that maintains efficiency across chosen operating temperatures?",
        "simplifiedQuestion": "When buying new IT gear, do you require vendors to provide power usage data across its full operating temperature range at different loads (0%, 50%, 100%)?",
        "reference": "2025 CoC Guide 4.1.6",
        "priority": "High (Value: 5, Expected: New IT Equipment)",
        "stepSection": "step_2",
        "recommendationTitle": "IT equipment power usage against inlet temperature",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "IT procurement policies or RFP templates requiring vendors to supply temperature-dependent power consumption data. Vendor submissions showing this data. EU Eco Design Directive Lot 9 compliance statements from vendors."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT equipment: Procurement documents requiring vendors to supply IT equipment power consumption data across its allowable inlet temperature range at 100% load (and ideally 0% and 50% load) using recognized benchmarks. Evidence of vendor compliance with EU Eco Design/Lot 9.",
        "verification_method": [
          "Document Review"
        ],
        "verificationMethods": "Review of IT procurement policies, RFP templates, and vendor responses to confirm that temperature-dependent power consumption data is requested and evaluated.",
        "nonconformity": {
          "major": "No requirement for vendors to provide power consumption data across the equipment's operating temperature range, or failure to ensure compliance with EU Eco Design Lot 9.",
          "minor": "Data is requested only for peak load or at a single temperature, or data for 0%/50% load is not consistently requested/evaluated."
        },
        "majorNonconformity": "Failure to require vendors to provide total system power data across the equipment\u2019s full allowable inlet temperature range (at minimum 100% load on a recognized benchmark) for new IT equipment, and not ensuring compliance with relevant EU Eco Design directives (e.g., Lot 9).",
        "minorNonconformity": "Power consumption data across temperature ranges is requested but not for sufficiently small temperature steps (e.g., larger than 5\u00b0C), or data for 0% and 50% load conditions is not consistently requested, limiting ability to assess performance across the full operational spectrum.",
        "whatThisMeans": "When buying new IT equipment, you need to know how its power consumption changes as the inlet air temperature varies. Ask vendors for data showing the equipment's power draw across its entire allowable temperature range, especially at 100% workload (using standard tests like SERT or Linpack), ideally in 5\u00b0C steps. Also, ensure it meets EU Eco Design rules (Lot 9 for servers/storage). It's even better to get this data for idle (0%) and typical (50%) workloads too. This helps you pick equipment that stays efficient even if you run your data centre warmer.",
        "complianceRequires": "- When selecting new IT equipment, require vendors to supply (at minimum) total system power data for a range of temperatures covering the full allowable inlet temperature range for the equipment.\n- This data must be provided for the equipment at 100% load on a specified recognized benchmark (e.g., Linpack, SERT, SPECPower).\n- Data should be provided for 5\u00b0C or smaller steps of inlet temperature.\n- Ensure compliance with the EU Eco Design Directive and Lot 9 amendments to EU Commission regulation for servers and online storage systems as a minimum.\n- It is also recommended to request total system power data covering the full allowable inlet temperature range under 0% and 50% load on the selected benchmark.\n- This practice aims to improve thermal performance of IT equipment by enabling selection of devices with good cooling designs that operate efficiently at increased intake temperatures.",
        "expertTip": "Create a standardized template for vendors to submit this temperature-dependent power consumption data. This makes it easier to compare responses from different vendors. Plotting this data (Power vs. Inlet Temperature at different load levels) can quickly reveal equipment with poor thermal design (i.e., those whose power consumption, especially fan power, significantly increases at higher allowable inlet temperatures).",
        "recommendedEvidence": "- Standard IT procurement RFP/RFQ templates explicitly requiring vendors to provide power consumption data across the full allowable inlet temperature range at 0%, 50%, and 100% load (benchmarked), in 5\u00b0C (or smaller) increments.\n- Vendor submissions for recent IT procurements containing this detailed power consumption data.\n- Documentation confirming compliance with EU Eco Design Directive Lot 9 for relevant equipment.\n- Analysis or comparison charts used during procurement showing how this temperature-dependent power data influenced selection.",
        "recommendedActions": "- Update IT procurement specifications to mandate submission of temperature-dependent power consumption data (0%, 50%, 100% load, full allowable temperature range, \u22645\u00b0C increments, recognized benchmark) from vendors.\n- Ensure all relevant new IT equipment (servers, storage) meets EU Eco Design Directive Lot 9 requirements.\n- Train procurement staff to analyze this data to identify thermally efficient equipment.\n- Use this data to select equipment that will perform efficiently within your data centre's target operating temperature range.",
        "sampleResponseTemplate": "Our RFP for new servers (Ref: [RFP ID]) requires vendors to provide power consumption data across the ASHRAE A2 allowable inlet range at 0%, 50%, and 100% load on the SERT benchmark, in 5\u00b0C increments. All procured servers and storage systems must comply with EU Eco Design Lot 9. For our latest server purchase ([Vendor/Model]), this data (see [Vendor Submission Doc ID]) showed [Example of finding, e.g., 'less than 5% power increase up to 35\u00b0C inlet at 50% load'].",
        "humanInputAnswer": "Our standard server procurement specification (Doc: IT-HW-SERVPROC-V4) mandates vendors provide power consumption figures across the entire ASHRAE A2 allowable inlet temperature range, at 100% load using the SERT suite, with data points every 4\u00b0C. We also require this data for 50% and idle load states. All servers must be Lot 9 compliant. For example, vendor submissions for our Q1 2025 blade server refresh included detailed tables and charts of this data, which was a critical factor in selecting the 'VendorX PowerEdge R7xx' model due to its stable power draw up to 30\u00b0C inlet.",
        "llmGeneratedAnswer": "Yes, when selecting new IT equipment, particularly servers and storage, our organization's 'Procurement Standard for IT Infrastructure (PS-ITI-2025)' mandates that vendors provide total system power consumption data. This data must cover the equipment's full allowable inlet temperature range, be benchmarked at 100% load using SERT for servers (or an equivalent specified benchmark for other equipment), and be provided in 5\u00b0C (or smaller) increments. Compliance with the EU Eco Design Directive and Lot 9 amendments is a prerequisite. Furthermore, section 4.1.6 of PS-ITI-2025 strongly recommends vendors also submit this data for 0% (idle) and 50% load conditions. This information is crucial for our selection process, enabling us to choose equipment that demonstrates optimal energy efficiency across our planned operational temperature range and varying workloads.",
        "badExampleAnswer": "We look at the maximum power draw on the server specs. We assume it will be fine at our standard room temperature."
      },
      "question_29": {
        "id": 29,
        "old_question_text": null,
        "questionText": "When selecting new IT equipment for installation into cabinets, does the organization ensure that the equipment's air flow direction matches the designed air flow for that specific data centre area (commonly front-to-rear or front-to-top)? If equipment with a different air flow direction (e.g., side-to-side) must be used, is it deployed only with a correction mechanism such as ducts or specialized cabinets that divert the air flow to the defined direction to avoid compromising overall air flow management and the ability to raise temperature setpoints, or is such equipment segregated according to Practices 5.1.11, 5.3.4, and 5.3.5?",
        "simplifiedQuestion": "Do you ensure new IT equipment's airflow direction matches your data centre's design, or use corrective measures if it doesn't?",
        "reference": "2025 CoC Guide 4.1.7",
        "priority": "High (Value: 5, Expected: New IT Equipment)",
        "stepSection": "step_2",
        "recommendationTitle": "Select equipment suitable for the data centre - Air flow direction",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "IT procurement standards specifying airflow direction requirements. Data centre design documents defining airflow patterns. Documentation for any airflow correction mechanisms or segregated zones for non-standard airflow equipment."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory for new IT equipment: IT procurement specifications detailing required airflow direction (e.g., front-to-rear). If non-standard airflow equipment is deployed, documentation of airflow correction mechanisms (ducts, special cabinets) or segregation plans (Practices 5.1.11, 5.3.4, 5.3.5).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of IT procurement standards, data centre airflow design documents, and specifications for any airflow correction kits. Physical inspection of IT equipment installations to verify airflow alignment and use of correction mechanisms.",
        "nonconformity": {
          "major": "New IT equipment with non-standard airflow is deployed without correction mechanisms or segregation, significantly disrupting designed airflow patterns and compromising cooling efficiency.",
          "minor": "Non-standard airflow equipment is used with correction mechanisms that are not fully effective, or segregation is not optimally implemented."
        },
        "majorNonconformity": "Deployment of new IT equipment with an airflow direction that conflicts with the data centre's established hot/cold aisle or containment design, without effective airflow correction mechanisms or proper segregation, leading to significant air mixing and reduced cooling efficiency.",
        "minorNonconformity": "Airflow correction mechanisms for non-standard equipment are implemented but are partially ineffective, or the segregation of such equipment does not fully prevent negative impacts on the surrounding environment's airflow management.",
        "whatThisMeans": "When you buy and install new IT equipment in racks, make sure its cooling fans blow air in the same direction as the rest of the equipment in that area (usually front-to-back). If you have to use gear that blows air differently (like side-to-side), you must use special ducts or cabinets to redirect its airflow correctly or put it in a separate, specially designed area. Otherwise, it will mess up your data centre's cooling and make it less efficient.",
        "complianceRequires": "- When selecting equipment for installation into cabinets, ensure its air flow direction matches the designed airflow for that area (typically front-to-rear or front-to-top).\n- If equipment with a different airflow direction must be used, it should only be deployed with a correction mechanism (e.g., ducts, special cabinets) that redirects airflow to match the defined direction for the area.\n- Understand that equipment with non-standard airflow can compromise data centre airflow management and restrict the ability to increase temperature setpoints.\n- Alternatively, segregate such equipment according to Practices 5.1.11 (Equipment segregation), 5.3.4 (Separate environmental zones - new build), and 5.3.5 (Separate environmental zones - colocation/MSP).",
        "expertTip": "Standardize on a single primary airflow path (e.g., front-to-rear) for all IT equipment in your procurement policies. For unavoidable exceptions like some network switches with side-to-side airflow, pre-approve specific airflow correction kits or dedicated cabinet solutions that are known to be effective.",
        "recommendedEvidence": "- IT procurement policy or standard specifying required airflow direction for new equipment.\n- Data centre design documents illustrating the intended airflow pattern (e.g., hot/cold aisle layout diagrams).\n- Datasheets or specifications for airflow correction kits or specialized cabinets used for non-standard airflow equipment.\n- Photographic evidence of correctly installed equipment showing aligned airflow or effective correction measures.\n- Documentation of segregated zones for non-standard airflow equipment, if applicable.",
        "recommendedActions": "- Update IT procurement standards to specify acceptable airflow directions for all new rack-mounted equipment.\n- Train data centre staff on correct equipment installation to maintain airflow integrity.\n- Identify and pre-approve airflow correction solutions for common types of non-standard airflow equipment.\n- If significant non-standard airflow equipment is necessary, plan for segregated cooling zones during data centre design or retrofits.",
        "sampleResponseTemplate": "Our IT hardware procurement standard ([Doc ID: IT-HW-STD-AIRFLOW]) mandates front-to-rear airflow for all servers and storage. For network switches with side-to-side airflow, we utilize [Specific ducting kit/special cabinet model] as documented in our 'Network Deployment Guide'. All new equipment is installed according to our 'Hot/Cold Aisle Design Standard'.",
        "humanInputAnswer": "Our standard rack configuration requires front-to-back airflow for all IT equipment, as detailed in 'DC Installation Standard DC-INST-003'. Any newly procured equipment must adhere to this. For our core network switches which have side-to-side airflow, we deploy them in dedicated network racks equipped with manufacturer-supplied side-to-rear airflow ducting kits (Part #XYZ-DUCT) to maintain the integrity of our hot-aisle/cold-aisle design. This is verified during our deployment checklist.",
        "llmGeneratedAnswer": "Yes, the organization's 'IT Equipment Deployment Standard (IT-EDS-2025, Section 5.1)' mandates that all new IT equipment selected for cabinet installation must possess an airflow direction consistent with the established front-to-rear cooling design of our data centre halls. In exceptional cases where specific equipment has a non-conforming airflow pattern (e.g., side-to-side for certain legacy network devices), Section 5.1.b requires the deployment of approved airflow correction mechanisms, such as vendor-certified ducting kits or placement within specialized airflow management cabinets. Alternatively, such equipment is co-located in a segregated zone with tailored airflow management as per CoC practices 5.1.11 and 5.3.4. This ensures that non-standard airflow does not compromise overall thermal management or limit our ability to optimize temperature setpoints.",
        "badExampleAnswer": "We just install the servers in the racks. As long as they fit, it's usually fine. Some fans blow in different directions."
      },
      "question_30": {
        "id": 30,
        "old_question_text": null,
        "questionText": "Has the organization formally changed its IT equipment deployment process to include the mandatory enabling of power management features on IT hardware as it is deployed? Does this include configuring settings at the BIOS level, within the operating system, and for relevant drivers, to optimize energy consumption based on workload?",
        "simplifiedQuestion": "Do you always enable power management features (in BIOS, OS, drivers) when deploying new IT equipment?",
        "reference": "2025 CoC Guide 4.1.8",
        "priority": "High (Value: 5, Expected: New IT Equipment)",
        "stepSection": "step_2",
        "recommendationTitle": "Enable power management features",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "txt",
            "screenshots"
          ],
          "description": "IT equipment deployment procedures/checklists showing steps for enabling power management. Standard configuration templates for BIOS, OS, and driver power settings. Change management records for implementing these standards."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, txt, screenshots",
        "documentDescriptor": "Mandatory for new IT equipment: Documented IT deployment procedures that explicitly include enabling power management features (BIOS, OS, drivers). Standard configuration guides or scripts used for these settings.",
        "verification_method": [
          "Document Review",
          "Configuration Audit"
        ],
        "verificationMethods": "Review of IT deployment checklists and standard build procedures. Audit of a sample of newly deployed IT equipment to verify power management settings (BIOS, OS, drivers).",
        "nonconformity": {
          "major": "Power management features are not systematically enabled on newly deployed IT equipment, or are commonly left at default (often non-optimal) settings.",
          "minor": "Deployment process includes enabling power management, but it's not consistently applied across all levels (BIOS, OS, driver) or for all new equipment types."
        },
        "majorNonconformity": "Systematic failure to enable available power management features (BIOS, OS, driver level) as a standard part of the deployment process for new IT equipment.",
        "minorNonconformity": "Power management features are enabled inconsistently, or standard configurations for these features are not optimized for energy efficiency, or the process is not formally documented.",
        "whatThisMeans": "When you install new IT hardware, your team should make it a standard, mandatory step to turn on and correctly configure all available power-saving features. This includes settings in the equipment's BIOS (basic startup system), the operating system (like Windows or Linux), and any relevant software drivers. These settings help the equipment use less energy when it's not busy.",
        "complianceRequires": "- Formally change the IT equipment deployment process to include the enabling of power management features on IT hardware as a mandatory step during deployment.\n- Ensure this includes configuring settings at the BIOS level (e.g., C-states, P-states).\n- Ensure this includes configuring settings within the operating system's power management options.\n- Ensure this includes configuring relevant driver settings that affect power consumption.\n- These configurations should be optimized to reduce energy use based on expected workload patterns, without unduly impacting performance.",
        "expertTip": "Develop standardized power management profiles (e.g., 'Max Performance', 'Balanced', 'Max Power Save') for different types of IT workloads and server roles. Apply these profiles automatically during deployment using configuration management tools. Regularly review and update these profiles based on performance monitoring and changes in hardware/software capabilities.",
        "recommendedEvidence": "- Standard Operating Procedure (SOP) or checklist for new IT equipment deployment, explicitly including steps for enabling power management features at BIOS, OS, and driver levels.\n- Standard configuration templates or scripts used to apply these power management settings.\n- Examples of configuration settings for different types of equipment (e.g., screenshots from BIOS, OS power plans).\n- Change management records showing the formal adoption of this deployment process.",
        "recommendedActions": "- Develop and document standardized power management configurations (BIOS, OS, drivers) for various types of new IT equipment.\n- Update IT equipment deployment checklists and automated build processes to incorporate these configurations as a mandatory step.\n- Train deployment staff on the importance and methods of enabling and verifying these settings.\n- Periodically audit newly deployed systems to ensure compliance with power management configuration standards.",
        "sampleResponseTemplate": "Our 'New Server Deployment Procedure' (Ref: IT-DEPLOY-SOP-005, section 4.3) mandates the configuration of power management features. This includes setting BIOS to [e.g., 'OS Control' or specific C/P states], applying the '[Company Standard Balanced Power Plan]' in the OS, and ensuring latest energy-aware drivers are installed. These settings are verified via a post-deployment checklist.",
        "humanInputAnswer": "Our 'Standard Server Build Gold Image' process, documented in SOP-IT-BUILD-001, includes mandatory steps to enable OS-controlled power management in the BIOS (e.g., C-states, P-states enabled, Intel SpeedStep/AMD Cool'n'Quiet active) and configures the OS (Windows Server/Linux) for a 'Balanced' power plan by default. This is applied to all new servers during provisioning. We verify these settings via automated configuration audit tools within 24 hours of deployment.",
        "llmGeneratedAnswer": "Yes, enabling power management features is a formal part of our IT equipment deployment process, as detailed in the 'IT Hardware Commissioning Standard (IT-HCS-2025)'. Section 6.1 specifies that for all new servers, BIOS settings must be configured for optimal power management (e.g., enabling all C-states, P-states, and setting power profiles to 'OS Controlled'). Subsequently, the operating system's power plan is set to 'Balanced' or a custom equivalent defined in 'OS Power Configuration Guideline (OSPCG-002)'. Relevant hardware drivers are also checked and configured for energy-saving features. These steps are included in the deployment checklist and verified before the equipment is released to production.",
        "badExampleAnswer": "Servers are usually left at their default power settings from the manufacturer, as we prioritize performance."
      },
      "question_31": {
        "id": 31,
        "old_question_text": null,
        "questionText": "Does the organization provision power and cooling infrastructure capacity based only on the planned power draw of new IT equipment as configured (i.e., based on the components actually installed and their expected utilization), rather than provisioning based on the maximum Power Supply Unit (PSU) size or nameplate rating? Is this practice intended to avoid over-sizing of electrical infrastructure, which would result in inefficient low (partial) load operation, while acknowledging that provisioning may need changes if IT equipment performance is subsequently increased or upgraded?",
        "simplifiedQuestion": "Do you size power/cooling for new IT gear based on its actual expected power use, not just its PSU's max rating, to avoid inefficiency?",
        "reference": "2025 CoC Guide 4.1.9",
        "priority": "Medium (Value: 3, Expected: New IT Equipment)",
        "stepSection": "step_2",
        "recommendationTitle": "Provision power to the actual IT power usage required",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Capacity planning methodology for power and cooling. Process for determining expected power draw of new IT equipment configurations. Examples of recent provisioning decisions based on actual draw vs. nameplate."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT equipment: Capacity planning documents and M&E provisioning procedures that show power and cooling are provisioned based on the *actual configured/expected power draw* of IT equipment, not PSU nameplate ratings. Process for re-evaluating if IT equipment is upgraded.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of capacity planning processes, IT equipment power assessment methods, and M&E provisioning standards. Analysis of provisioned vs. actual power draw for recently deployed IT equipment.",
        "nonconformity": {
          "major": "Power and cooling infrastructure is consistently provisioned based on IT equipment PSU nameplate ratings, leading to significant over-sizing and inefficient operation of M&E systems.",
          "minor": "Provisioning is sometimes based on nameplate ratings, or methods for determining actual expected power draw are informal or inaccurate."
        },
        "majorNonconformity": "Systematic provisioning of power and cooling capacity based on IT equipment Power Supply Unit (PSU) nameplate ratings rather than the actual expected power draw of the configured equipment, leading to significant M&E over-sizing and inefficiency.",
        "minorNonconformity": "While actual draw is considered, there's no formal process, or PSU nameplate ratings still heavily influence provisioning, leading to some over-sizing. The process for adjusting provisioning upon IT equipment upgrades is unclear.",
        "whatThisMeans": "When planning power and cooling for new IT equipment, base your calculations on how much power the equipment will *actually use* with its specific components and typical workload, not just the maximum power its power supply unit (PSU) *could* draw (its nameplate rating). Using the PSU nameplate often leads to overbuilding your power and cooling systems, making them run inefficiently at lower loads. Remember to re-evaluate if you later upgrade the IT gear.",
        "complianceRequires": "- Provision power and cooling infrastructure capacity based on the *planned power draw* of the IT equipment *as configured* (i.e., based on components actually installed and their expected power draw under typical/peak operational loads).\n- Avoid provisioning based solely on the Power Supply Unit (PSU) size or the equipment's maximum nameplate rating.\n- The intent is to prevent over-sizing of electrical infrastructure, which results in inefficient operation due to low (partial) load conditions.\n- Acknowledge that provisioning may need to be re-evaluated and potentially changed if the IT equipment's performance is subsequently increased or the hardware is upgraded (e.g., adding more CPUs, GPUs, memory).",
        "expertTip": "Utilize IT vendor power configuration tools and historical data from similar existing systems to accurately estimate the expected power draw of new IT equipment configurations. For critical deployments, consider conducting actual power measurements on a sample configuration before finalizing infrastructure provisioning.",
        "recommendedEvidence": "- Capacity planning methodology or procedure that specifies provisioning based on actual/expected IT power draw, not nameplate.\n- Examples of power budget calculations for recent IT deployments showing derivation of expected power draw.\n- Documentation from IT vendor power estimator tools used for planning.\n- Process description for reviewing and adjusting M&E provisioning when IT equipment is upgraded or its configuration changes.",
        "recommendedActions": "- Develop a standardized methodology for estimating the actual expected power draw of new IT equipment configurations.\n- Update capacity planning and infrastructure provisioning processes to use these actual draw estimates instead of PSU nameplate ratings.\n- Train IT and facilities staff on this provisioning methodology.\n- Implement a feedback loop to compare estimated power draw with actual measured consumption post-deployment to refine future estimates.",
        "sampleResponseTemplate": "Our capacity planning process ([Process Doc ID]) requires provisioning power and cooling based on the calculated expected power draw of new IT configurations, typically at [X]% of PSU nameplate after component-specific calculation using [Vendor tools/Internal methodology]. For instance, for [Server Model Y], configured with [Components], we provisioned [Z kW] despite a [PSU_Rating kW] PSU. We re-evaluate if internal components are significantly upgraded.",
        "humanInputAnswer": "Our 'Data Centre Capacity Planning Standard (DCCPS-002)' mandates provisioning based on 'Configured Maximum Power Draw' (CMPD) derived from vendor configuration tools for the specific server build, plus a 15% headroom, rather than PSU nameplate. For our recent HPC cluster deployment, the servers had dual 1600W PSUs (3200W nameplate), but the CMPD was 950W. We provisioned power and cooling per rack based on 1.15 * 950W * number of servers, not the 3200W nameplate, significantly reducing stranded M&E capacity.",
        "llmGeneratedAnswer": "Yes, our organization's 'IT Infrastructure Provisioning Policy (ITIPP-2025)' explicitly states that power and cooling capacity for new IT equipment shall be provisioned based on the 'Projected Operational Power Draw' (POPD). The POPD is calculated using vendor-provided power estimators for the specific hardware configuration (CPU, memory, drives, add-in cards) under an anticipated 70% sustained workload, rather than relying on the PSU nameplate rating. This policy (Section 4.2) aims to prevent oversizing of M&E infrastructure and ensure efficient operation. A documented review process is triggered if an IT asset undergoes significant hardware upgrades that alter its POPD.",
        "badExampleAnswer": "We look at the server's power supply rating and make sure our PDUs and circuits can handle that, plus a bit extra just in case."
      },
      "question_32": {
        "id": 32,
        "old_question_text": null,
        "questionText": "Has the organization ensured that, as a minimum, all new IT equipment (servers and online storage systems) complies with the EU Eco Design Directive and its relevant Lot 9 amendments? Is the Energy Star labelling program for IT equipment used as a guide for server selection where and when available for that class of equipment, unless the organization can determine in-use energy efficiency of hardware through more advanced or effective analysis to select the most efficient equipment for their specific scenario?",
        "simplifiedQuestion": "Do you ensure new IT equipment meets EU Eco Design/Lot 9 and consider Energy Star, or use better methods to pick the most efficient gear?",
        "reference": "2025 CoC Guide 4.1.10",
        "priority": "High (Value: 4, Expected: New IT Equipment)",
        "stepSection": "step_2",
        "recommendationTitle": "EU Eco Design / Energy Star compliant hardware",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "IT procurement policies specifying compliance with EU Eco Design/Lot 9 and consideration of Energy Star. Vendor declarations of conformity for procured equipment. Documentation of any advanced/alternative efficiency analysis methods used for selection."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT equipment: Procurement policies requiring EU Eco Design/Lot 9 compliance for servers/storage. Evidence of Energy Star consideration or use of more advanced efficiency selection analysis for new IT hardware.",
        "verification_method": [
          "Document Review",
          "Procurement Record Audit"
        ],
        "verificationMethods": "Review of IT procurement policies and standards. Audit of recent IT equipment purchases to verify compliance documentation (Eco Design declarations, Energy Star certifications) or records of alternative efficiency analyses performed.",
        "nonconformity": {
          "major": "New servers or online storage systems are procured without ensuring compliance with EU Eco Design Directive/Lot 9, or Energy Star/equivalent efficiency criteria are not considered.",
          "minor": "Compliance with EU Eco Design/Lot 9 is sought, but Energy Star is not consistently used as a guide where applicable, and no alternative advanced efficiency analysis is documented."
        },
        "majorNonconformity": "Systematic failure to ensure new servers and online storage systems comply with the EU Eco Design Directive (Lot 9) or to use Energy Star (or a more advanced analysis) as a guide for selecting energy-efficient IT equipment.",
        "minorNonconformity": "Inconsistent application of EU Eco Design/Lot 9 compliance checks, or reliance on basic Energy Star compliance without exploring more advanced efficiency analyses where beneficial and feasible.",
        "whatThisMeans": "When buying new IT equipment like servers and storage, you must, at a minimum, ensure they meet the EU's Eco Design rules (specifically Lot 9). You should also use the Energy Star label as a guide to pick efficient servers if it's available for that type of equipment. If your organization has better ways to analyze and pick the most efficient hardware for your specific needs, that's also acceptable.",
        "complianceRequires": "- Ensure all new IT equipment (servers and online storage systems) complies with the EU Eco Design Directive and its Lot 9 amendments as a minimum.\n- Utilize the Energy Star labelling programs for IT equipment as a guide for server selection when available for that class of equipment.\n- Alternatively, if the organization possesses more advanced or effective analysis capabilities, use these to determine the in-use energy efficiency of hardware and select the most efficient equipment for the specific scenario.",
        "expertTip": "Maintain a database or list of preferred IT equipment models that are verified for EU Eco Design Lot 9 compliance and high Energy Star ratings (e.g., top 25% percentile). This can streamline procurement and ensure consistent selection of efficient hardware. For advanced analysis, consider using tools like SERT or SPECPower under your typical workload profiles.",
        "recommendedEvidence": "- IT procurement policy explicitly requiring EU Eco Design Lot 9 compliance for servers/storage and use of Energy Star as a selection guide (or justification for alternative advanced analysis).\n- Vendor Declarations of Conformity for recently purchased servers/storage showing Eco Design compliance.\n- Energy Star certificates or datasheets for procured equipment.\n- If alternative advanced analysis is used, documentation of the methodology and results leading to equipment selection.",
        "recommendedActions": "- Update IT procurement policies to mandate EU Eco Design Lot 9 compliance for all new servers and online storage systems.\n- Incorporate Energy Star ratings (or a documented, more effective energy efficiency analysis) into the standard evaluation criteria for new IT equipment.\n- Train procurement staff on how to verify Eco Design compliance and interpret Energy Star labels or other efficiency metrics.\n- Regularly review and update the list of compliant and energy-efficient IT equipment models.",
        "sampleResponseTemplate": "Our IT procurement standard [Doc ID: IT-PROC-001] requires all new servers and storage to be EU Eco Design Lot 9 compliant. We use Energy Star as a primary guide for selection, targeting equipment in the [e.g., top quartile] for efficiency. For example, our recent server purchase ([Model X]) is Lot 9 compliant and Energy Star v3.0 certified. [If applicable: We also conduct [Specific Advanced Analysis] for specialized high-performance systems.]",
        "humanInputAnswer": "Our 'IT Hardware Acquisition Policy (HWAP-2024)' mandates that all new servers and storage systems must comply with the latest EU Eco Design Directive Lot 9 requirements. Vendor self-declarations and technical files are reviewed as part of the tender. We also require servers to have an Energy Star for Servers v3.0 (or newer) certification. For our Q1 2025 refresh, all 50 servers procured (Model ABC) met these criteria, evidenced by vendor documentation (Ref: VENDOR_ECO_DEC_ABC) and Energy Star listing.",
        "llmGeneratedAnswer": "Yes, compliance with the EU Eco Design Directive (specifically Lot 9 for servers and online storage systems) is a mandatory minimum requirement for all new IT hardware acquisitions, as stipulated in our 'Sustainable IT Procurement Policy (SIPP-004, Section 3.1)'. Additionally, Energy Star labelling is used as a key selection guide for servers where applicable. For instance, our standard server specification requires Energy Star v3.0 (or higher) certification. In cases where Energy Star is not applicable or for highly specialized equipment, our 'Advanced Efficiency Assessment Protocol (AEAP-001)' is invoked, which uses a combination of SERT benchmarks and in-house workload simulation to determine the most energy-efficient solution for our specific operational context.",
        "badExampleAnswer": "We buy servers from major brands, and they usually have some energy efficiency features. We don't specifically check for EU rules or Energy Star on everything."
      }
    }
  },
  "step_3": {
    "description": "2025 Guide Section 4: IT Equipment and Services (Practices 4.1.11-4.1.15 from Sec 4.1; and 4.2.1-4.2.5 from Sec 4.2)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_IT_Equip_SVC_2"
    ],
    "critical_questions": [],
    "questions": {
      "question_33": {
        "id": 33,
        "old_question_text": null,
        "questionText": "When selecting new IT equipment, does the organization choose equipment with built-in power and inlet temperature reporting capabilities? Is it preferred that this equipment reports energy used as a counter (kWh) in addition to power as a gauge (kW)? Where applicable, are industry-standard reporting approaches used, such as IPMI, DMTF Redfish, or SMASH? To facilitate broad monitoring, is it expected that all new devices with an IP interface support SNMP polling of inlet temperature and power draw, IPMI polling for the same, or an interface protocol allowing the organization's existing monitoring platform to retrieve this data without additional vendor licenses, with the intent of providing energy and environmental monitoring through normal equipment churn?",
        "simplifiedQuestion": "Do you buy new IT gear that can report its own energy use and inlet temperature using standard methods (e.g., SNMP, IPMI, Redfish) without extra license fees?",
        "reference": "2025 CoC Guide 4.1.11",
        "priority": "Medium (Value: 3, Expected: New IT Equipment)",
        "stepSection": "step_3",
        "recommendationTitle": "Energy & temperature reporting hardware",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "IT procurement specifications requiring built-in energy/temperature reporting. Vendor documentation for selected equipment confirming these capabilities and supported protocols (SNMP, IPMI, Redfish). Configuration details of monitoring systems integrating this data."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT equipment: IT procurement policies/specifications requiring built-in power and inlet temperature reporting capabilities using standard protocols (e.g., SNMP, IPMI, Redfish) without additional licensing costs for data access.",
        "verification_method": [
          "Document Review",
          "Configuration Audit"
        ],
        "verificationMethods": "Review of IT procurement specifications and recent tender documents. Audit of newly deployed IT equipment and monitoring system configurations to verify data reporting and integration.",
        "nonconformity": {
          "major": "New IT equipment is procured without built-in energy and temperature reporting capabilities, or these capabilities require additional vendor licenses for access via standard protocols.",
          "minor": "Reporting capabilities are present but do not consistently use industry-standard protocols, or integration with existing monitoring systems is problematic or incomplete."
        },
        "majorNonconformity": "Systematic procurement of new IT equipment lacking built-in power and inlet temperature reporting capabilities, or where such capabilities are only accessible via proprietary protocols or require additional vendor licensing fees for standard (SNMP, IPMI, Redfish) data retrieval.",
        "minorNonconformity": "New IT equipment has reporting features, but they are not consistently industry-standard (SNMP, IPMI, Redfish), or the data is not actively integrated into the organization's monitoring platform due to complexity or minor access issues (not related to extra licensing).",
        "whatThisMeans": "When you buy new IT equipment, choose models that can report their own power usage (ideally in kWh consumed over time, not just current kW) and their air intake temperature. This should use common industry standards like IPMI, Redfish, or SNMP so your existing monitoring tools can collect the data without needing to buy extra licenses from the equipment vendor. This helps you monitor energy and environmental conditions across your data centre as you naturally replace equipment.",
        "complianceRequires": "- Select new IT equipment with built-in power and inlet temperature reporting capabilities.\n- Prefer equipment that reports energy used (kWh counter) in addition to instantaneous power (kW gauge).\n- Where applicable, ensure industry-standard reporting approaches are used (e.g., IPMI, DMTF Redfish, SMASH).\n- For all new devices with an IP interface, expect support for one of the following for inlet temperature and power draw reporting:\n    - SNMP polling (traps and configuration not required).\n    - IPMI polling (subject to inlet temperature being included as per IPMI or Redfish).\n    - An interface protocol allowing retrieval by the operator's existing monitoring platform without additional vendor licenses.\n- The intent is to enable comprehensive energy and environmental monitoring through normal equipment refresh cycles.",
        "expertTip": "Standardize on one or two preferred reporting protocols (e.g., Redfish and SNMP) in your procurement specifications to simplify integration with your monitoring systems. Also, verify during proof-of-concept or testing that the data provided is accurate and granular enough for your needs.",
        "recommendedEvidence": "- IT procurement specifications mandating built-in energy and temperature reporting via standard protocols (SNMP, IPMI, Redfish) without additional licensing.\n- Vendor datasheets for recently procured IT equipment confirming these reporting capabilities and supported protocols.\n- Screenshots or configuration details from the monitoring system showing integration and collection of this data from new IT equipment.\n- Policy stating preference for energy (kWh) counters.",
        "recommendedActions": "- Update IT procurement standards to require new equipment to have embedded power and inlet temperature sensors accessible via standard, license-free protocols (IPMI, Redfish, SNMP).\n- Specify the preference for energy (kWh) reporting in addition to power (kW).\n- During equipment evaluation, verify the supported reporting protocols and ease of integration with your existing monitoring platforms.\n- Develop standard procedures for configuring and integrating new IT equipment into the energy and environmental monitoring system upon deployment.",
        "sampleResponseTemplate": "Our standard server specification ([Doc ID: SERV-SPEC-2025]) requires onboard power (kW and kWh) and inlet temperature reporting via [Redfish/IPMI/SNMP]. This data must be accessible by our [Monitoring System Name] without additional licenses. For our latest server deployment ([Model X]), we are polling these metrics via [Protocol] every [Interval].",
        "humanInputAnswer": "All servers procured since our Q3 2024 policy update ('IT Hardware Standards v5.0') must provide Redfish API access for power (kW and accumulated kWh) and inlet temperature reporting. Our DCIM platform, Nlyte, polls this data every 5 minutes without requiring any additional vendor licenses. For example, our new 'VendorX Gen11' servers are fully compliant and integrated.",
        "llmGeneratedAnswer": "Yes, our 'IT Equipment Procurement Standard (ITEPS-2025, Section 6)' mandates that all new server, storage, and network equipment with an IP interface must feature onboard power (kW and kWh) and inlet temperature reporting capabilities. These capabilities must be accessible via industry-standard protocols, specifically DMTF Redfish or, alternatively, SNMPv3, without necessitating additional software licenses from the equipment vendor for data retrieval. For instance, all servers acquired in the 2025 fiscal year (e.g., 'Project Titan' deployment) support Redfish for environmental and power telemetry, which is integrated into our central monitoring platform.",
        "badExampleAnswer": "Some of our newer servers have monitoring features, but we mostly rely on PDU-level power readings."
      },
      "question_34": {
        "id": 34,
        "old_question_text": null,
        "questionText": "When selecting new IT equipment, does the organization consider models which provide mechanisms for external control of their energy use? Does this include capabilities such as an external system being able to restrict a server's maximum energy use or trigger the shutdown of components, entire systems, or sub-systems, potentially through user-defined policies?",
        "simplifiedQuestion": "Do you choose new IT gear that allows external systems to control its energy use (e.g., cap power, shut down parts)?",
        "reference": "2025 CoC Guide 4.1.12",
        "priority": "High (Value: 5, Expected: Optional Practice)",
        "stepSection": "step_3",
        "recommendationTitle": "Control of equipment energy use",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "IT procurement specifications that include evaluation of external energy control capabilities. Vendor documentation detailing these features (e.g., power capping APIs, component shutdown capabilities). Documentation of any implemented systems using these external controls."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: IT procurement criteria that assess equipment for external energy control capabilities. If implemented, documentation of how these features (e.g., power capping, dynamic component shutdown) are utilized.",
        "verification_method": [
          "Document Review",
          "System Demonstration"
        ],
        "verificationMethods": "Review of procurement criteria and equipment specifications for external energy control features. If implemented, demonstration of the control system interacting with IT equipment to manage energy use.",
        "nonconformity": {
          "major": null,
          "minor": "New IT equipment is selected without consideration for external energy control capabilities, missing opportunities for advanced energy management (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to evaluate or select new IT equipment based on its capability for external energy use control (e.g., power capping, component shutdown via API/policy), where such features could offer significant energy management benefits.",
        "whatThisMeans": "When selecting new IT hardware, your organization should look for equipment that allows its energy consumption to be controlled by external management systems. This could mean, for example, the ability for a central system to set a maximum power limit for a server, or to instruct servers to shut down specific components (like unused processors or disk drives) or even entire systems based on defined policies. This provides more advanced ways to manage energy.",
        "complianceRequires": "- When selecting new IT equipment, consider models that provide mechanisms for external control of their energy use.\n- Evaluate capabilities such as the ability for an external system to restrict a server's maximum energy use (power capping).\n- Assess if the equipment allows external triggering for the shutdown of components, entire systems, or sub-systems.\n- Consider how user-defined policies could be applied through such external control mechanisms.",
        "expertTip": "Look for IT equipment that supports open APIs for power management, like Redfish. This allows for easier integration with third-party DCIM or energy management software, enabling sophisticated, policy-driven control of IT energy consumption (e.g., aligning server power caps with cooling capacity or electricity prices).",
        "recommendedEvidence": "- IT procurement specifications or evaluation criteria that include 'external energy control capabilities' (e.g., power capping, API-driven component shutdown).\n- Vendor datasheets or technical documentation for selected IT equipment highlighting these control features.\n- If implemented, documentation or demonstration of the energy management system utilizing these external control capabilities.\n- Examples of user-defined policies for controlling IT equipment energy use.",
        "recommendedActions": "- Include evaluation of external energy control features (e.g., power capping, component shutdown via APIs like Redfish) in the selection criteria for new IT equipment.\n- If such equipment is procured, explore integrating these capabilities with higher-level energy management or workload orchestration systems.\n- Develop policies for when and how to apply these controls to optimize energy use without impacting essential services.\n- Pilot these features in non-critical environments to understand their impact and benefits before wider deployment.",
        "sampleResponseTemplate": "During our IT procurement process for [Equipment Type], we evaluate the availability of external energy control mechanisms such as [e.g., 'Redfish API for power capping', 'DCMI power limiting']. For [Selected Equipment Model], these features [are/are not significantly] utilized via [Management System/Policy Name] to [achieve X, e.g., 'cap power during peak demand periods'].",
        "humanInputAnswer": "Our latest server refresh cycle (Q4 2024) included a mandatory requirement for Redfish API support for power capping. We selected 'Vendor Y SuperServer Z' models which allow us to dynamically set power limits via our in-house developed 'Datacenter Orchestrator Tool'. This tool applies power caps based on rack-level power budget and thermal conditions, ensuring we don't exceed safe operating limits while maximizing compute density.",
        "llmGeneratedAnswer": "Yes, our 'Server Hardware Technical Specification (SHTS-2025, Appendix C)' includes a desirable criterion for 'External Energy Control Capabilities'. This involves assessing if new servers provide programmatic interfaces (e.g., Redfish, IPMI) that allow external management systems to set power consumption limits (power capping) or trigger low-power states for specific components or the entire system based on user-defined policies. While not yet universally implemented across all workloads, selected HPC clusters leverage this feature via our 'Workload Aware Power Scheduler' to cap non-critical batch jobs during periods of high energy cost or limited cooling capacity.",
        "badExampleAnswer": "We don't usually control server power externally; they just run based on their internal power management."
      },
      "question_35": {
        "id": 35,
        "old_question_text": null,
        "questionText": "If the organization is forced to use free-standing IT equipment or equipment supplied in custom cabinets (where no alternative is available), does it select such equipment so that the airflow direction of the enclosures matches the established airflow design (e.g., hot/cold aisle layout, containment scheme) in that area of the data centre, commonly front-to-rear or front-to-top? Is it understood that equipment with non-standard airflow can compromise overall airflow management and that such equipment might need segregation (Practices 5.1.11, 5.3.4, 5.3.5) and that free-standing equipment is generally to be avoided if it disrupts organised airflow?",
        "simplifiedQuestion": "If you must use free-standing IT gear, do you ensure its airflow matches your data centre's design, or manage it separately?",
        "reference": "2025 CoC Guide 4.1.13",
        "priority": "Medium (Value: 4, Expected: New IT Equipment)",
        "stepSection": "step_3",
        "recommendationTitle": "When forced to use select free standing equipment suitable for the data centre Airflow direction",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Procurement justification for free-standing/custom cabinet equipment. Specifications of such equipment showing airflow characteristics. Data centre layout plans indicating placement and any special airflow management for these units (e.g., segregation, custom ducting)."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory (if applicable): Justification for using free-standing equipment. Specifications showing its airflow. Plans for ensuring its airflow aligns with the data centre's design (e.g., via custom enclosures, ducting, or placement in segregated zones).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of procurement justifications and equipment specifications for any free-standing IT equipment. Physical inspection of such installations to verify airflow management strategies.",
        "nonconformity": {
          "major": "Free-standing or custom-cabinet IT equipment with conflicting airflow is deployed without any measures to align its airflow with the data centre design, causing significant disruption to overall cooling efficiency.",
          "minor": "Measures are taken for non-standard airflow free-standing equipment, but they are only partially effective, or the justification for using such equipment is weak."
        },
        "majorNonconformity": "Deployment of new free-standing IT equipment or equipment in custom cabinets with an airflow direction that conflicts with the data centre's established design without any corrective measures, leading to compromised airflow management.",
        "minorNonconformity": "Free-standing equipment with non-standard airflow is deployed with attempts at airflow matching that are not fully effective, or its placement is not optimally segregated.",
        "whatThisMeans": "If you absolutely have to use IT equipment that doesn't fit in standard racks (free-standing or in its own special cabinet), you must choose models whose cooling airflow direction (e.g., front-to-back) matches your data centre's overall airflow plan. It should fit into your hot aisle/cold aisle setup. If it doesn't, it can mess up cooling for other gear and make it harder to run the data centre efficiently. Such equipment might need its own special isolated area. Generally, try to avoid free-standing gear if it disrupts organized airflow.",
        "complianceRequires": "- If no alternative to free-standing equipment or equipment in custom cabinets is available, select it such that the airflow direction of its enclosures matches the airflow design of that data centre area (commonly front-to-rear or front-to-top).\n- Ensure such equipment specifically matches the hot/cold aisle layout or containment scheme implemented.\n- Understand that equipment with non-standard airflow can compromise overall airflow management and restrict the ability to raise temperature setpoints.\n- Consider mitigating compromises by segregating such equipment according to Practices 5.1.11, 5.3.4, and 5.3.5.\n- Note that free-standing equipment should generally be avoided if it disrupts well-organized airflow, especially in rooms primarily equipped with rack-mounted IT gear.",
        "expertTip": "If free-standing equipment is unavoidable, try to place it at the end of an aisle or in a dedicated zone where its airflow will have minimal impact on standard rack-mounted equipment. Work with the equipment vendor to explore custom ducting or shrouding options that can help integrate its airflow into your existing scheme.",
        "recommendedEvidence": "- Justification document for the procurement of free-standing or custom-cabinet IT equipment.\n- Equipment datasheets specifying its airflow pattern.\n- Data centre layout plans showing the placement of such equipment and any specific airflow management provisions (e.g., custom ducting, baffles, or segregation into a specific zone).\n- Photographic evidence of the installed equipment and its airflow management.",
        "recommendedActions": "- Minimize the use of free-standing IT equipment that does not conform to the standard data centre airflow patterns.\n- When unavoidable, select free-standing equipment whose airflow can be made compatible with the data centre's design (e.g., using vendor-supplied or custom-designed ducting kits).\n- If compatibility cannot be achieved, plan for deployment in a segregated zone with independent airflow management, as per Practices 5.1.11, 5.3.4, and 5.3.5.\n- Include airflow compatibility in the procurement criteria for any non-rack-mounted IT equipment.",
        "sampleResponseTemplate": "We rarely use free-standing IT equipment. However, for our [Specific System, e.g., 'Mainframe Model Z'], which is free-standing, it was selected with a [front-to-rear] airflow pattern and is located in [Zone X] which is specifically designed to accommodate its airflow. This is documented in our [Data Centre Layout Plan, Rev Y]. We generally avoid equipment that would disrupt our standard hot/cold aisle containment.",
        "humanInputAnswer": "Our policy is to avoid free-standing IT equipment. However, a specific legacy 'System Alpha' required for a critical application is housed in its original custom cabinet. This system has a front-to-top airflow. To manage this, it's placed at the end of a row, outside our standard hot-aisle containment, and is fitted with a custom exhaust chimney that directs its hot air into the main hot air return plenum, as per design drawing 'DC-Layout-Variant-Alpha'. This was approved via exception process 'EXC-2023-012'.",
        "llmGeneratedAnswer": "Yes, in the rare instances where free-standing IT equipment is unavoidable (e.g., specialized legacy systems with no rack-mount alternative), our 'Non-Standard Equipment Deployment Protocol (NSEDP-001)' mandates strict adherence to airflow compatibility. For example, a recently integrated 'System Titanus' (free-standing archival unit) was selected with a front-to-rear airflow path, matching our data hall's hot/cold aisle design. It is positioned at the end of Row C, with custom blanking panels ensuring its integration into the contained cold aisle and preventing air recirculation, as specified in 'Deployment Plan DP-Titanus-2025'. If direct airflow matching is impossible, the protocol requires segregation as per CoC practices 5.1.11, 5.3.4, and 5.3.5, although we strive to avoid such scenarios to maintain optimal airflow dynamics.",
        "badExampleAnswer": "We have an old large printer in the data centre; it just sits in a corner. It has its own fans."
      },
      "question_36": {
        "id": 36,
        "old_question_text": null,
        "questionText": "For new IT equipment whose primary cooling method is direct liquid cooling (not air-cooled), is it understood that such devices are not subject to the air environmental requirements specified in Practices 4.1.2 or 4.1.3 (ASHRAE A1/A2 air ranges)? Does this apply specifically to devices where cooling liquid is delivered directly to heat-removing components (e.g., water-cooled heat sinks, heat pipes) and not to systems with internal mechanical refrigeration or in-chassis air cooling that still rely on specific coolant liquid or air delivery ranges to IT components? Is the potential for direct liquid cooling to offer advantages in very high-density applications (e.g., HPC) and energy efficiency (including waste heat reuse) recognized, referencing ASHRAE guidelines like 'Liquid Cooling Guidelines for Datacom Equipment Centers'?",
        "simplifiedQuestion": "For new direct liquid-cooled IT gear, do you follow specific liquid cooling guidelines instead of air-cooling environmental standards?",
        "reference": "2025 CoC Guide 4.1.14",
        "priority": "Medium (Value: 4, Expected: Optional Practice)",
        "stepSection": "step_3",
        "recommendationTitle": "Operating temperature range - Direct liquid cooled IT equipment",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "If direct liquid-cooled IT equipment is used: Specifications of this equipment, design of the liquid cooling infrastructure, operational parameters for the liquid cooling loop. References to ASHRAE liquid cooling guidelines."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: If direct liquid-cooled IT equipment is deployed, provide specifications for this equipment and the supporting liquid cooling infrastructure. Documentation should show adherence to liquid cooling best practices (e.g., ASHRAE guidelines) rather than air-based environmental standards for such equipment.",
        "verification_method": [
          "Document Review",
          "System Inspection"
        ],
        "verificationMethods": "If direct liquid cooling is implemented: Review of design documents for liquid-cooled IT equipment and its supporting infrastructure. Inspection of the liquid cooling system and its operational parameters.",
        "nonconformity": {
          "major": null,
          "minor": "Direct liquid-cooled IT equipment is used, but its environmental management or supporting infrastructure does not follow recognized liquid cooling guidelines, or it's incorrectly subjected to air-based environmental constraints (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to manage the liquid cooling environment according to specific liquid cooling guidelines (e.g., ASHRAE), or misapplication of air-based environmental standards to primarily liquid-cooled components, potentially leading to suboptimal operation or missed efficiency opportunities.",
        "whatThisMeans": "If you're using new IT equipment that is primarily cooled by liquid directly (like water flowing over a heat sink on a chip), the usual air temperature and humidity rules (like ASHRAE A1/A2) don't apply to those liquid-cooled parts. This is different from equipment that has its own mini air conditioner or refrigeration inside. Direct liquid cooling can be good for very high-power gear (like in High-Performance Computing) and can be more energy efficient, even allowing you to reuse the heat. You should follow specific liquid cooling guidelines, like those from ASHRAE.",
        "complianceRequires": "- Understand that IT devices whose primary cooling method is direct liquid cooling are not subject to the air environmental requirements of Practices 4.1.2 (Restricted/legacy air range) or 4.1.3 (Expected A2 air range).\n- This applies to devices where cooling liquid is delivered directly to the heat removal system of components (e.g., water-cooled heat sinks, heat pipes).\n- This does *not* apply to devices with internal mechanical refrigeration plants or in-chassis air cooling systems that are themselves liquid-cooled indirectly.\n- Recognize the potential advantages of direct liquid cooling in very high-density applications (e.g., HPC) and for energy efficiency, including useful waste heat extraction.\n- Reference relevant guidelines, such as ASHRAE's 'Liquid Cooling Guidelines for Datacom Equipment Centers, Second Edition'.",
        "expertTip": "When implementing direct liquid cooling, pay close attention to the quality, temperature, and flow rate of the coolant as specified by the IT equipment vendor and liquid cooling guidelines. Also, fully explore the potential for waste heat reuse, as liquid cooling often provides higher-grade (higher temperature) waste heat compared to air cooling, making it more viable for reuse applications.",
        "recommendedEvidence": "- If direct liquid-cooled IT equipment is deployed: Datasheets and specifications for this equipment.\n- Design documents for the direct liquid cooling infrastructure (e.g., CDUs, piping, heat rejection methods).\n- Operational parameters for the liquid cooling loop (e.g., coolant temperature, flow rate, pressure).\n- Evidence of adherence to relevant liquid cooling guidelines (e.g., ASHRAE's liquid cooling guidelines).",
        "recommendedActions": "- If considering or deploying very high-density IT equipment, evaluate direct liquid cooling as a potentially more efficient cooling solution.\n- When selecting direct liquid-cooled IT equipment, ensure it and its supporting infrastructure are designed and operated according to established liquid cooling guidelines (e.g., ASHRAE).\n- Clearly differentiate between direct liquid-cooled systems and air-cooled systems with liquid-assisted components when defining environmental operating parameters.\n- Investigate and plan for waste heat reuse opportunities from liquid cooling systems.",
        "sampleResponseTemplate": "We have deployed [Vendor/Model] direct liquid-cooled servers for our [HPC cluster/AI platform]. These systems are not subject to our standard data hall air environmental controls, but rather operate with a [Type of coolant, e.g., warm water] loop maintained at [Temperature range, e.g., 25-40\u00b0C inlet] as per [ASHRAE Liquid Cooling Guidelines/Vendor Specifications]. This is documented in our [Liquid Cooling SOP ID].",
        "humanInputAnswer": "Our new HPC cluster, 'Hydra', utilizes 'VendorX AquaServe' direct-to-chip liquid-cooled servers. The primary cooling for these servers is managed via a closed-loop warm water system, with inlet water temperatures between 30-35\u00b0C, as per vendor specifications and ASHRAE liquid cooling guidelines. The surrounding air environment is maintained for other components but not for the primary cooling of the CPUs/GPUs. This design (Ref: HYDRA_COOL_DESIGN_DOC) allows for significant energy savings and heat recovery potential.",
        "llmGeneratedAnswer": "Yes, for our high-density computing (HDC) clusters that utilize direct liquid cooling (DLC) technology (specifically, rear-door heat exchangers with direct-to-chip components on some nodes), we acknowledge that these components are not governed by the air-based environmental parameters of Practices 4.1.2 or 4.1.3. Our 'DLC Operational Protocol (DLC-OP-001)' references ASHRAE's 'Liquid Cooling Guidelines for Datacom Equipment Centers, Second Edition' for managing the liquid coolant loop parameters (temperature, flow, quality). This allows us to optimize the energy efficiency of these specialized systems and explore higher-grade waste heat reuse, which is a key feature of our 'HDC Energy Strategy'. Air cooling in these zones is supplementary and managed for non-DLC components only.",
        "badExampleAnswer": "All our equipment is air-cooled to the same room temperature. We don't use any liquid cooling."
      },
      "question_37": {
        "id": 37,
        "old_question_text": null,
        "questionText": "When selecting new IT equipment, does the organization ensure that it contains high efficiency AC/DC power converters (Power Supply Units - PSUs)? Are these PSUs rated at 90% power efficiency or better across the range of loads expected for the equipment to be installed? Is compliance with standards such as ISO/IEC 30134-4 (ITEEsv - for PSU efficiency aspects) and ETSI EN 303 470, as well as IEC 62040-5 (relevant for UPS but indicates PSU quality/efficiency thinking), considered or required for these components?",
        "simplifiedQuestion": "Do you choose new IT gear with highly efficient power supplies (PSUs) (e.g., 90%+ efficient at typical loads)?",
        "reference": "2025 CoC Guide 4.1.15",
        "priority": "Medium (Value: 3, Expected: New IT Equipment)",
        "stepSection": "step_3",
        "recommendationTitle": "AC/DC Converter efficiency",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "IT procurement specifications requiring high-efficiency PSUs (e.g., 80 PLUS Titanium/Platinum, or specific efficiency percentages at various loads). Vendor datasheets confirming PSU efficiency ratings. References to ITEEsv, ETSI EN 303 470, or IEC 62040-5 in procurement if used for PSU selection."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT equipment: IT procurement policies/specifications requiring high-efficiency AC/DC power converters (PSUs), targeting 90%+ efficiency at expected loads. Evidence of referencing standards like 80 PLUS ratings (e.g., Titanium, Platinum), ITEEsv, or ETSI EN 303 470 in selection.",
        "verification_method": [
          "Document Review",
          "Equipment Specification Audit"
        ],
        "verificationMethods": "Review of IT procurement specifications for PSU efficiency requirements. Audit of datasheets for recently procured IT equipment to verify PSU efficiency ratings (e.g., 80 PLUS certification level, vendor-stated efficiency curves).",
        "nonconformity": {
          "major": "New IT equipment is procured with PSUs that do not meet high efficiency standards (e.g., significantly below 90% at typical loads, or lower-tier 80 PLUS ratings like Bronze/Silver without justification).",
          "minor": "High-efficiency PSUs are generally sought, but specific targets (like 90%+) or verification across expected load ranges are not consistently applied or documented."
        },
        "majorNonconformity": "Systematic procurement of new IT equipment with AC/DC power converters (PSUs) that do not meet high efficiency standards (e.g., failing to achieve at least 90% efficiency across expected operational loads, or not meeting at least 80 PLUS Gold/Platinum equivalent).",
        "minorNonconformity": "PSU efficiency is considered, but without a strict requirement for 90%+ efficiency across expected loads, or without consistent verification against standards like 80 PLUS ratings (Titanium, Platinum), ITEEsv, or ETSI EN 303 470.",
        "whatThisMeans": "The power supplies (PSUs) inside your new IT equipment, which convert AC power from the wall to DC power for the components, should be very efficient \u2013 ideally 90% or better when the equipment is running at its typical load. Look for certifications like '80 PLUS Platinum' or 'Titanium', or check for compliance with standards like ITEEsv or ETSI EN 303 470. Efficient PSUs waste less energy as heat.",
        "complianceRequires": "- Select new IT equipment containing high efficiency AC/DC power converters (PSUs).\n- Ensure these PSUs are rated at 90% power efficiency or better across the range of loads expected for the installed equipment (not just at one optimal point).\n- Reference relevant standards like ISO/IEC 30134-4 (ITEEsv) for server efficiency aspects which include PSU efficiency, and ETSI EN 303 470 for server energy efficiency methodology.\n- Consider the principles of efficiency from standards like IEC 62040-5 (though for UPS systems, it reflects high-quality power conversion expectations applicable to PSUs).",
        "expertTip": "Specify a minimum 80 PLUS certification level (e.g., Platinum or Titanium) for all new IT equipment PSUs in your procurement standards. Also, request PSU efficiency curves from vendors to understand how efficiency varies across different load levels (e.g., 20%, 50%, 100% load) to ensure good performance at your typical operational load.",
        "recommendedEvidence": "- IT procurement policy or standard specifying minimum PSU efficiency requirements (e.g., 80 PLUS Platinum/Titanium, or target percentage efficiency at defined load points).\n- Datasheets or technical specifications for recently procured IT equipment, clearly indicating the PSU model and its efficiency rating/certification.\n- Vendor documentation referencing compliance with ISO/IEC 30134-4 or ETSI EN 303 470 for the PSU or server as a whole.\n- Purchase orders or invoices for IT equipment that list PSU specifications.",
        "recommendedActions": "- Update IT procurement specifications to mandate high-efficiency PSUs (e.g., 80 PLUS Platinum or Titanium equivalent, or specific efficiency percentages at various load points) for all new IT equipment.\n- Ensure that PSU efficiency is evaluated not just at a single point, but across the expected operating load range of the IT equipment.\n- Train procurement staff to verify PSU efficiency claims and understand relevant certifications and standards.\n- Prioritize vendors who transparently provide detailed PSU efficiency data and certifications.",
        "sampleResponseTemplate": "Our standard procurement for new servers ([Doc ID: SERV-PROC-SPEC-2025]) requires AC/DC power converters to be at least [e.g., 80 PLUS Platinum certified / 94% efficient at 50% load]. For instance, our recently acquired [Server Model X] features [PSU Model/Rating] which meets this criterion. We reference ITEEsv reports where available to compare overall server efficiency which includes PSU performance.",
        "humanInputAnswer": "Our 'Server Procurement Standard (SPS-004)' mandates that all new servers must be equipped with 80 PLUS Titanium certified power supplies. This ensures a minimum of 90% efficiency at 10% load, 94% at 20% load, 96% at 50% load, and 91% at 100% load. For our current generation servers (VendorX Model Y), the PSUs are rated 80 PLUS Titanium and vendor datasheets confirm efficiency exceeding 94% across our typical 30-60% operational load range. We also verify ITEEsv results when provided by vendors.",
        "llmGeneratedAnswer": "Yes, our organization's 'IT Hardware Procurement Policy (ITHPP-2025, Section 5.3)' specifies that all new IT equipment, particularly servers and storage arrays, must be equipped with high-efficiency AC/DC power converters. The minimum acceptable standard is an 80 PLUS Platinum rating, ensuring at least 90% efficiency at 20% load, 92-94% at 50% load, and 89-91% at 100% load. We prioritize Titanium-rated PSUs (target >94% efficiency at 50% load) where available and economically viable for the specific equipment class. Vendor submissions must include PSU efficiency curves and compliance documentation with ISO/IEC 30134-4 (ITEEsv) or ETSI EN 303 470 where applicable.",
        "badExampleAnswer": "Our servers come with whatever power supplies the manufacturer installs. They are usually pretty good brands."
      },
      "question_38": {
        "id": 38,
        "old_question_text": null,
        "questionText": "Has the organization established formal processes that require senior business approval for any new IT service deployment that necessitates dedicated physical hardware (servers, storage, networking) and will not run on a shared resource platform (e.g., existing virtualization or cloud environments)? Does this apply to server, storage, and networking aspects of the service?",
        "simplifiedQuestion": "Do new IT services require senior business approval if they need dedicated hardware instead of using shared/virtualized platforms?",
        "reference": "2025 CoC Guide 4.2.1",
        "priority": "High (Value: 5, Expected: New IT Equipment)",
        "stepSection": "step_3",
        "recommendationTitle": "Deploy Virtualisation technologies",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx"
          ],
          "description": "Policy documents outlining the 'virtualization first' or 'shared platform first' approach. Process documentation for requesting and approving dedicated hardware for new services, including levels of approval required. Examples of recent approvals/rejections for dedicated hardware."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx",
        "documentDescriptor": "Mandatory: Documented policy or procedure requiring senior business approval for deploying new services on dedicated hardware instead of shared/virtualized platforms. Evidence of this approval process in action.",
        "verification_method": [
          "Document Review",
          "Process Verification",
          "Interview"
        ],
        "verificationMethods": "Review of IT deployment policies, new service provisioning workflows, and records of dedicated hardware approvals. Interviews with IT management and business stakeholders involved in new service approval.",
        "nonconformity": {
          "major": "No formal process or requirement for senior business approval when new services are deployed on dedicated hardware instead of shared platforms; dedicated hardware is commonly used without strategic review.",
          "minor": "An approval process exists but is not consistently followed, lacks senior business involvement, or the justification for dedicated hardware is not rigorously assessed against shared platform alternatives."
        },
        "majorNonconformity": "Absence of a formal process requiring senior business approval for new services needing dedicated hardware, leading to uncontrolled proliferation of physical assets.",
        "minorNonconformity": "Approval process for dedicated hardware is informal, lacks clear criteria for exception, or does not consistently involve senior business stakeholders before deployment.",
        "whatThisMeans": "Your organization should have a formal rule that any new IT service must use shared resources (like virtual machines or cloud platforms) by default. If a new service truly needs its own physical servers, storage, or networking gear, this should require approval from senior business leaders, not just IT. This helps ensure dedicated hardware is only used when absolutely necessary, promoting better resource utilization and energy efficiency.",
        "complianceRequires": "- Implement formal processes requiring senior business approval for any new IT service that demands dedicated hardware (servers, storage, networking).\n- This approval should be necessary if the service will not run on an existing resource-sharing platform (e.g., virtualization clusters, private/public cloud).\n- This requirement applies to all aspects of the service: servers, storage, and networking components.\n- The aim is to maximize the use of virtualization and shared platforms, thereby improving hardware utilization and energy efficiency.",
        "expertTip": "Establish a 'Virtualization First' or 'Cloud First' policy. Any request for dedicated physical hardware should trigger an exception process requiring a strong business case, technical justification (why shared platforms are unsuitable), and cost-benefit analysis, reviewed by a governance body that includes business representation.",
        "recommendedEvidence": "- IT policy document stating a 'virtualization first' or 'shared platform first' principle.\n- Documented procedure for requesting and approving dedicated hardware for new IT services, including the required approval levels (showing senior business involvement).\n- Examples of recent new service deployment requests, showing either use of shared platforms or the approval documentation for dedicated hardware.\n- Charter or ToR for the governance body that approves dedicated hardware requests.",
        "recommendedActions": "- Develop and implement a formal policy that new IT services should, by default, be deployed on existing shared/virtualized platforms.\n- Define a clear exception process, requiring senior business and IT approval, for any new service needing dedicated physical hardware.\n- Ensure the exception process requires a robust justification explaining why shared resources are not viable.\n- Regularly review the utilization of existing shared platforms to ensure capacity for new services and to identify further consolidation opportunities.",
        "sampleResponseTemplate": "Our 'New Service Deployment Policy' ([Policy ID SDC-005]) mandates virtualization as the default. Any new service requiring dedicated hardware (server, storage, or network) must receive approval from the [e.g., IT Steering Committee, which includes business unit VPs]. This process requires a formal justification for not using our [e.g., VMware vSphere platform / private cloud]. For instance, [New Service X] was approved for dedicated hardware on [Date] due to [specific reasons], as documented in [Approval Record Y].",
        "humanInputAnswer": "Our 'Application Hosting Standard AHS-2024' mandates that all new services be deployed on our central VMware virtualization platform unless a documented technical exception is approved by the IT Architecture Review Board and the sponsoring business unit Director. In Q1 2025, out of 15 new services deployed, only one (a specialized real-time analytics platform with specific hardware dependencies) required and received approval for dedicated physical servers. The approval (Ref: ARB-2025-EX003) includes the business justification.",
        "llmGeneratedAnswer": "Yes, our organization enforces a 'Shared Infrastructure First' policy, as documented in the 'IT Service Deployment Framework (ITSDF-2025, Section 3.1)'. This framework mandates that all new IT services must be provisioned on existing virtualized or cloud-based shared resource platforms. Any request for dedicated physical hardware for a new service (covering compute, storage, or network elements) requires a formal exception request. This request must be justified based on technical infeasibility or significant adverse business impact if deployed on a shared platform, and necessitates approval from both the CIO and the relevant Business Unit Head. This process is designed to maximize resource utilization and minimize hardware sprawl.",
        "badExampleAnswer": "The application teams decide if they need their own servers for new projects. We just provide what they ask for."
      },
      "question_39": {
        "id": 39,
        "old_question_text": null,
        "questionText": "When deploying new IT services on new IT equipment, does the organization determine the business impact of service incidents for each deployed service and then deploy only the level of IT hardware resilience (e.g., redundant power supplies, RAID configurations, clustered servers) that is fully justified by this impact analysis? Is it ensured that the chosen IT hardware resilience level is formally agreed upon by the application owners?",
        "simplifiedQuestion": "For new services on new gear, do you use only the hardware resilience level truly needed and agreed by application owners, based on business impact?",
        "reference": "2025 CoC Guide 4.2.2",
        "priority": "High (Value: 4, Expected: New IT Equipment)",
        "stepSection": "step_3",
        "recommendationTitle": "Reduce IT hardware resilience level",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Business Impact Analysis (BIA) for new services. IT hardware specifications showing resilience features. Documentation of agreement with application owners on the chosen resilience level. Policy on matching hardware resilience to business needs."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT services on new equipment: Business Impact Analysis for the service. Documentation showing the selected IT hardware resilience level is justified by the BIA and agreed by application owners.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of BIAs for new services, hardware configuration documents, and sign-offs from application owners. Interviews with application owners and IT infrastructure teams.",
        "nonconformity": {
          "major": "New IT services are deployed on hardware with excessive resilience levels (e.g., default full redundancy) without justification from a business impact analysis or agreement from application owners.",
          "minor": "Business impact is considered informally, or hardware resilience decisions are not consistently documented or agreed with application owners."
        },
        "majorNonconformity": "Systematic deployment of new IT services on new hardware with resilience levels (e.g., dual PSUs, extensive RAID, server clustering) that are not justified by a formal business impact analysis of the service, or without documented agreement from application owners.",
        "minorNonconformity": "Hardware resilience levels for new services are determined without a formal BIA, or the agreement with application owners is not documented, leading to potential over-provisioning of resilience features.",
        "whatThisMeans": "When setting up new IT services on new hardware, don't just give everything maximum protection (like dual power supplies, complex disk setups, or server clusters) by default. First, figure out how critical each service is to the business if it has problems. Then, only build in the amount of hardware resilience that's really necessary and get the application owners to agree to it. Overdoing hardware resilience wastes energy and money.",
        "complianceRequires": "- For each new IT service deployed on new IT equipment, determine the business impact of potential service incidents.\n- Deploy only the level of IT hardware resilience (e.g., redundant PSUs, RAID levels, server clustering) that is fully justified by this business impact analysis.\n- Ensure that the selected IT hardware resilience level is formally agreed upon by the application owners responsible for the service.",
        "expertTip": "Develop a standard classification for service criticality (e.g., Tier 1, 2, 3) based on RTO/RPO and business impact. Then, define standard, justified hardware resilience profiles for each tier. This streamlines the process and ensures consistency, while still linking resilience to business need.",
        "recommendedEvidence": "- Business Impact Analysis (BIA) methodology and reports for new services.\n- Standard hardware configurations or resilience profiles for different service criticality tiers.\n- Documentation (e.g., service design document, sign-off sheet) showing agreement between IT and application owners on the resilience level for specific new services.\n- Examples of new server/storage configurations showing how resilience features were selected based on the BIA.",
        "recommendedActions": "- Integrate a Business Impact Analysis step into the new IT service design and deployment process.\n- Define clear criteria for different levels of IT hardware resilience.\n- Establish a formal sign-off process where application owners agree to the chosen hardware resilience level for their service, based on the BIA.\n- Avoid default 'gold plating' of hardware resilience; make it a conscious, justified decision.",
        "sampleResponseTemplate": "For all new services deployed on new IT equipment, we conduct a Business Impact Analysis (as per [BIA Procedure ID]). The resulting RTO/RPO determines the required hardware resilience tier, documented in our '[Service Tier Standard]' and agreed with application owners. For example, 'Service Omega' (BIA Ref: [BIA-Omega-2025]) was classified as Tier 2, requiring [specific resilience like N+1 PSUs, RAID 5], and this was signed off by [Application Owner Name].",
        "humanInputAnswer": "Our 'Service Deployment Framework (SDF-002)' requires a Business Impact Assessment (BIA) for every new service. Based on the BIA's RTO/RPO findings, a hardware resilience profile is selected from our pre-defined standards (e.g., Bronze, Silver, Gold). For example, our new CRM module (BIA-CRM-005) was classified as 'Silver', requiring N+1 power supplies and RAID 6 storage, but not full server clustering. This was formally agreed with the Head of Sales, the application owner. The default is not to deploy dual PSUs unless justified.",
        "llmGeneratedAnswer": "Yes, when deploying new IT services on new hardware, our 'Service Impact and Resilience Policy (SIRP-2025)' mandates a formal Business Impact Analysis (BIA). The BIA (template BIA-TMPL-001) quantifies the impact of potential incidents and defines the required RTO/RPO for the service. Based on this, only the justified level of IT hardware resilience is deployed. For instance, our 'New Product Catalog Service' (BIA Ref: NPC-BIA-2024) was determined to require N+1 power supply redundancy but not fully redundant server clustering due to its RPO. This resilience level was documented and signed off by the application owner (Product Management Director) prior to hardware procurement for the new service, as per SIRP-2025 Section 4.2.",
        "badExampleAnswer": "All our new servers get dual power supplies and RAID for reliability."
      },
      "question_40": {
        "id": 40,
        "old_question_text": null,
        "questionText": "When deploying new IT services that require new IT equipment, does the organization determine the business impact of service incidents to justify the level of Business Continuity / Disaster Recovery (BC/DR) standby IT equipment and its resilience? Is only the level of standby IT equipment and resilience that is fully justified by this business impact analysis deployed for the new service?",
        "simplifiedQuestion": "For new services on new gear, do you only set up the amount of standby/DR equipment that's truly justified by business impact analysis?",
        "reference": "2025 CoC Guide 4.2.3",
        "priority": "High (Value: 4, Expected: New IT Equipment)",
        "stepSection": "step_3",
        "recommendationTitle": "Reduce hot / cold standby equipment",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Business Impact Analysis (BIA) for new services focusing on BC/DR needs. BC/DR plans specifying standby equipment requirements. Justification for the level of standby equipment deployed, linked to BIA."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new IT services on new equipment: Business Impact Analysis for the service covering BC/DR. Documentation justifying the level of standby IT equipment and its resilience based on this BIA, avoiding unnecessary 'always-on' or overly redundant standby systems.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of BIAs, BC/DR plans, and specifications for standby IT equipment for new services. Interviews with business continuity planners and IT infrastructure teams regarding standby provisioning.",
        "nonconformity": {
          "major": "Excessive hot/cold standby IT equipment is deployed for new services without clear justification from a business impact analysis, leading to significant underutilized and powered assets.",
          "minor": "The level of standby IT equipment is not consistently justified by BIA, or the resilience of standby equipment itself is over-specified."
        },
        "majorNonconformity": "Deployment of significant hot or cold standby IT equipment for new services without a formal business impact analysis justifying this level of BC/DR provisioning, leading to continuously powered but underutilized assets.",
        "minorNonconformity": "Standby IT equipment for new services is provisioned without a rigorous BIA, or the chosen resilience/quantity of standby gear exceeds the documented business recovery requirements, resulting in unnecessary energy consumption.",
        "whatThisMeans": "When you set up new IT services that need new hardware, carefully assess how critical they are for business continuity and disaster recovery (BC/DR). Based on this business impact, decide exactly how much standby IT equipment (like servers ready to take over in another location, or 'hot spares') is truly needed, and what level of protection that standby gear needs. Avoid setting up too much standby equipment that just sits there powered on but unused, as this wastes energy.",
        "complianceRequires": "- For each new IT service requiring new IT equipment, determine the business impact of service incidents to justify Business Continuity/Disaster Recovery (BC/DR) needs.\n- Based on this impact analysis, deploy only the level of standby IT equipment (both quantity and its own resilience features) that is fully justified.\n- Avoid deploying excessive hot standby (continuously powered on and ready) or cold standby (powered off but ready to be activated) equipment if not warranted by the BC/DR requirements derived from the business impact.",
        "expertTip": "Instead of dedicated physical standby hardware for every service, leverage virtualization and cloud-based DR solutions (DRaaS) where appropriate. These can often provide required RTO/RPOs with lower energy consumption and cost for standby capacity, as resources are provisioned on demand or replicated efficiently.",
        "recommendedEvidence": "- Business Impact Analysis (BIA) reports for new services, specifically addressing BC/DR requirements (RTO/RPO).\n- BC/DR plans for new services detailing the standby IT equipment strategy (e.g., hot, warm, cold standby; physical vs. virtual; on-premise vs. cloud DR).\n- Justification documents linking the amount and resilience level of deployed standby IT equipment directly to the BIA findings for new services.\n- Inventory of standby IT equipment and its utilization/power status.",
        "recommendedActions": "- Integrate BC/DR requirements gathering, based on BIA, into the design phase for all new IT services using new equipment.\n- Formally justify the quantity, type, and power status (hot/cold) of any standby IT equipment provisioned.\n- Explore and adopt modern DR strategies (e.g., virtualization-based replication, cloud DR) that can reduce the need for dedicated, continuously powered physical standby hardware.\n- Regularly review and right-size standby IT equipment based on evolving business needs and technological capabilities.",
        "sampleResponseTemplate": "Our BC/DR planning for new services on new equipment is driven by a Business Impact Analysis (Ref: [BIA Procedure ID]). For 'Service Alpha', the BIA justified [e.g., 'cold standby VMs at our DR site with 4-hour RTO'], rather than dedicated hot standby physical servers. This approach, documented in '[DR Plan Alpha]', minimizes powered, underutilized assets. The standby equipment for Service Alpha is [describe its own resilience level if applicable].",
        "humanInputAnswer": "Our 'Business Continuity Planning Policy (BCP-001)' requires a BIA for all new critical services. For our recently launched 'ClientX Portal' (BIA Ref: BIA-CXP-2024), the analysis determined an RTO of 4 hours and RPO of 1 hour. Consequently, we provisioned replicated virtual machines to our DR site, which are restored on demand, instead of deploying dedicated, continuously powered hot-standby physical servers. This significantly reduced the energy footprint of our DR solution for this service.",
        "llmGeneratedAnswer": "Yes, the deployment of Business Continuity/Disaster Recovery (BC/DR) standby IT equipment for new services is strictly governed by business impact. Our 'Service Resilience and Continuity Standard (SRCS-2025, Section 5)' mandates that for any new IT service requiring new IT equipment, a BIA must determine the RTO and RPO. Based on these, the level of standby IT equipment (e.g., quantity of servers, storage capacity) and its operational state (hot, warm, cold) is justified and documented. For example, our 'New Inventory Management Service (NIMS)' required an RTO of 2 hours; the BIA justified warm standby virtual machines at our secondary site, rather than a fully replicated, continuously powered hot standby environment, thus minimizing energy consumption for the standby resources. This decision is recorded in 'NIMS_DR_Plan_v1.pdf'.",
        "badExampleAnswer": "We replicate all our important new servers to our DR site and keep them powered on so we can failover quickly if needed."
      },
      "question_41": {
        "id": 41,
        "old_question_text": null,
        "questionText": "Has the organization made the energy use performance of software a primary selection factor when procuring new software? While specific forecasting and measurement tools are still developing, does the organization use approximations, such as the (under load) power draw of the hardware required to meet performance and availability targets for different software options, extending existing capacity planning and benchmarking processes to evaluate software efficiency? (Referencing Section 11 'Items under consideration' for 'Further development of software efficiency definitions').",
        "simplifiedQuestion": "When buying new software, is its energy efficiency a primary factor in your decision, considering the hardware it would need?",
        "reference": "2025 CoC Guide 4.2.4",
        "priority": "High (Value: 5, Expected: New Software)",
        "stepSection": "step_3",
        "recommendationTitle": "Select efficient software",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Software procurement policies or guidelines specifying energy efficiency as a selection criterion. Evaluation criteria used in recent software tenders showing weighting for energy/resource efficiency. Analysis comparing hardware requirements/power draw for different software options considered."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new software procurement: Procurement policies that list software energy use performance (or its proxy, such as required hardware power draw) as a primary selection factor. Evidence from recent software selections showing this criterion was applied.",
        "verification_method": [
          "Document Review",
          "Process Verification"
        ],
        "verificationMethods": "Review of software procurement policies, tender evaluation frameworks, and documentation from recent software selection processes.",
        "nonconformity": {
          "major": "Energy efficiency or resource demand of new software is not considered a primary selection factor in procurement.",
          "minor": "Software efficiency is considered but not as a primary factor, or the methods for approximating its energy impact are not well-defined or consistently applied."
        },
        "majorNonconformity": "Failure to make the energy use performance of new software (or its hardware resource demand as a proxy) a primary selection factor during procurement.",
        "minorNonconformity": "Software efficiency is a selection criterion but given low priority, or the methodology for assessing it (e.g., hardware power draw projections) is not robust or consistently used.",
        "whatThisMeans": "When your organization buys new software, how much energy it (and the hardware it needs to run on) will consume should be a major factor in your decision. Even though exact software energy measurement tools are still evolving, you can estimate this by looking at the power an application will likely cause the hardware to draw to meet performance targets. This should be part of your normal capacity planning when choosing software.",
        "complianceRequires": "- Make the energy use performance of software a primary selection factor during procurement.\n- In the absence of direct software energy metrics, use approximations such as the (under load) power draw of the hardware required to meet performance and availability targets for different software alternatives.\n- Extend existing capacity planning and benchmarking processes to include these software efficiency considerations.\n- Refer to Section 11 of the Guide ('Items under consideration') for ongoing developments in software efficiency definitions and metrics.",
        "expertTip": "When evaluating software, request vendors to provide information on typical hardware resource requirements (CPU, memory, I/O) for expected transaction volumes or user loads. Use this to model the potential power draw on your standard hardware platforms. Software that is more computationally efficient will generally require less hardware or lower-powered hardware, leading to energy savings.",
        "recommendedEvidence": "- Software procurement policy explicitly stating energy/resource efficiency as a primary selection criterion.\n- RFP templates for software including sections on energy efficiency or hardware resource requirements.\n- Evaluation reports from recent software procurements showing how different options were compared based on their potential energy impact (e.g., projected hardware power draw).\n- Capacity planning documents that incorporate software efficiency considerations.",
        "recommendedActions": "- Update software procurement policies to include energy use performance (or its hardware demand proxy) as a primary selection criterion.\n- Develop a methodology for estimating and comparing the potential energy impact of different software solutions.\n- Train procurement and IT teams on how to evaluate software for energy efficiency.\n- Engage with software vendors to encourage them to provide more data on the energy and resource efficiency of their products.",
        "sampleResponseTemplate": "Our 'Software Acquisition Policy' ([Policy ID SAQ-002]) mandates that the projected energy footprint (based on required hardware to meet SLAs) is a primary selection factor, weighted at [X]%. For the recent [Software Type] procurement, we compared three vendors based on the server infrastructure needed to support our [Y] users/transactions per second. Vendor A was selected partly because its solution required an estimated 20% less server power draw, as detailed in '[Evaluation Report Z]'.",
        "humanInputAnswer": "Our software procurement process, documented in 'Vendor Software Selection SOP VSS-003', requires an 'Efficiency Impact Assessment' for all major software purchases. This involves estimating the hardware resources needed to meet our performance SLAs (transactions per minute, user concurrency). For our recent CRM system selection, 'VendorX CRM' was chosen over 'VendorY CRM' partly because it demonstrated the ability to meet our load requirements on 2 fewer virtual servers, translating to an estimated 1.5kW lower continuous power draw. This assessment is a primary decision factor.",
        "llmGeneratedAnswer": "Yes, the organization's 'Software Procurement and Lifecycle Management Policy (SPLMP-2025, Section 4.2)' explicitly states that the energy use performance of software is a primary selection factor. Given the developing nature of direct software energy metrics, our current practice involves evaluating the projected hardware power consumption required by competing software solutions to meet defined performance and availability targets for our specific workloads. This analysis, which extends our standard capacity planning and benchmarking, is a mandatory component of the technical evaluation for all new enterprise software. For example, in the Q1 2025 procurement of our new ERP module, the chosen solution demonstrated a 15% lower projected server power footprint compared to the runner-up, based on our standardized hardware profiles.",
        "badExampleAnswer": "We choose software based on its features, usability, and cost. We assume it will run on our existing servers."
      },
      "question_42": {
        "id": 42,
        "old_question_text": null,
        "questionText": "When developing new software in-house, has the organization made the energy use performance of the software a major success factor of the project? While forecasting and measurement tools are still developing, does the organization use approximations, such as the (under load) power draw of the hardware required to meet performance and availability targets, as a key metric? Is performance optimization, which directly impacts energy efficiency, treated as a high-impact area and not compromised to reduce project budgets? (Referencing Section 11 'Items under consideration' for 'Further development of software efficiency definitions').",
        "simplifiedQuestion": "For software you develop in-house, is its energy efficiency a key project goal, and do you avoid cutting corners on performance optimization?",
        "reference": "2025 CoC Guide 4.2.5",
        "priority": "High (Value: 5, Expected: New Software)",
        "stepSection": "step_3",
        "recommendationTitle": "Develop efficient software",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "md"
          ],
          "description": "Software development lifecycle (SDLC) policies or guidelines that include energy efficiency as a success factor. Project charters or requirements documents for in-house software showing energy/resource efficiency targets. Performance testing reports that include resource utilization metrics."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, md",
        "documentDescriptor": "Mandatory for in-house software development: SDLC guidelines or project documentation making software energy use performance (or its hardware resource demand proxy) a major success factor. Evidence that performance optimization is prioritized.",
        "verification_method": [
          "Document Review",
          "Interview",
          "Code Review (spot check if feasible)"
        ],
        "verificationMethods": "Review of SDLC policies, coding standards, project requirements, and performance testing strategies. Interviews with development managers and lead developers.",
        "nonconformity": {
          "major": "Energy efficiency or resource optimization is not a defined success factor for in-house software development projects; performance optimization is frequently de-scoped for budget/schedule reasons.",
          "minor": "Software efficiency is mentioned as a goal but lacks measurable targets or is not consistently prioritized during development and testing."
        },
        "majorNonconformity": "Absence of making energy use performance (or its hardware resource demand proxy) a major success factor in the in-house software development lifecycle; performance optimization related to efficiency is consistently compromised.",
        "minorNonconformity": "Energy efficiency is a stated goal for software development but lacks defined metrics or is often deprioritized against other project constraints without formal impact assessment.",
        "whatThisMeans": "When your organization develops its own software, making that software energy-efficient should be a key goal for the project. Just like meeting deadlines or feature requirements, its impact on hardware energy use should be considered important. This means performance optimization (making the software run fast and use resources well) shouldn't be an area where corners are cut to save time or money, as this directly affects energy consumption.",
        "complianceRequires": "- Make the energy use performance of in-house developed software a major success factor of the development project.\n- Utilize approximations for energy impact, such as the (under load) power draw of the hardware required to meet performance and availability targets, as a key metric during development and testing.\n- Treat performance optimization (which contributes to energy efficiency) as a high-impact area that should not be compromised to reduce project budgets or timelines without significant justification.\n- Refer to Section 11 of the Guide for ongoing developments in software efficiency definitions and metrics.",
        "expertTip": "Incorporate resource efficiency (CPU cycles, memory usage, I/O operations) as key metrics in your automated testing and CI/CD pipelines for in-house software. Set performance budgets for these metrics and fail builds that regress significantly. This helps catch inefficiencies early in the development cycle.",
        "recommendedEvidence": "- Software Development Lifecycle (SDLC) documentation or coding standards that include energy/resource efficiency as a development goal or success criterion.\n- Project charters or requirements documents for in-house developed software that specify performance and efficiency targets.\n- Examples of performance testing reports for in-house software that include analysis of resource utilization (CPU, memory, estimated power impact).\n- Documentation showing that performance optimization tasks were prioritized and completed, even under project pressures.",
        "recommendedActions": "- Integrate energy and resource efficiency goals into your SDLC and definition of 'done' for software projects.\n- Train developers on energy-efficient coding practices and performance optimization techniques.\n- Implement tools for profiling software resource consumption during development and testing.\n- Ensure project management methodologies prioritize performance optimization tasks appropriately and resist de-scoping them without strong justification.",
        "sampleResponseTemplate": "Our 'Secure & Efficient SDLC Policy' ([Policy ID SDLC-003]) defines resource efficiency as a major success factor for all in-house developed software. Project [ProjectName], for example, had a target to reduce query response times by [X]% and CPU utilization by [Y]% on its target hardware, which directly translates to lower power draw. Performance optimization is a dedicated phase in our sprint planning.",
        "humanInputAnswer": "Our internal 'Software Development Guideline SDG-001' mandates that resource efficiency (CPU, memory, I/O) is a key quality attribute and success factor for all new applications. During development, we use profiling tools like Dynatrace to monitor resource consumption under simulated load. For our 'CustomerAnalytics Engine v2.0' project, a specific goal was to reduce the per-query CPU time by 30% compared to v1.0, which was achieved through algorithm optimization and directly led to a lower hardware footprint for the same throughput.",
        "llmGeneratedAnswer": "Yes, for all in-house software development, our 'Engineering Excellence Framework (EEF-2025, Section 5.4: Resource Optimization)' establishes energy use performance as a major project success factor. While direct energy measurement of software is complex, we use proxy metrics such as CPU cycles, memory footprint, and I/O operations per transaction, with targets set to minimize the hardware resources required to meet performance and availability SLAs. Performance optimization is a dedicated workstream in our agile development process and is explicitly protected from budget cuts unless a formal impact assessment on energy consumption is approved by the Architecture Review Board. This aligns with our understanding of the developing nature of software efficiency definitions as noted in CoC Section 11.",
        "badExampleAnswer": "Our developers focus on getting the features working on time. Performance tuning happens later if there are complaints or issues."
      }
    }
  },
  "step_4": {
    "description": "2025 Guide Section 4: IT Equipment and Services (Practices 4.2.6-4.2.9 from Sec 4.2; and 4.3.1-4.3.4 from Sec 4.3)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_IT_Equip_SVC_3"
    ],
    "critical_questions": [],
    "questions": {
      "question_43": {
        "id": 43,
        "old_question_text": null,
        "questionText": "If software development is outsourced, has the organization included the energy use performance of the software (or its proxy, such as required hardware power draw under load to meet performance/availability targets) in the bonus/penalty clauses of the contract? Is performance optimisation, which impacts energy efficiency, treated as a non-negotiable, high-impact area rather than one to be compromised for budget reasons in such contracts? (Referencing Section 11 'Items under consideration' for 'Further development of software efficiency definitions').",
        "simplifiedQuestion": "If you outsource software development, do your contracts incentivize or penalize based on the software's energy efficiency?",
        "reference": "2025 CoC Guide 4.2.6",
        "priority": "High (Value: 5, Expected: Optional Practice)",
        "stepSection": "step_4",
        "recommendationTitle": "Incentives to develop efficient software",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx"
          ],
          "description": "Standard contract templates for outsourced software development that include clauses related to energy/resource efficiency performance, bonuses for achieving targets, or penalties for inefficiency. Examples of contracts with such clauses implemented."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx",
        "documentDescriptor": "Optional: Contract templates or actual contracts for outsourced software development that include clauses linking payment/penalties to the energy use performance of the delivered software (or its hardware demand proxy).",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of contract templates and specific outsourced software development agreements. Interviews with procurement and vendor management staff.",
        "nonconformity": {
          "major": null,
          "minor": "Contracts for outsourced software development do not include any clauses or incentives related to the energy efficiency or resource performance of the developed software (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to include energy use performance (or its hardware demand proxy) as a incentivized/penalized metric in contracts, or allowing performance optimization to be easily compromised in outsourced projects.",
        "whatThisMeans": "If you pay another company to develop software for you, your contract with them should encourage them to make that software energy-efficient. This could mean offering them a bonus if the software runs efficiently (uses less hardware power) or applying a penalty if it's inefficient. The contract should make it clear that making the software perform well and efficiently is a key requirement, not something that can be skipped to save money on the project.",
        "complianceRequires": "- If software development is outsourced, include the energy use performance of the software in the bonus/penalty clauses of the contract.\n- Use approximations for energy impact, such as the (under load) power draw of the hardware required to meet performance and availability targets, as the basis for these clauses.\n- Treat performance optimization as a high-impact area in contract negotiations, ensuring it is not compromised for budgetary reasons.\n- Refer to Section 11 of the Guide for ongoing developments in software efficiency definitions and metrics that could inform such contractual clauses.",
        "expertTip": "When including efficiency clauses in outsourced development contracts, clearly define the target hardware platform, workload profile, and specific, measurable resource consumption targets (e.g., CPU utilization, memory footprint per user/transaction) that will be used for acceptance testing. Tie a percentage of the contract value or specific milestone payments to meeting these efficiency targets.",
        "recommendedEvidence": "- Standard contract addendums or clauses for outsourced software development that address energy/resource efficiency.\n- Examples of specific contracts with outsourced developers that include bonus/penalty clauses related to software performance and efficiency.\n- Documentation of the metrics and testing methodology used to assess the efficiency of outsourced software for contractual purposes.",
        "recommendedActions": "- Develop standard contractual language that incorporates energy/resource efficiency requirements, targets, and bonus/penalty mechanisms for outsourced software development.\n- Include these clauses in RFPs and contract negotiations with software development vendors.\n- Establish clear, measurable acceptance criteria for the energy efficiency of software developed by third parties.\n- Ensure your organization has the capability to test and verify the efficiency of delivered software against these contractual requirements.",
        "sampleResponseTemplate": "For outsourced software development, our standard 'Master Services Agreement' ([Agreement Template ID]) includes a 'Performance and Efficiency Addendum' (Addendum C). This addendum specifies that [X]% of the final project payment is tied to the software meeting predefined resource utilization targets (e.g., CPU utilization < Y% under Z load on our standard test hardware). Performance optimization is defined as a key deliverable. For project [OutsourcedProjectName] with [VendorName], these clauses [were/are being] applied.",
        "humanInputAnswer": "While we primarily develop software in-house, for a recent outsourced mobile application project (Project 'ConnectSphere' with Vendor 'MobileDev Inc.'), our contract (Ref: MSA-MD-2024-001, Appendix B) included a 5% bonus tied to the application achieving a battery consumption rate 10% below a pre-defined benchmark on target devices, and a clause stipulating that performance optimization to meet responsiveness SLAs was a non-negotiable deliverable. This helped ensure the vendor prioritized efficient coding practices.",
        "llmGeneratedAnswer": "Yes, where software development is outsourced, our 'Vendor Engagement Framework for Software Development (VEFSD-2025)' incorporates energy efficiency considerations. Section 7.3 of the VEFSD outlines performance-based incentives. While direct energy use metrics are still evolving (as noted in CoC Section 11), we contractually specify maximum allowable hardware resource footprints (CPU, RAM) for the developed software to meet defined service level objectives on our target platforms. Performance optimization is stipulated as a key deliverable, and failure to meet resource efficiency targets can result in a holdback of up to 10% of the project value, or a bonus of up to 5% for exceeding targets by a significant margin. This is currently being applied to the 'Customer Loyalty Platform' being developed by [Vendor Name].",
        "badExampleAnswer": "We hire outside developers for some projects, but we mostly focus on getting the features we need. We don't usually specify energy use in their contracts."
      },
      "question_44": {
        "id": 44,
        "old_question_text": null,
        "questionText": "Has the organization determined the business impact of short service incidents for each deployed service and, where appropriate for new IT equipment deployments, replaced traditional active/passive server hardware clusters with fast recovery approaches, such as restarting virtual machines elsewhere, for services that can tolerate such recovery methods? (This does not refer to grid deployments or High-Performance Compute clusters).",
        "simplifiedQuestion": "Do you use fast VM recovery methods instead of traditional active/passive hardware clusters for services that can tolerate it, to save energy?",
        "reference": "2025 CoC Guide 4.2.7",
        "priority": "Medium (Value: 4, Expected: Optional Practice)",
        "stepSection": "step_4",
        "recommendationTitle": "Eliminate traditional 2N hardware clusters",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Service design documents, BIA for services, IT architecture showing use of fast VM recovery instead of active/passive clusters. Justification for choices made regarding clustering vs. VM recovery for different services."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: Documentation of Business Impact Analysis for services. Service architecture designs that favor fast VM recovery over traditional active/passive hardware clusters where appropriate. Justification for any remaining active/passive clusters.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of service design documents, BIA, and IT architecture decisions regarding high availability. Interviews with IT architects and service owners.",
        "nonconformity": {
          "major": null,
          "minor": "Traditional active/passive hardware clusters are still the default for high availability, even for services where fast VM recovery could meet business RTO/RPO and be more energy efficient (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If relevant services exist and this practice is adopted: Continued deployment of energy-intensive active/passive hardware clusters for services where faster, more efficient virtual machine recovery mechanisms could meet documented business continuity requirements.",
        "whatThisMeans": "Instead of using traditional server clusters where one server is active and another is just a powered-on spare (active/passive), consider if your IT services can tolerate a very quick restart on a different virtual machine if something fails. For many applications, this 'fast VM recovery' can provide the needed uptime much more energy-efficiently than keeping dedicated physical spare servers running. This doesn't apply to specialized setups like grid computing or HPC clusters.",
        "complianceRequires": "- Determine the business impact of short service incidents for each deployed service.\n- For services where this is acceptable, replace traditional active/passive server hardware clusters with fast recovery approaches (e.g., restarting virtual machines on alternative hosts).\n- This applies to new IT equipment deployments for such services.\n- This practice is not intended for grid deployments or High-Performance Compute (HPC) clusters which have different architectural needs.",
        "expertTip": "Leverage features in modern virtualization platforms like VMware HA (High Availability) or Hyper-V Failover Clustering, which can automatically restart VMs on other hosts in minutes. Ensure your storage and network infrastructure can support these rapid failovers. Quantify the energy savings from avoiding dedicated, often underutilized, passive cluster nodes.",
        "recommendedEvidence": "- Service Level Agreements (SLAs) or Business Impact Analyses (BIAs) defining acceptable recovery times (RTO) for various services.\n- IT architecture documents or policies showing a preference for VM-level high availability (e.g., VMware HA, Hyper-V Failover Cluster) over traditional physical active/passive clusters where RTOs permit.\n- Examples of services designed or migrated to use fast VM recovery instead of dedicated physical clusters.\n- Configuration details of virtualization platform high availability features.",
        "recommendedActions": "- For new service deployments, evaluate if business RTO/RPO can be met by fast VM recovery mechanisms rather than physical active/passive clusters.\n- For existing services using active/passive clusters, assess if they can be migrated to a virtualized HA environment during a tech refresh or major upgrade.\n- Ensure virtualization hosts and shared storage are robustly configured to support reliable and fast VM recovery.\n- Document the RTO capabilities of your virtualized HA solutions and match them to service requirements.",
        "sampleResponseTemplate": "Our standard high availability strategy for [Tier X services], documented in '[HA Design Standard ID]', prioritizes fast VM recovery (using [VMware HA/Hyper-V Failover Clustering]) over traditional active/passive hardware clusters. For service [Service Y], the BIA determined an RTO of [e.g., 15 minutes], which is met by our VM HA capabilities, thus avoiding the need for dedicated physical cluster nodes. We avoid active/passive clusters unless specifically required by [application vendor/unique performance needs].",
        "humanInputAnswer": "Our default HA strategy for Tier 2 applications, as outlined in 'IT Resiliency Standards v3.0', is to utilize VMware vSphere High Availability, which restarts VMs on other hosts in the cluster within minutes. This approach, used for over 80% of our new virtualized service deployments, avoids the need for dedicated physical active/passive clusters, significantly reducing idle hardware. For instance, our internal collaboration suite was designed this way, meeting its 30-minute RTO without dedicated standby servers.",
        "llmGeneratedAnswer": "Yes, our organization prioritizes more energy-efficient high-availability solutions over traditional active/passive hardware clusters where business impact allows. Our 'Service Resiliency Design Guide (SRDG-2025, Section 6)' requires an assessment of service incident impact (RTO/RPO needs). For services that can tolerate brief recovery times (e.g., RTO > 15 minutes), we leverage our virtual infrastructure's fast VM recovery capabilities (e.g., VMware HA) on new IT equipment deployments. This is preferred to deploying dedicated physical standby servers. This approach is standard for our internal web services and development platforms, but not for our real-time transaction processing systems (which are outside the scope of this specific CoC practice regarding HPC/grid).",
        "badExampleAnswer": "All our critical systems are set up in active/passive clusters to ensure immediate failover. We don't want to risk any downtime."
      },
      "question_45": {
        "id": 45,
        "old_question_text": null,
        "questionText": "Has the organization set minimum or average utilization targets for its IT equipment (servers, networking, storage)? Is it understood that this practice presents uncertainty without considering load-to-power profiles, the impact of cloud/mobile services, and dynamic workload relocation capabilities, and that this is a specialist area being examined by bodies specializing in this field (referencing ISO/IEC 30134-5 ITEUsv and ISO/IEC 21836 SEEM)?",
        "simplifiedQuestion": "Do you set utilization targets for your IT gear (servers, network, storage), understanding the complexities involved?",
        "reference": "2025 CoC Guide 4.2.8",
        "priority": "Medium (Value: 4, Expected: Optional Practice)",
        "stepSection": "step_4",
        "recommendationTitle": "IT equipment utilisation",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Documentation of IT equipment utilization targets (servers, network, storage). Reports showing actual utilization against these targets. References to standards like ITEUsv or SEEM if used."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: Defined minimum or average utilization targets for IT equipment categories. Monitoring reports showing actual utilization against these targets. Awareness/use of relevant ISO standards (ITEUsv, SEEM) is a plus.",
        "verification_method": [
          "Document Review",
          "Data Analysis",
          "Interview"
        ],
        "verificationMethods": "Review of any documented utilization targets and monitoring reports. Analysis of IT equipment utilization data. Interviews with IT operations and capacity planning teams.",
        "nonconformity": {
          "major": null,
          "minor": "No utilization targets are set for IT equipment, or targets are set but not monitored or acted upon (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to establish and monitor minimum or average utilization targets for key IT equipment (servers, networking, storage), or setting targets without due consideration for the complexities mentioned (load profiles, dynamic workloads, relevant standards).",
        "whatThisMeans": "Your organization should aim to use its IT equipment (servers, network gear, storage) efficiently by setting targets for how much it's being used (its utilization). For example, you might aim for servers to be, on average, 60% busy rather than 10%. It's a complex area because server power use doesn't always scale directly with utilization, and things like cloud services and workloads moving around dynamically add more complexity. There are ISO standards like ITEUsv and SEEM that provide guidance in this specialist area.",
        "complianceRequires": "- Set minimum or average utilization targets for IT equipment (servers, networking, storage).\n- Acknowledge the complexities in this area, including:\n    - The relationship between IT load and power profiles of equipment.\n    - The impact of cloud and mobile services on utilization patterns.\n    - The increasing ability to dynamically relocate IT compute functions.\n- Be aware that this is a specialist area with ongoing research and standardization efforts (e.g., ISO/IEC 30134-5 IT Equipment Utilisation for servers (ITEUsv), ISO/IEC 21836 Server energy effectiveness metric (SEEM)).",
        "expertTip": "When setting utilization targets, differentiate between peak and average utilization, and consider the type of workload. For example, a batch processing server might have very high utilization during its run, while an interactive web server needs headroom for bursts. Use monitoring tools that can track utilization over time and alert on sustained underutilization, triggering a review for consolidation or decommissioning.",
        "recommendedEvidence": "- Document defining IT equipment utilization targets (e.g., % CPU utilization for servers, % port utilization for network switches, % capacity utilization for storage).\n- Reports from IT monitoring systems showing actual utilization levels against these targets.\n- Capacity planning documents that reference utilization targets.\n- Evidence of processes to address underutilized equipment (e.g., consolidation initiatives, decommissioning policies).\n- If ISO/IEC 30134-5 (ITEUsv) or ISO/IEC 21836 (SEEM) are used, provide relevant reports or data.",
        "recommendedActions": "- Establish baseline utilization levels for key IT equipment categories.\n- Set realistic but challenging minimum or average utilization targets.\n- Implement tools and processes to continuously monitor IT equipment utilization.\n- Regularly review utilization reports and identify underutilized assets.\n- Develop and implement strategies to improve utilization (e.g., virtualization, workload consolidation, decommissioning of idle equipment).\n- Stay informed about evolving standards and metrics for IT equipment utilization and energy effectiveness.",
        "sampleResponseTemplate": "Our organization has set IT equipment utilization targets: [e.g., >60% average CPU for production servers, >70% for storage capacity]. These are documented in our '[Capacity Management Plan ID]' and monitored via '[Monitoring Tool Name]'. We are aware of standards like ITEUsv and are evaluating their incorporation. Underutilized assets are reviewed quarterly for [consolidation/decommissioning].",
        "humanInputAnswer": "We have established average utilization targets for our production server fleet at 65% CPU and 70% memory, as per our 'IT Resource Optimization Policy IROP-001'. These are tracked using vRealize Operations. For storage, our target is 75% capacity utilization before triggering expansion. While we monitor network port utilization, specific targets are still under development due to the dynamic nature of our network traffic. We are currently reviewing ISO/IEC 30134-5 (ITEUsv) for potential adoption in our server efficiency metrics.",
        "llmGeneratedAnswer": "Yes, our 'IT Asset Performance Standard (ITAPS-2025)' sets minimum average utilization targets for key IT equipment categories. For servers, the target is 60% CPU utilization over a 24-hour period. For primary storage arrays, the target is 75% capacity utilization. These targets are monitored through our DCIM and IT monitoring platforms. We acknowledge the complexities cited, such as non-linear power-to-load profiles and dynamic workload relocation, and thus use these targets as primary indicators for further investigation and optimization rather than absolute efficiency measures. Our advanced analytics team is currently evaluating the methodologies described in ISO/IEC 30134-5 (ITEUsv) and ISO/IEC 21836 (SEEM) for more nuanced energy effectiveness assessment.",
        "badExampleAnswer": "Our servers are busy when they need to be. We don't set specific utilization targets as long as applications are performing well."
      },
      "question_46": {
        "id": 46,
        "old_question_text": null,
        "questionText": "When purchasing new cloud services or assessing a cloud strategy, does the organization assess the impact on its own network equipment usage and the potential increase or decrease in energy consumption, with the aim of informing purchasing decisions? Does this assessment, at a minimum, include elements inside the data centre, with an ambition to include overall energy consumption and efficiency related to multiple site operations and the network energy use between those sites?",
        "simplifiedQuestion": "When moving to cloud, do you assess how it affects your own network's energy use, both within your DC and potentially wider?",
        "reference": "2025 CoC Guide 4.2.9",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_4",
        "recommendationTitle": "Network Energy Use",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Cloud strategy documents or new cloud service procurement evaluations that include an assessment of the impact on internal network equipment usage and energy consumption. Analysis comparing on-premise vs. cloud networking energy implications."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: Documentation showing that assessments of cloud service adoption include an analysis of the impact on internal network energy use (both within the DC and potentially inter-site networking).",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of cloud strategy documents, procurement evaluations for cloud services, and any related network energy impact assessments. Interviews with IT strategists and network architects.",
        "nonconformity": {
          "major": null,
          "minor": "Adoption of cloud services occurs without assessing the impact on the organization's own network equipment energy consumption (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursuing cloud strategies and adopting this practice: Failure to assess the impact of new cloud services on internal network equipment utilization and energy consumption, potentially leading to inefficient legacy network infrastructure or unforeseen energy increases.",
        "whatThisMeans": "When you're considering using new cloud services or developing your cloud strategy, you should think about how this will change the use of your own network equipment and its energy consumption. This assessment should at least cover the network gear inside your data centre, but ideally, it should also look at the bigger picture, including energy used for networking between your sites if you operate multiple locations. This helps make informed decisions about cloud adoption.",
        "complianceRequires": "- When purchasing new cloud services or assessing a cloud strategy, conduct an assessment of the impact on the organization's own network equipment usage.\n- Evaluate the potential increase or decrease in energy consumption of this internal network equipment as a result of cloud adoption.\n- Use this assessment to inform purchasing decisions related to cloud services.\n- The minimum scope for this assessment should include network elements inside the data centre.\n- The ambition should be to extend this assessment to include overall energy consumption and efficiency, particularly if it involves multiple site operations and the network energy use between those sites.",
        "expertTip": "When migrating services to the cloud, analyze the resulting changes in internal network traffic patterns. This can identify opportunities to decommission or consolidate underutilized internal network equipment, leading to direct energy savings. Conversely, increased traffic to cloud services might necessitate upgrades to internet connectivity and edge networking, whose energy impact should be factored in.",
        "recommendedEvidence": "- Cloud strategy documents or business cases for cloud service adoption that include a section on network impact assessment.\n- Analysis reports showing the projected or actual impact of cloud adoption on internal network device utilization and energy consumption.\n- Network architecture diagrams before and after significant cloud adoption, highlighting changes relevant to energy use.\n- Procurement decisions for cloud services where network energy impact was a documented consideration.",
        "recommendedActions": "- Incorporate an 'internal network energy impact assessment' into the evaluation process for all new cloud service adoptions or major cloud strategy shifts.\n- Monitor utilization and energy consumption of internal network equipment (especially edge routers, firewalls, and core switches) as workloads migrate to/from the cloud.\n- Based on these assessments, identify opportunities to right-size, consolidate, or decommission internal network infrastructure.\n- Consider the energy efficiency of network solutions required for optimal cloud connectivity.",
        "sampleResponseTemplate": "Our 'Cloud Adoption Framework' ([Doc ID: CAF-001]) includes a mandatory 'Network Impact & Energy Assessment' phase. Before migrating [Service X] to [Cloud Provider Y], we assessed that it would [increase/decrease] our internal data centre network load by [Z%], with an estimated energy impact of [+/- W kWh]. This informed our decision on [e.g., 'decommissioning N legacy switches' / 'upgrading our internet edge routers to more efficient models'].",
        "humanInputAnswer": "As part of our 'Cloud Migration Strategy Phase 2' (CMSP2-Report.pdf), we conducted a network energy impact assessment. Migrating our internal file services to a cloud storage solution is projected to reduce internal LAN traffic significantly, allowing us to decommission two older core switches, saving an estimated 1.5kW. However, this will increase load on our internet edge routers, and we are evaluating newer, more energy-efficient models for that function as part of the project.",
        "llmGeneratedAnswer": "Yes, our 'Cloud Service Procurement Policy (CSPP-2025, Section 5.2)' requires an assessment of the impact on internal network equipment usage and associated energy consumption when considering new cloud services. This analysis primarily focuses on network devices within our data centres that facilitate cloud connectivity (e.g., edge routers, firewalls). For example, our recent decision to adopt [SaaS Platform X] included an evaluation (Ref: 'CloudImpact_SaaSX_NetEnergy.pdf') projecting a 10% increase in load on our primary internet gateways, which was factored into capacity planning and an ongoing review of their energy efficiency. We are developing methodologies to extend this analysis to inter-site network energy use for hybrid cloud scenarios.",
        "badExampleAnswer": "When we move services to the cloud, we assume our internal network load goes down, which should save some energy on those old switches."
      },
      "question_47": {
        "id": 47,
        "old_question_text": null,
        "questionText": "Has the organization audited its existing physical IT hardware estate and logical IT service estate to establish what equipment is currently in place and precisely which service(s) each piece of equipment delivers? Has consideration been given to implementing an ITIL-type Configuration Management Database (CMDB) and a Service Catalogue to support this, potentially referencing ISO 55000 (Asset Management, as per Practice 3.2.7) for a structured and effective approach to managing these assets and their service mappings?",
        "simplifiedQuestion": "Have you audited your current IT hardware and the services they run, possibly using a CMDB/Service Catalogue (like ITIL or ISO 55000 principles)?",
        "reference": "2025 CoC Guide 4.3.1",
        "priority": "High (Value: 5, Expected: Entire Data Centre)",
        "stepSection": "step_4",
        "recommendationTitle": "Audit existing physical and service estate",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "xlsx",
            "docx"
          ],
          "description": "IT asset inventory, service catalogue, CMDB extracts or documentation, audit reports of the IT estate, procedures for conducting these audits and maintaining the data. Link to ISO 55000 implementation if applicable."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, xlsx, docx",
        "documentDescriptor": "Mandatory: Documented evidence of a comprehensive audit of the physical and logical IT estate (e.g., asset inventory linked to a service catalogue or CMDB). If a CMDB/Service Catalogue is implemented, provide evidence of its use and maintenance. Link to ISO 55000 if applicable.",
        "verification_method": [
          "Document Review",
          "System Audit",
          "Interview"
        ],
        "verificationMethods": "Review of IT asset registers, service catalogues, CMDB data and structure, and audit procedures. Interviews with IT asset managers, CMDB administrators, and service owners to verify accuracy and completeness.",
        "nonconformity": {
          "major": "No comprehensive or systematic audit of the existing IT physical and service estate; lack of accurate mapping between hardware and the services they support.",
          "minor": "Audits are infrequent or incomplete; CMDB/Service Catalogue (if present) is not well-maintained or does not accurately reflect the current state of the IT estate and service dependencies."
        },
        "majorNonconformity": "Absence of a current, accurate, and comprehensive inventory mapping physical IT assets to the specific business services they support. Significant discrepancies found between documented and actual state.",
        "minorNonconformity": "An IT asset inventory or CMDB/Service Catalogue exists but is not regularly updated, contains significant inaccuracies, or the service mapping is incomplete or superficial.",
        "whatThisMeans": "Your organization needs a clear, up-to-date understanding of all IT equipment you own and operate, and exactly which business services each piece of hardware supports. This involves regular audits. Using systems like a Configuration Management Database (CMDB) and a Service Catalogue, as often found in ITIL frameworks or aligned with ISO 55000 asset management, is highly recommended to achieve this effectively.",
        "complianceRequires": "- Conduct thorough audits of the existing physical IT hardware estate to identify all deployed equipment.\n- Conduct audits of the existing logical IT service estate to map each service to the underlying physical equipment that delivers it.\n- Consider implementing, or effectively utilizing if already in place, an ITIL-type Configuration Management Database (CMDB) to store and manage this information.\n- Consider developing or using a Service Catalogue that clearly defines services and their dependencies on IT assets.\n- Refer to and align with ISO 55000 principles for asset management (as detailed in Practice 3.2.7) to ensure a structured approach.",
        "expertTip": "Automated discovery tools can significantly aid in populating and maintaining the accuracy of your CMDB and asset inventory. Ensure that the process for updating this information is integrated with your change management procedures so that any changes to the IT estate are promptly reflected.",
        "recommendedEvidence": "- A comprehensive and up-to-date IT asset inventory (physical and virtual servers, storage, network equipment).\n- A Service Catalogue or equivalent documentation linking business services to IT infrastructure components.\n- If a CMDB is used: documentation of its structure, scope, and examples of its use for service mapping.\n- Procedures for regular auditing and updating of the IT estate information.\n- Evidence showing this information is used for decision-making (e.g., impact analysis for changes, decommissioning planning).",
        "recommendedActions": "- Implement or enhance automated discovery tools to maintain an accurate inventory of all physical and virtual IT assets.\n- Establish or refine a CMDB and Service Catalogue, ensuring strong processes for maintaining data accuracy and completeness, including service mapping.\n- Integrate the CMDB/Service Catalogue with change management, incident management, and problem management processes.\n- Conduct regular (e.g., quarterly or biannual) audits to verify the accuracy of the documented IT estate against the physical reality.",
        "sampleResponseTemplate": "We maintain a comprehensive IT asset inventory and service mapping within our [CMDB Tool Name, e.g., ServiceNow CMDB], which is audited and updated [Frequency, e.g., quarterly] as per our '[Asset Management Policy ID]' which aligns with ISO 55000 principles (see Practice 3.2.7). Our Service Catalogue ([Catalogue Name/ID]) details all production services and their underlying infrastructure.",
        "humanInputAnswer": "Our IT estate is fully documented in our ServiceNow CMDB, which was populated through automated discovery and manual verification completed in Q4 2024. This CMDB maps all physical servers, virtual machines, storage arrays, and network devices to the business services they support, as defined in our 'Enterprise Service Catalogue v3.1'. The process for maintaining this is aligned with our ISO 55000 certified Asset Management framework. Regular audits are performed quarterly to ensure accuracy.",
        "llmGeneratedAnswer": "Yes, our organization conducts a biannual audit of its existing physical and logical IT estate. The findings are documented and maintained within our 'Enterprise Asset Register (EAR)', which functions as our Configuration Management Database (CMDB) and is aligned with ITIL v4 principles. This EAR (EAR_Policy_Doc_Rev2.1) establishes a clear mapping between physical hardware (servers, storage, network devices) and the specific IT services they deliver, as outlined in our 'Service Catalogue (SC_2025_Q1)'. This process is supported by our ISO 55000 compliant asset management framework (Ref: Practice 3.2.7).",
        "badExampleAnswer": "We have spreadsheets with lists of most of our servers. The server team generally knows what applications run on which machines."
      },
      "question_48": {
        "id": 48,
        "old_question_text": null,
        "questionText": "Has the organization established and consistently implemented a formal process to completely decommission (power off, disconnect, unrack) and physically remove any IT equipment that has been identified as no longer required or is found to be supporting services that are no longer in use? Does this process include the regular examination of all installed hardware to ensure its continued necessity and active service support, and specifically ensure the full decommissioning and removal of test and development platforms once they are no longer needed?",
        "simplifiedQuestion": "Do you have a formal process to find, fully switch off, unplug, and physically remove IT equipment that's no longer used or needed (including old test/dev systems)?",
        "reference": "2025 CoC Guide 4.3.2",
        "priority": "High (Value: 5, Expected: Entire Data Centre)",
        "stepSection": "step_4",
        "recommendationTitle": "Decommission and remove unused equipment",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Formal decommissioning policy and procedure. Asset retirement forms/records. CMDB or inventory reports showing equipment status (e.g., 'to be decommissioned', 'decommissioned'). Process for identifying and retiring unused test/development platforms."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory: A documented IT equipment decommissioning procedure. Records demonstrating regular identification, full decommissioning (power-off, unplug, unrack), and physical removal of unused/obsolete equipment, including test/dev systems.",
        "verification_method": [
          "Document Review",
          "Process Verification",
          "Site Inspection (for ghost servers)"
        ],
        "verificationMethods": "Review of decommissioning policies, procedures, and asset disposal records. Verification of processes for identifying 'ghost' servers. Physical inspection for powered-on but unused equipment or equipment pending removal.",
        "nonconformity": {
          "major": "Significant quantity of IT equipment is powered on but serving no useful purpose ('ghost servers'), and there's no effective process for their identification and removal.",
          "minor": "Decommissioning process exists but is inconsistently applied, leading to delays in removing unused equipment, or test/dev platforms are often left running after projects end."
        },
        "majorNonconformity": "Absence of a formal and consistently executed process for identifying, fully decommissioning (powering off, disconnecting, un-racking), and physically removing IT equipment that is no longer required or supports retired services, resulting in significant 'ghost server' presence.",
        "minorNonconformity": "A decommissioning process is documented but not rigorously followed, leading to significant delays in removing unused hardware, or test/development platforms are frequently not decommissioned promptly after project completion.",
        "whatThisMeans": "Your organization must have a regular, formal process to find any IT equipment (servers, storage, network devices) that is switched on but no longer doing any useful work or supporting active services. This includes old test and development systems. Once identified, this equipment should be properly shut down, unplugged, taken out of the racks, and physically removed from the data centre to save energy and space.",
        "complianceRequires": "- Implement a formal process to completely decommission and physically remove any IT equipment that is identified as not required or is supporting services no longer in use.\n- This process must include the regular examination of installed hardware to confirm it is still required and actively supporting services.\n- Ensure the process explicitly covers the decommissioning and physical removal of test and development platforms once they are no longer needed.",
        "expertTip": "Use your CMDB (Practice 4.3.1) in conjunction with utilization monitoring tools to proactively identify 'zombie servers' (servers that are powered on but serving no useful load or application). Establish a clear 'sunset' policy for applications and their associated infrastructure, including decommissioning of test/dev environments once a project moves to production or is cancelled.",
        "recommendedEvidence": "- Formal IT equipment decommissioning policy and standard operating procedure (SOP).\n- Change management records or work orders showing the decommissioning of specific assets.\n- Asset disposal certificates or records of equipment being recycled/resold.\n- Reports from CMDB/monitoring tools used to identify candidate equipment for decommissioning.\n- Procedures for retiring test and development environments after project completion.",
        "recommendedActions": "- Establish a clear, documented procedure for identifying, authorizing, and executing the decommissioning and physical removal of unused IT equipment.\n- Assign responsibility for regularly reviewing the IT asset inventory for decommissioning candidates.\n- Implement strict protocols for data sanitization before equipment disposal.\n- Integrate the decommissioning process with financial asset management for proper write-offs.\n- Track and report on the number of assets decommissioned and the associated energy/space savings.",
        "sampleResponseTemplate": "Our 'IT Asset Decommissioning Policy' ([Policy ID ITAD-001]) mandates a [e.g., quarterly] review of all IT assets against our Service Catalogue and CMDB to identify unused equipment. In [Last Quarter/Year], we decommissioned and removed [Number] servers and [Number] storage units that were no longer supporting active services, including [Number] test/dev platforms. Decommissioning records are maintained in [System Name].",
        "humanInputAnswer": "We have a 'Server Decommissioning Standard Operating Procedure (SOP-IT-DEC-004)' triggered by our quarterly application portfolio review. If a service is retired, or if utilization monitoring shows a server has been idle (below 5% CPU) for 90 days without justification, it's flagged. In Q4 2024, we identified and decommissioned 35 physical servers and 12 legacy test/dev environments, all physically removed from racks. Asset disposal certificates are archived.",
        "llmGeneratedAnswer": "Yes, our organization has a formal 'IT Equipment Decommissioning Protocol (ITEDP-2025)' which is initiated following our biannual 'Service and Asset Rationalization Review'. This review cross-references our CMDB with service utilization data to identify hardware supporting no active services or exhibiting prolonged idle states. For example, following the Q4 2024 review, 27 servers and 3 storage arrays were identified as redundant. These assets were fully decommissioned (power disconnected, data sanitized, physically un-racked and removed) by January 2025, as documented in 'Decommissioning Batch Report DBR-2025-01'. This protocol explicitly includes the shutdown and removal of test and development platforms once projects conclude or systems are moved to production.",
        "badExampleAnswer": "We turn off servers when we know they are not needed anymore. Some old ones are still in the racks but unplugged."
      },
      "question_49": {
        "id": 49,
        "old_question_text": null,
        "questionText": "For existing IT services that cannot be decommissioned due to compliance or other reasons but are not used regularly, has the organization virtualized these legacy services and then archived the disk images to low-power media, allowing them to be brought online only when genuinely required?",
        "simplifiedQuestion": "For old services you must keep but rarely use, do you virtualize and archive them to save power, only restoring when needed?",
        "reference": "2025 CoC Guide 4.3.3",
        "priority": "High (Value: 5, Expected: Entire Data Centre)",
        "stepSection": "step_4",
        "recommendationTitle": "Virtualise and archive legacy services",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Policy or procedure for identifying and managing infrequently used legacy services. Records of services that have been virtualized and archived. Documentation of the archive media and restoration process."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory (if applicable for existing legacy services): Policy and procedure for identifying, virtualizing, and archiving infrequently used legacy services to low-power media. Evidence of services treated this way, and documentation of the restoration process.",
        "verification_method": [
          "Document Review",
          "Process Verification"
        ],
        "verificationMethods": "Review of policies for managing legacy applications, inventory of archived services, and procedures for restoration. Interviews with IT application and infrastructure managers.",
        "nonconformity": {
          "major": "Infrequently used legacy services that must be retained are kept running on dedicated physical hardware instead of being virtualized and archived.",
          "minor": "Some legacy services are virtualized but not archived to low-power media, or the process for identifying and archiving such services is not systematic."
        },
        "majorNonconformity": "Failure to virtualize and archive (to low-power media) legacy services that cannot be decommissioned but are infrequently used, leading to them consuming power unnecessarily on active hardware.",
        "minorNonconformity": "A process for virtualizing legacy services exists, but archiving to low-power media and on-demand restoration is not consistently implemented, or identification of candidate services is ad-hoc.",
        "whatThisMeans": "If you have old IT services that you can't get rid of (perhaps for legal or compliance reasons) but are very rarely used, you should convert them into virtual machines. Once virtualized, the system's disk image can be saved onto a low-power storage device (archived). The original hardware can then be switched off. If the service is ever needed again, the virtual machine can be quickly restored and brought online.",
        "complianceRequires": "- Identify existing legacy services that cannot be decommissioned (e.g., for compliance reasons) but are not used on a regular basis.\n- Virtualize these identified legacy services (P2V - Physical to Virtual conversion).\n- Archive the resulting disk images of these virtualized services to a low-power storage medium (e.g., tape, low-spin disk arrays, cloud archive storage).\n- Ensure these archived services can be brought online (restored from archive) when genuinely required.\n- Decommission the original physical hardware once the service is successfully virtualized and archived.",
        "expertTip": "Establish clear criteria for identifying 'infrequently used' legacy services (e.g., no access logs for X months, business owner confirmation of low usage). Develop a standardized P2V and archiving workflow, including testing the restoration process to ensure services can be brought back online within acceptable timeframes when needed.",
        "recommendedEvidence": "- Policy or procedure for identifying, virtualizing, and archiving infrequently used legacy services.\n- A list or inventory of legacy services that have been virtualized and archived.\n- Description or specifications of the low-power archive media used.\n- Documented procedure for restoring an archived virtualized service on demand.\n- Evidence of original physical hardware being decommissioned after successful P2V and archiving.",
        "recommendedActions": "- Conduct a review of all existing IT services to identify candidates for virtualization and archiving based on usage frequency and retention requirements.\n- Develop a standardized process for P2V conversion, disk image archiving, and on-demand restoration.\n- Select appropriate low-power, long-term archive storage media.\n- Regularly test the restoration process for a sample of archived services.\n- Ensure that compliance and data retention requirements are met for archived services.",
        "sampleResponseTemplate": "Our 'Legacy Application Management Policy' ([Policy ID LAM-001]) includes a process for virtualizing and archiving infrequently used but retained services. For example, our '[Legacy System Name]' was virtualized in [Year], and its VM image is archived on our [Archive Storage System Type]. It can be restored within [X hours] if required. The original hardware was decommissioned on [Date]. We have [Y] such services currently archived.",
        "humanInputAnswer": "We maintain several legacy financial reporting applications for regulatory compliance that are accessed less than five times a year. As per our 'Application Archival SOP (SOP-IT-ARCH-002)', these were virtualized using VMware P2V tools and their VMDKs are now stored on our nearline SAS archive tier, which spins down when not accessed. The original physical servers were decommissioned in 2023. We test restoration annually for each archived application; last test was successful in March 2025.",
        "llmGeneratedAnswer": "Yes, our organization implements a 'Legacy Service Archival' procedure for IT services that have mandatory retention requirements but infrequent operational use. This procedure, detailed in 'IT Service Lifecycle Policy (ITSLP-2025, Section 7)', involves virtualizing the legacy service (P2V conversion), followed by archiving the resultant disk images onto our dedicated low-power archival storage platform. For example, our 'Historical Sales Database (ArchiveVer_2010)' was virtualized and its images archived in Q2 2024. The service can be restored and brought online within 48 hours upon formal request. This has allowed us to decommission the 5 physical servers previously dedicated to this service.",
        "badExampleAnswer": "We keep all our old applications running just in case someone needs them. It's easier than trying to bring them back from a backup."
      },
      "question_50": {
        "id": 50,
        "old_question_text": null,
        "questionText": "Has the organization implemented a strategy for the consolidation of existing IT services where individual services do not achieve high utilization of their dedicated hardware? Does this strategy involve the use of resource-sharing technologies (e.g., virtualization, containerization, shared databases/storage) to improve the utilization of physical resources across servers, storage, and networking devices?",
        "simplifiedQuestion": "Do you consolidate existing IT services that underutilize their hardware onto shared platforms to improve resource use?",
        "reference": "2025 CoC Guide 4.3.4",
        "priority": "High (Value: 5, Expected: Entire Data Centre)",
        "stepSection": "step_4",
        "recommendationTitle": "Consolidation of existing services",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "IT consolidation strategy or policy. Reports from utilization monitoring identifying candidates for consolidation. Project plans and outcomes for service consolidation initiatives. Architecture diagrams showing use of shared resource platforms."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory: Documented strategy and evidence of consolidating underutilized existing services onto shared resource platforms (virtualization, containers, etc.) covering servers, storage, and networking.",
        "verification_method": [
          "Document Review",
          "Data Analysis",
          "Interview"
        ],
        "verificationMethods": "Review of IT consolidation strategy, utilization reports, and project documentation for consolidation efforts. Interviews with IT architects and infrastructure managers.",
        "nonconformity": {
          "major": "No systematic effort to consolidate existing services that underutilize their dedicated hardware, leading to widespread low physical resource utilization.",
          "minor": "Consolidation efforts are ad-hoc, focus only on servers (not storage/network), or resource-sharing technologies are not effectively leveraged to improve overall utilization."
        },
        "majorNonconformity": "Absence of a strategy or ongoing effort to consolidate existing IT services that exhibit low hardware utilization, resulting in continued inefficient use of physical server, storage, and network resources.",
        "minorNonconformity": "Consolidation initiatives are sporadic or incomplete, or they focus narrowly (e.g., only server virtualization) without addressing storage and network consolidation opportunities or fully leveraging resource-sharing technologies.",
        "whatThisMeans": "If you have existing IT services running on their own dedicated hardware (servers, storage, network gear) but not using that hardware very much, your organization should have a plan to move these services onto shared platforms. This often means using technologies like virtualization or containers so that multiple services can share the same physical resources, making much better use of your hardware and saving energy.",
        "complianceRequires": "- Identify existing IT services that do not achieve high utilization of their dedicated hardware.\n- Implement a strategy to consolidate these services through the use of resource-sharing technologies.\n- This consolidation should aim to improve the utilization of physical resources across servers, storage, and networking devices.\n- Examples of resource-sharing technologies include virtualization (server, storage, network), containerization, and use of shared database or storage platforms.",
        "expertTip": "Use continuous utilization monitoring to identify prime candidates for consolidation. Focus on services running on older, less efficient hardware first, as consolidating them onto newer, shared platforms often yields the largest energy savings and performance improvements. Develop standardized migration paths for common application types to streamline the consolidation process.",
        "recommendedEvidence": "- IT strategy document outlining consolidation goals and approaches.\n- Reports from monitoring tools showing utilization levels of physical servers, storage, and network devices, highlighting underutilized assets.\n- Project documentation for consolidation initiatives (e.g., P2V migrations, application re-platforming to containers or shared databases).\n- Before-and-after metrics showing improvements in physical resource utilization and reductions in hardware footprint/energy consumption due to consolidation.",
        "recommendedActions": "- Implement comprehensive IT asset utilization monitoring across servers, storage, and networking.\n- Establish clear thresholds for identifying underutilized hardware supporting existing services.\n- Develop a recurring process (e.g., quarterly) to review utilization data and plan consolidation projects.\n- Invest in and leverage resource-sharing technologies like virtualization, container platforms, and shared storage/database services.\n- Track the energy and cost savings achieved through consolidation efforts.",
        "sampleResponseTemplate": "Our 'IT Resource Optimization Program' ([Program ID: ITROP-001]) includes a continuous consolidation strategy for existing services. We use [Monitoring Tool] to identify servers/storage/network devices with utilization below [X%]. In [Last Quarter/Year], we consolidated [Number] services from dedicated hardware to our [VMware/Kubernetes/Shared Storage Platform], resulting in the decommissioning of [Number] physical assets and an estimated [Y kWh] annual energy saving. This is documented in '[Consolidation Report QZ/YYYY]'.",
        "humanInputAnswer": "Our ongoing 'Server Fleet Optimization' initiative targets physical servers with sustained CPU utilization below 30%. In 2024, we identified 75 such servers supporting various departmental applications. Through a P2V (Physical-to-Virtual) project, 68 of these services were successfully migrated to our existing VMware vSphere clusters. This allowed us to decommission 60 physical servers, improving overall host utilization by 15% and saving an estimated 90,000 kWh annually. The process is documented in 'IT Consolidation SOP - SERVCON-002'.",
        "llmGeneratedAnswer": "Yes, the organization has an active 'Service Consolidation Program' (SCP-2025) aimed at improving hardware utilization. As per 'SCP Framework Document (SCP-DOC-001)', existing services running on dedicated hardware with an average utilization below 40% over a 90-day period are flagged for review. The primary strategy involves migrating these services to our centralized VMware vSphere virtualization platform, leveraging shared storage and network resources. In Q1 2025, 15 legacy departmental applications were consolidated, resulting in the retirement of 12 physical servers and an estimated 20% increase in the utilization of the target host cluster. This program covers server, storage (via SAN consolidation), and network (via port reclamation and virtual networking) resources.",
        "badExampleAnswer": "Most of our applications have their own servers. We don't usually move them unless the server gets too old."
      }
    }
  },
  "step_5": {
    "description": "2025 Guide Section 4: IT Equipment and Services (Practices 4.3.5-4.3.8 from Sec 4.3; and 4.4.1-4.4.6 from Sec 4.4)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_IT_Equip_SVC_4"
    ],
    "critical_questions": [],
    "questions": {
      "question_51": {
        "id": 51,
        "old_question_text": null,
        "questionText": "Has the organization implemented a process to identify existing IT services whose business value or criticality is low and that do not justify their current financial or environmental (energy consumption) overhead? Does this process lead to considering the decommissioning or archiving of these services, or their removal to locations/platforms with less reliability/resilience to reduce their energy and financial overhead?",
        "simplifiedQuestion": "Do you identify low-value/low-criticality IT services and then decommission, archive, or move them to less resource-intensive platforms to save energy and cost?",
        "reference": "2025 CoC Guide 4.3.5",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_5",
        "recommendationTitle": "Decommission low business value services",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Application portfolio management policy/process. Criteria for assessing business value and criticality of services. Records of services identified as low value/criticality. Decisions and actions taken (decommissioning, archiving, migration to lower-tier platforms)."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory: Documented process for evaluating the business value/criticality of existing IT services and their associated financial/environmental overhead. Evidence of actions taken for low-value services (decommissioning, archiving, migration).",
        "verification_method": [
          "Document Review",
          "Process Verification",
          "Interview"
        ],
        "verificationMethods": "Review of application portfolio management processes, service criticality assessment criteria, and records of decisions made regarding low-value services. Interviews with business stakeholders and IT service portfolio managers.",
        "nonconformity": {
          "major": "No process exists to evaluate the business value versus the financial/environmental overhead of existing IT services, leading to continued operation of unjustified, resource-intensive services.",
          "minor": "Services are reviewed for business value, but the link to their operational overhead (including energy) is not systematically considered, or actions to decommission/archive/migrate are inconsistent."
        },
        "majorNonconformity": "Absence of a formal process to identify and address low business value or low criticality IT services that incur significant financial or environmental (energy) overhead, leading to their continued operation without justification.",
        "minorNonconformity": "A process to review service value exists but doesn't adequately consider financial/environmental overheads, or decisions to decommission, archive, or migrate low-value services are not consistently implemented.",
        "whatThisMeans": "Your organization should regularly review all its existing IT services to see if their importance to the business still justifies the money and energy they consume. If a service isn't very valuable or critical anymore, you should consider shutting it down completely (decommissioning), archiving it (if the data needs to be kept but the service doesn't need to be live), or moving it to a less resource-intensive platform (e.g., one with lower power/cooling protection if it doesn't need high availability). This saves energy and reduces costs.",
        "complianceRequires": "- Implement a process to identify existing IT services whose business value or criticality is low.\n- Assess if these low-value/criticality services justify their current financial and environmental (energy consumption) overhead.\n- For services deemed unjustified, consider appropriate actions:\n    - Decommissioning the service.\n    - Archiving the service (if data retention is required but live operation is not).\n    - Removing the service to locations or platforms with less reliability/resilience to reduce energy and financial overhead (e.g., from a Tier III to a Tier I environment, or from physical to a less resource-intensive virtual platform if not already virtualized).",
        "expertTip": "Incorporate a 'Total Cost of Ownership (TCO)' and 'Total Value of Ownership (TVO)' assessment into your regular application portfolio review. This should explicitly include the ongoing energy and infrastructure costs. Establish clear criteria and a governance process for making decisions on services that no longer provide sufficient value relative to their cost and environmental impact.",
        "recommendedEvidence": "- Application portfolio management policy or procedure that includes assessment of business value vs. operational overhead.\n- Criteria used to define 'low business value' or 'low criticality'.\n- Records of service reviews identifying candidates for decommissioning, archiving, or migration.\n- Change management records or project documentation for services that have been decommissioned, archived, or migrated to lower-resilience platforms due to low business value.\n- Evidence of energy/cost savings achieved from these actions.",
        "recommendedActions": "- Establish a regular (e.g., annual or biannual) application portfolio review process that assesses the business value, criticality, and operational overhead (including energy) of all existing IT services.\n- Define clear criteria for identifying services that are candidates for decommissioning, archiving, or migration.\n- Develop standardized processes for each of these actions (decommissioning, archiving, migration).\n- Ensure business stakeholders are involved in the decision-making process for these services.\n- Track and report on the resources (hardware, software licenses, energy) saved by these actions.",
        "sampleResponseTemplate": "Our 'Application Portfolio Management Framework' ([Doc ID: APM-001]) includes an annual review of all services for business value versus TCO, including energy overhead. In [Year], [Number] services were identified as low value. [Example: Service Z was decommissioned, Service Y was archived, Service X was migrated from our high-resilience platform to a standard virtual environment]. This resulted in [quantifiable benefit, e.g., 'the retirement of 5 servers and estimated X kWh savings'].",
        "humanInputAnswer": "Our annual 'Application Rationalization Review' (documented in APM-Policy-005) assesses each service's business value against its operational cost, including estimated energy footprint. In 2024, this review identified 'OldArchiveSearchTool' as low value with high legacy hardware overhead. The decision was made to decommission it, and its essential data was migrated to a shared, low-power archival platform. This freed up 3 physical servers and reduced annual energy consumption by an estimated 12,000 kWh.",
        "llmGeneratedAnswer": "Yes, our organization conducts an annual 'Service Value Assessment (SVA)' as part of our IT Portfolio Management process (Ref: 'ITPM-SVA Protocol 2025'). This SVA identifies services with low current business value or criticality relative to their operational costs, including an estimated energy consumption overhead. For services flagged by the SVA, a formal review determines whether to decommission, archive (if data retention is required, see Practice 4.3.3), or migrate them to a lower-cost, lower-resilience, and more energy-efficient platform (e.g., from on-premise high-availability infrastructure to a basic cloud storage tier). In 2024, three such services were identified, resulting in two decommissionings and one migration to a lower-tier archive, reducing our active server count by five.",
        "badExampleAnswer": "We keep all our applications running because someone might need them. It's hard to turn things off."
      },
      "question_52": {
        "id": 52,
        "old_question_text": null,
        "questionText": "Has the organization implemented a process to identify servers, networking, and storage equipment that is idle for significant periods of time and cannot be virtualized or archived (as per Practice 4.3.3)? Does this process lead to such equipment being shut down or put into a low-power 'sleep' state, with complete physical removal also considered? Is the ability of legacy applications and hardware to survive these state changes without loss of function or reliability validated before implementing shutdown/sleep states?",
        "simplifiedQuestion": "Do you identify idle IT gear (that can't be virtualized/archived) and then shut it down, put it to sleep, or remove it, after checking it's safe to do so?",
        "reference": "2025 CoC Guide 4.3.6",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_5",
        "recommendationTitle": "Shut down and consider removal of idle equipment",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Policy/procedure for identifying and managing idle IT equipment. Records of equipment shut down, put into sleep states, or removed. Validation testing procedures for legacy systems before applying power state changes."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory: Documented process for identifying and managing idle IT equipment (servers, network, storage) that cannot be virtualized/archived. Evidence of actions taken (shutdown, sleep mode, removal). Validation process for legacy system compatibility with power state changes.",
        "verification_method": [
          "Document Review",
          "Data Analysis",
          "Interview"
        ],
        "verificationMethods": "Review of idle equipment management policies, utilization monitoring data, and records of actions taken (shutdown, removal). Interviews with IT operations staff regarding identification and handling of idle assets.",
        "nonconformity": {
          "major": "Significant amounts of IT equipment are identified as idle for long periods but remain powered on without being put into low-power states or removed, and without validation of their ability to handle power state changes.",
          "minor": "Process for managing idle equipment is informal or inconsistently applied; validation of legacy system compatibility with power state changes is not always performed."
        },
        "majorNonconformity": "Systematic failure to shut down, put into a low-power state, or physically remove IT equipment (servers, networking, storage) that is idle for significant periods and cannot be virtualized or archived. No validation for legacy systems' ability to handle power state changes.",
        "minorNonconformity": "Identification of idle equipment is ad-hoc, or actions (shutdown, sleep mode, removal) are inconsistently applied. Validation of legacy system compatibility with power state changes is not consistently performed or documented prior to action.",
        "whatThisMeans": "Your organization needs a way to find IT equipment (servers, network gear, storage) that is switched on but doing nothing for long stretches of time. If this idle equipment can't be dealt with by virtualizing or archiving its services (as in Practice 4.3.3), you should shut it down or put it into a very low-power 'sleep' mode. You should also consider physically removing it. Before you do this to older systems, you must check carefully that shutting them down or putting them to sleep won't cause problems when they are turned back on.",
        "complianceRequires": "- Identify servers, networking, and storage equipment that is idle for significant periods of time.\n- This applies to equipment that cannot be virtualized or its services archived (as per Practice 4.3.3).\n- For such identified idle equipment, implement a process to:\n    - Shut it down completely, or\n    - Put it into a low-power 'sleep' state.\n- Consider complete physical removal of such persistently idle equipment.\n- Crucially, validate the ability of any legacy applications and their hardware to survive these power state changes (shutdown/sleep and restart) without loss of function or reliability *before* implementing such measures.",
        "expertTip": "Use utilization monitoring tools to identify equipment with prolonged low activity (e.g., CPU <5%, network I/O near zero for extended periods). Before implementing shutdown/sleep for legacy systems, perform controlled tests in a non-production environment or during a maintenance window to confirm they can be reliably power-cycled.",
        "recommendedEvidence": "- Policy or procedure for identifying and managing idle IT equipment.\n- Reports from monitoring systems showing idle equipment candidates.\n- Change records or work orders for shutting down, putting to sleep, or removing idle equipment.\n- Documentation of validation testing performed on legacy systems to ensure they can tolerate power cycling or sleep states.\n- Inventory lists showing equipment that has been put into a managed low-power state or removed.",
        "recommendedActions": "- Implement monitoring to detect persistently idle IT equipment across servers, storage, and networking.\n- Develop a formal process for evaluating idle equipment: first for virtualization/archiving (4.3.3), then for shutdown/sleep/removal.\n- Create and document a validation checklist for testing legacy systems' ability to handle power state changes before applying them.\n- Establish clear guidelines for when to move from 'sleep state' to 'complete shutdown and removal' for persistently idle assets.",
        "sampleResponseTemplate": "Our 'Idle Asset Management Procedure' ([Doc ID: IT-IDLE-001]) uses [Monitoring Tool] to identify equipment idle for over [X days]. If not suitable for P2V/Archival (per 4.3.3), legacy systems undergo a 'Power Cycle Validation Test' ([Test Plan ID]). If successful, they are scheduled for [shutdown/low-power mode/removal]. Last quarter, [Number] idle [switches/servers] were [action taken].",
        "humanInputAnswer": "Our IT Operations team runs a monthly 'Idle Asset Scan' using SolarWinds, identifying servers and network ports with no significant activity for >45 days. For equipment not covered by Practice 4.3.3 (virtualize/archive), if it's modern hardware, it's powered down. For legacy systems, like our old 'DataLogger_AppServer', we first performed a documented power-cycle validation (Ref: 'PCV_DataLogger_Test_2024'). It passed, so it's now scheduled for nightly shutdown and weekend power-off via remote power management scripts. We aim for removal within 6 months if still unused.",
        "llmGeneratedAnswer": "Yes, our organization has a 'Proactive Idle Equipment Management (PIEM)' process, detailed in 'IT Operations Manual (ITOM-2025, Section 8.2)'. This process leverages utilization data from our monitoring systems to identify servers, network devices, and storage arrays that exhibit less than 5% activity for a continuous 30-day period and are not candidates for virtualization or archiving. For such equipment, a technical review is conducted. If the equipment supports reliable low-power 'sleep' states, these are implemented. If not, or if the equipment remains persistently idle for over 90 days, it is scheduled for complete shutdown and eventual physical removal. Crucially, for any legacy systems identified, a 'Power State Change Impact Assessment' (documented in 'PSIA-Template-001') is performed and approved before implementing shutdown/sleep modes to ensure system stability upon restart. For instance, 12 such idle legacy data acquisition servers were transitioned to a nightly shutdown schedule in Q1 2025 after successful PSIA.",
        "badExampleAnswer": "If a server isn't being used, we usually just leave it on because it's too much trouble to turn it off and then back on if someone suddenly needs it."
      },
      "question_53": {
        "id": 53,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented resource management systems capable of analysing and optimising where, when, and how IT workloads are executed to manage their consequent energy use? Do these systems include technologies that allow remote deployment, delayed execution of non-critical jobs, or the dynamic movement of jobs/VMs within the infrastructure to enable the shutdown of components, entire systems, or sub-systems, with the desired outcome of limiting localized heat output or constraining system power draw to a fixed limit at a data centre, row, or cabinet level?",
        "simplifiedQuestion": "Do you use systems to control IT workload placement and timing to optimize energy use (e.g., by shutting down idle parts of the DC)?",
        "reference": "2025 CoC Guide 4.3.7",
        "priority": "Medium (Value: 4, Expected: Optional Practice)",
        "stepSection": "step_5",
        "recommendationTitle": "Control of system energy use",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Documentation of any resource management or workload orchestration systems used. Policies or procedures for workload scheduling, placement, or migration to optimize energy. Case studies or reports showing outcomes of such optimizations."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: Documentation of workload/resource management systems and policies used to optimize IT energy consumption, including features for job scheduling, workload migration, power capping, or dynamic resource allocation for energy efficiency.",
        "verification_method": [
          "Document Review",
          "System Demonstration",
          "Interview"
        ],
        "verificationMethods": "Review of documentation for workload management systems and energy optimization policies. Demonstration of system capabilities. Interviews with IT operations and automation teams.",
        "nonconformity": {
          "major": null,
          "minor": "No consideration or implementation of resource/workload management systems for energy optimization, despite having opportunities to do so (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Resource management systems are in place but not utilized for energy optimization, or features for workload scheduling/migration for energy savings are not actively used or are ineffective.",
        "whatThisMeans": "Your organization should explore using smart systems that can control where and when your IT jobs run to save energy. These systems might delay less important jobs, move tasks around to consolidate work onto fewer machines (allowing others to be powered down), or even limit the power used by certain equipment or areas. The goal is to better manage heat output and overall power consumption.",
        "complianceRequires": "- Consider implementing resource management systems that can analyze and optimize IT workload execution for energy efficiency.\n- Evaluate technologies that support:\n    - Remote deployment of workloads.\n    - Delayed execution of non-critical jobs (e.g., to off-peak hours).\n    - Dynamic movement of jobs or virtual machines within the infrastructure.\n- The aim of these technologies should be to enable shutdown of underutilized components, entire systems, or sub-systems.\n- The desired outcome is the ability to limit localized heat output or constrain system power draw to a fixed limit (at data centre, row, or cabinet levels).",
        "expertTip": "Leverage workload orchestration and cloud management platforms that have built-in energy-aware scheduling or power management integrations. These can automate the consolidation of virtual machines onto fewer physical hosts during periods of low demand, enabling physical hosts to be powered down or put into deep sleep states.",
        "recommendedEvidence": "- Documentation of any resource management or workload orchestration tools with energy optimization features.\n- Policies or procedures for workload scheduling, placement, or migration aimed at reducing energy consumption.\n- Examples of how workloads are managed to enable power-down of IT components or systems.\n- Reports showing energy savings or improved resource utilization due to these systems/practices.\n- Descriptions of any power capping or load balancing strategies implemented at the IT system level.",
        "recommendedActions": "- Evaluate available workload management and resource optimization tools that offer energy efficiency capabilities.\n- Identify IT workloads that are suitable for dynamic placement, delayed execution, or consolidation.\n- Develop policies and automated processes for optimizing workload execution to save energy.\n- Implement monitoring to track the effectiveness of these measures in reducing IT energy consumption and enabling infrastructure power savings.",
        "sampleResponseTemplate": "We utilize [Workload Management System, e.g., VMware DRS with DPM, Kubernetes with custom schedulers] to optimize workload placement and energy use. This system allows us to [describe capability, e.g., 'consolidate VMs onto fewer hosts during off-peak hours, powering down unused hosts']. Our policy ([Policy ID]) dictates that non-critical batch jobs are scheduled for [Time period]. This has resulted in [quantifiable benefit, e.g., 'an average X% reduction in active server power during nights'].",
        "humanInputAnswer": "Our VMware vSphere environment uses Distributed Resource Scheduler (DRS) for workload balancing and Distributed Power Management (DPM) to consolidate VMs and power down underutilized ESXi hosts during off-peak hours (10 PM - 6 AM). This is documented in our 'Virtual Infrastructure Management SOP (SOP-VIM-002)'. We estimate this saves approximately 3-5% of our total server energy consumption annually. We also use power capping features on our HPC cluster during periods of high electricity tariffs.",
        "llmGeneratedAnswer": "Yes, our organization employs a resource management system, specifically [Turbonomic/VMware vRealize Operations/Other Platform Name], which includes capabilities for analyzing and optimising IT workload execution with energy efficiency as a consideration. As per our 'Dynamic Workload Management Policy (DWMP-2025)', the system is configured to: \n1. Consolidate virtual machines onto a minimum number of physical hosts during periods of low demand, enabling other hosts to enter a low-power state or be temporarily powered off. \n2. Identify and recommend placement for new workloads based on available capacity and energy efficiency of target hardware. \n3. Provide dashboards that correlate workload demand with power consumption at the cluster and host level. \nThis allows us to dynamically control system energy use and limit heat output in specific zones, aiming for a reduction in idle power draw. We are currently piloting policy-based power capping for non-critical development environments.",
        "badExampleAnswer": "Workloads run where they are deployed. We don't really move them around much unless there's a hardware failure."
      },
      "question_54": {
        "id": 54,
        "old_question_text": null,
        "questionText": "Has the organization conducted an audit to identify the allowable intake temperature and humidity ranges for all existing installed IT equipment? Is equipment with more restrictive intake temperature ranges identified so that it may either be marked for replacement with newer equipment capable of a wider intake range, or moved and managed according to practices for equipment segregation (Practice 5.1.11) and separate environmental zones (Practices 5.3.4 and 5.3.5)?",
        "simplifiedQuestion": "Have you audited your existing IT gear to know its allowed temperature/humidity limits, and do you manage or replace items with restrictive needs?",
        "reference": "2025 CoC Guide 4.3.8",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_GuideSection4_ITEquipmentServices_Part3",
        "recommendationTitle": "Audit of existing IT equipment environmental requirements",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "xlsx",
            "docx"
          ],
          "description": "Inventory of existing IT equipment with documented allowable environmental ranges. Process for auditing these requirements. Plans or records for replacing or segregating equipment with restrictive ranges."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, xlsx, docx",
        "documentDescriptor": "Mandatory: Documented audit of existing IT equipment identifying their allowable intake temperature/humidity ranges. Records showing how equipment with restrictive ranges is managed (marked for replacement, moved to segregated zones per 5.1.11, 5.3.4, 5.3.5).",
        "verification_method": [
          "Document Review",
          "Data Analysis",
          "Site Inspection"
        ],
        "verificationMethods": "Review of IT asset inventory that includes environmental specifications. Examination of plans for equipment with restrictive ranges. Physical inspection if segregated zones exist.",
        "nonconformity": {
          "major": "No audit of existing IT equipment environmental requirements has been conducted, and equipment with restrictive ranges dictates overall data centre environmental settings, hindering efficiency.",
          "minor": "Audit of environmental requirements is incomplete or outdated, or plans for managing restrictive equipment are not fully implemented or effective."
        },
        "majorNonconformity": "Failure to audit and document the allowable intake temperature and humidity ranges for existing IT equipment, or allowing equipment with restrictive ranges to dictate the environmental settings for broader areas without segregation.",
        "minorNonconformity": "An audit of IT equipment environmental requirements has been performed, but it is incomplete, or the process for managing identified restrictive equipment (replacement, segregation) is not consistently followed or effective.",
        "whatThisMeans": "Your organization needs to check and record the acceptable operating temperature and humidity ranges for all the IT equipment currently installed in your data centre. If you find equipment that needs much tighter (usually cooler or more specific humidity) conditions than most of your other gear, you need a plan for it. This plan could be to replace it with newer, more tolerant equipment, or to move it to a special area with its own dedicated environmental controls, so it doesn't force your whole data centre to run less efficiently.",
        "complianceRequires": "- Conduct an audit to identify and document the allowable intake temperature and humidity ranges for all existing installed IT equipment.\n- Specifically identify equipment that has more restrictive intake temperature (and/or humidity) ranges compared to modern standards (e.g., ASHRAE A2 or wider).\n- For such identified equipment, implement a management strategy:\n    - Mark it for replacement with newer equipment capable of operating within a wider intake range during the next refresh cycle.\n    - Or, move and manage this restrictive equipment according to the principles of equipment segregation (Practice 5.1.11) and by creating separate environmental zones with dedicated controls (Practices 5.3.4 and 5.3.5).",
        "expertTip": "Integrate the collection of IT equipment environmental specifications (allowable temperature/humidity ranges) into your asset management system (CMDB). This allows for easy identification of restrictive equipment and helps in planning data centre environmental setpoints or identifying candidates for segregation or priority replacement.",
        "recommendedEvidence": "- An IT asset inventory that includes the manufacturer-specified allowable intake temperature and humidity ranges for existing equipment.\n- Report or documentation from an audit specifically focused on identifying equipment with restrictive environmental requirements.\n- Plans or records showing actions taken for equipment with restrictive ranges (e.g., scheduled for replacement, moved to a segregated zone, risk assessment for continued operation).\n- If segregated zones exist, documentation of their environmental setpoints and the equipment housed within.",
        "recommendedActions": "- Systematically gather and document the allowable environmental operating ranges for all existing IT assets.\n- Identify any equipment whose restrictive ranges are limiting the ability to operate the data centre at more efficient, wider environmental setpoints.\n- Develop a plan to either replace this restrictive equipment as part of a refresh cycle or to physically segregate it into an area with tailored environmental controls.\n- Ensure that any newly created segregated zones are themselves operated efficiently for the specific needs of the equipment they house.",
        "sampleResponseTemplate": "We conducted an audit of our existing IT equipment's environmental requirements in [Month, Year] (Ref: '[Audit Report ID]'). This audit identified [Number] legacy systems, such as [Example System], with restrictive intake temperature ranges below ASHRAE A2. These systems have been [e.g., 'scheduled for replacement in the YYYY refresh cycle' / 'relocated to our specialized equipment zone (Pod Z) which is maintained at X\u00b0C']. Our asset database ([System Name]) now tracks these environmental limits.",
        "humanInputAnswer": "Our CMDB includes a field for 'Max Allowable Inlet Temp' for all servers. A Q4 2024 audit (Ref: 'IT_Env_Audit_2024_Q4.pdf') identified 12 legacy servers requiring inlet temperatures below 25\u00b0C. Of these, 8 were part of a system scheduled for decommissioning in Q1 2025. The remaining 4 have been moved to a dedicated 'Cool Pod' which is independently controlled at 22\u00b0C, allowing the rest of the data hall to operate at 26\u00b0C. This aligns with our segregation practice 5.1.11.",
        "llmGeneratedAnswer": "Yes, an audit of existing IT equipment environmental requirements was completed in Q1 2025, documented in 'IT Asset Environmental Specification Audit Report (ITAESAR-2025)'. This audit identified all installed IT equipment and their manufacturer-specified allowable intake temperature and humidity ranges. Equipment found to have restrictive intake ranges (i.e., narrower than ASHRAE A2 recommended) has been flagged in our asset management system. For example, three legacy 'SystemX' servers requiring inlet temperatures not exceeding 24\u00b0C were identified. These units have been scheduled for priority replacement in the H2 2025 refresh cycle. In the interim, they have been relocated to Rack A1-A3, which is adjacent to a dedicated CRAC unit allowing for more targeted cooling, as a temporary segregation measure (Practice 5.1.11).",
        "badExampleAnswer": "All our equipment seems to run fine at the current room temperature. We replace things when they get too old or break."
      },
      "questions": {
        "question_55": {
          "id": 55,
          "old_question_text": null,
          "questionText": "Has the organization developed, communicated, and enforced a formal data management policy that defines which data should be kept, for how long (retention periods), and at what level of protection (e.g., resilience, backup frequency)? Does this policy require particular care in understanding the energy consumption impact of any specified data retention requirements?",
          "simplifiedQuestion": "Do you have a formal data management policy covering what data to keep, for how long, its protection level, and the energy impact of these choices?",
          "reference": "2025 CoC Guide 4.4.1",
          "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
          "stepSection": "step_GuideSection4_ITEquipmentServices_Part4",
          "recommendationTitle": "Data management policy",
          "document_requirements": {
            "required": true,
            "accepted_types": [
              "pdf",
              "docx"
            ],
            "description": "Formal data management policy document, data retention schedules, data protection guidelines, and any analysis or considerations of the energy impact of data retention policies."
          },
          "docRequired": "Yes",
          "acceptedFiles": "pdf, docx",
          "documentDescriptor": "Mandatory: Documented data management policy defining data retention, protection levels, and consideration of the energy impact of retention requirements. Evidence of communication and enforcement.",
          "verification_method": [
            "Document Review",
            "Interview"
          ],
          "verificationMethods": "Review of the data management policy, data retention schedules, and data protection standards. Interviews with data owners, IT governance, and records management personnel.",
          "nonconformity": {
            "major": "No formal data management policy exists, or the existing policy does not address data retention, protection levels, or the energy impact of these decisions.",
            "minor": "Data management policy is incomplete, not consistently communicated or enforced, or the energy impact of data retention is not adequately considered."
          },
          "majorNonconformity": "Absence of a developed, communicated, and enforced data management policy that defines data retention periods, protection levels, and the energy consumption impact of these requirements.",
          "minorNonconformity": "A data management policy exists but is not comprehensive (e.g., lacks energy impact considerations for retention), is poorly communicated, or its enforcement is inconsistent across the organization.",
          "whatThisMeans": "Your organization needs a formal, written policy about how data is managed. This policy should clearly state what data needs to be stored, for how long it must be kept, and what level of protection (like backups or resilient storage) it needs. Importantly, this policy should also consider how these decisions, especially long data retention periods on active storage, affect energy consumption.",
          "complianceRequires": "- Develop a formal data management policy.\n- Define within the policy which data should be kept.\n- Specify data retention periods (how long data is kept).\n- Define the level of data protection required for different data types.\n- Communicate this policy to users and ensure it is enforced.\n- Take particular care to understand and document the energy consumption impact of any data retention requirements (e.g., keeping large volumes of data on powered-on, high-performance storage for extended periods).",
          "expertTip": "Integrate your data management policy with a data classification framework. Classify data based on its business value, sensitivity, and regulatory retention requirements. This allows you to apply appropriate (and potentially more energy-efficient) storage tiers and retention rules based on actual data needs, rather than a one-size-fits-all approach.",
          "recommendedEvidence": "- The formal data management policy document.\n- Data retention schedules for different types of data.\n- Data classification guidelines (if used to inform retention and protection).\n- Communication records showing the policy has been disseminated to users.\n- Examples of how the energy impact of data retention has been considered (e.g., in TCO analysis for storage solutions, or in defining storage tiers for long-term data).",
          "recommendedActions": "- Establish a cross-functional team (IT, legal, business units) to develop or review the data management policy.\n- Define clear data retention schedules based on business, legal, and regulatory requirements, while also considering energy impacts.\n- Implement mechanisms to enforce the policy (e.g., automated data lifecycle management, regular audits).\n- Educate users on the policy and their responsibilities for data management.",
          "sampleResponseTemplate": "Our 'Enterprise Data Management Policy' ([Doc ID: EDM-POL-001]), last updated [Date], defines data classification, retention schedules (e.g., financial data: 7 years, project data: project life + 2 years), and protection levels. Section 4.5 specifically addresses minimizing the energy footprint of long-term data storage by mandating tiering to lower-power archival systems for data beyond its active lifecycle. This policy is communicated via our intranet and annual compliance training.",
          "humanInputAnswer": "Our corporate 'Data Governance and Management Policy (DGMP-2024)' defines data retention schedules and protection tiers based on data classification (Public, Internal, Confidential, Restricted). For example, 'Restricted' financial records must be kept for 10 years on resilient, encrypted storage. The policy's Annex C, 'Energy Considerations for Data Storage,' outlines that data older than 2 years, unless actively used, should be migrated to our lower-cost, lower-power archival storage tier to minimize energy overhead. This is enforced via quarterly data audits.",
          "llmGeneratedAnswer": "Yes, our organization has a comprehensive 'Data Management and Governance Policy (DMGP-001-2025)' that was formally approved and communicated in Q1 2025. This policy specifies data classification, retention periods (ranging from 1 year for transient logs to 7+ years for financial records), and required protection levels (e.g., backup frequency, DR tiering) for all major data categories. Section 5.3 of DMGP-001 explicitly mandates an 'Energy Impact Assessment' for any data retention requirement exceeding 5 years for high-volume datasets, requiring justification if high-performance, energy-intensive storage is proposed over archival tiers. This policy is enforced through regular IT audits and automated data lifecycle management tools.",
          "badExampleAnswer": "We keep all data indefinitely on our main storage systems just in case we need it. We have backups too."
        },
        "question_56": {
          "id": 56,
          "old_question_text": null,
          "questionText": "Has the organization provided users with multiple logical data storage areas or repositories that are clearly identified by their respective data retention policies and levels of data protection? Is this policy communicated to users to enable them to store data in an area that appropriately matches the required levels of protection and retention? Is this considered particularly valuable where strong retention requirements exist, allowing data subject to these requirements to be separated at source to present opportunities for cost and energy savings, with automation of policy application where possible?",
          "simplifiedQuestion": "Do you offer users different storage areas clearly marked with their specific data retention and protection rules, so they can choose appropriately and help save energy/cost?",
          "reference": "2025 CoC Guide 4.4.2",
          "priority": "Medium (Value: 3, Expected: Optional Practice)",
          "stepSection": "step_GuideSection4_ITEquipmentServices_Part4",
          "recommendationTitle": "Separate user logical data storage areas by retention and protection policy",
          "document_requirements": {
            "required": false,
            "accepted_types": [
              "pdf",
              "docx",
              "screenshots"
            ],
            "description": "Documentation of different logical storage areas/tiers offered to users. Communication materials explaining the retention/protection policies for each area. Examples of automated policy application, if implemented."
          },
          "docRequired": "No",
          "acceptedFiles": "pdf, docx, screenshots",
          "documentDescriptor": "Optional: Documentation showing the provision of multiple logical storage areas for users, clearly identified by their retention and protection policies. Evidence of communication to users and any automation of these policies.",
          "verification_method": [
            "Document Review",
            "System Check",
            "Interview"
          ],
          "verificationMethods": "Review of storage architecture, user documentation for storage services, and data lifecycle management policies. Check system configurations for defined storage areas. Interviews with users and IT storage administrators.",
          "nonconformity": {
            "major": null,
            "minor": "A single, undifferentiated storage area is provided to users, or if multiple areas exist, their distinct retention/protection policies are not clearly communicated, leading to inefficient storage use (if practice is adopted)."
          },
          "majorNonconformity": null,
          "minorNonconformity": "If pursued: Users are not provided with clearly differentiated logical storage areas based on retention/protection policies, or these policies are not effectively communicated, hindering users from making appropriate storage choices for cost and energy savings.",
          "whatThisMeans": "To help manage data more efficiently, your organization can offer users different places to store their data, where each place has clear rules about how long data is kept and how well it's protected. Users should be informed about these options so they can choose the right storage for their needs. This is especially useful for data that must be kept for a long time, as it can be stored on less energy-intensive systems if it's separated from actively used data. Automating this separation where possible is also beneficial.",
          "complianceRequires": "- Provide users with multiple logical data storage areas (e.g., different network shares, storage tiers, repositories).\n- Clearly identify each storage area by its specific data retention policy and level of data protection.\n- Communicate this policy and the characteristics of each storage area to users, enabling them to store data appropriately.\n- Recognize the value of this approach, especially for data with strong retention requirements, as it allows segregation at source, offering potential cost and energy savings.\n- Where possible, automate the application of these storage policies (e.g., based on data type, age, or user department).",
          "expertTip": "Implement user-friendly interfaces or self-service portals that guide users in selecting the appropriate storage tier based on their data's characteristics (e.g., sensitivity, access frequency, retention needs). Combine this with automated data lifecycle management that can move data between tiers based on policy, reducing reliance on manual user actions.",
          "recommendedEvidence": "- Screenshots or documentation of the different logical storage areas available to users (e.g., naming conventions, descriptions).\n- User guides or internal communications detailing the retention policies and protection levels associated with each storage area.\n- Examples of how users are instructed to choose the appropriate storage area.\n- If automation is used, documentation of the tools or scripts that enforce these policies.",
          "recommendedActions": "- Define distinct logical storage areas or tiers based on different retention and protection requirements (e.g., active project data, short-term archive, long-term compliant archive).\n- Clearly document and communicate the purpose, retention policy, and protection level of each storage area to all users.\n- Provide training or guidance to users on selecting the correct storage for their data.\n- Explore and implement tools for automated data classification and tiering based on defined policies.",
          "sampleResponseTemplate": "We provide users with [Number] distinct logical storage areas, such as '[Area1 Name]' for [Policy1: e.g., active data, 90-day retention, high availability] and '[Area2 Name]' for [Policy2: e.g., archive data, 7-year retention, standard backup]. These are documented in our '[User Storage Guide ID]' and users are trained to select the appropriate area. Data subject to specific long-term retention for [Regulation X] is directed to [Specific Archive Area], which utilizes lower-power storage.",
          "humanInputAnswer": "Our users have access to three primary logical storage tiers via mapped network drives: 'Active_Projects' (high-performance, daily backup, 1-year retention), 'Completed_Projects_Archive' (medium-performance, weekly backup, 5-year retention), and 'LongTerm_Compliance_Hold' (lower-performance, monthly backup, indefinite retention as per legal requirements). These tiers and their policies are clearly communicated in our 'Data Storage SOP (DS-SOP-001)' and reinforced during onboarding. This allows users to self-select appropriate storage, reducing costs for long-term data.",
          "llmGeneratedAnswer": "Yes, our organization provides users with distinct logical data storage areas based on data lifecycle and retention requirements. As detailed in our 'Data Storage Service Levels (DSSL-2025)', users can select from 'Tier 1: Active Production Storage' (high availability, frequent backups, 90-day primary retention), 'Tier 2: Active Archive Storage' (standard availability, monthly backups, 1-7 year retention), and 'Tier 3: Deep Archive Storage' (lower access speed, yearly backups, 7+ year retention on lower-power media). These options and their associated policies are published on our intranet and communicated during data management training. This separation allows data with long retention periods, such as regulatory compliance data, to be stored more cost-effectively and with lower energy overhead on appropriate tiers.",
          "badExampleAnswer": "Everyone just saves their work on the main company network drive. It's all backed up."
        },
        "question_57": {
          "id": 57,
          "old_question_text": null,
          "questionText": "Has the organization created a tiered storage environment that utilizes multiple physical media types (e.g., flash/SSD, high-RPM disk, capacity-optimized disk, tape, cloud tiers) to deliver the required combinations of performance, capacity, and resilience for different data classes? Are there clear guidelines on the usage of these storage tiers with defined Service Level Agreements (SLAs) for performance and availability? Is a tiered charging model considered or implemented, based on usage at each performance level, to incentivize appropriate tier selection?",
          "simplifiedQuestion": "Do you use different types of physical storage (SSD, fast disk, slow disk, tape, cloud) in tiers, with clear rules and SLAs for each, to match data needs efficiently?",
          "reference": "2025 CoC Guide 4.4.3",
          "priority": "High (Value: 4, Expected: Optional Practice)",
          "stepSection": "step_GuideSection4_ITEquipmentServices_Part4",
          "recommendationTitle": "Separate physical data storage areas by protection and performance requirements",
          "document_requirements": {
            "required": false,
            "accepted_types": [
              "pdf",
              "docx",
              "xlsx"
            ],
            "description": "Storage architecture documents detailing the tiered storage environment and media types used. SLAs for different storage tiers. Guidelines for users/applications on selecting appropriate tiers. Details of any tiered charging model if implemented."
          },
          "docRequired": "No",
          "acceptedFiles": "pdf, docx, xlsx",
          "documentDescriptor": "Optional: Documentation of a tiered storage environment using multiple physical media types. Clearly defined SLAs for each tier. Guidelines for usage and, if applicable, a tiered charging model.",
          "verification_method": [
            "Document Review",
            "System Configuration Review"
          ],
          "verificationMethods": "Review of storage architecture documents, SLAs, usage guidelines, and charging models (if any). Examination of storage system configurations to verify tiering implementation.",
          "nonconformity": {
            "major": null,
            "minor": "A single type of high-performance storage is used for all data, irrespective of access frequency or performance needs, or if tiers exist, guidelines/SLAs are unclear (if practice is adopted)."
          },
          "majorNonconformity": null,
          "minorNonconformity": "If pursued: A tiered storage environment is not implemented, or if it is, it lacks clear differentiation by media type, performance/resilience SLAs, or guidelines for appropriate usage, leading to inefficient use of high-cost/high-energy storage for data that does not require it.",
          "whatThisMeans": "Your organization should consider using different types of physical storage media (like very fast SSDs, standard hard drives, slower large-capacity drives, tape, or different cloud storage classes) organized into 'tiers'. Each tier would offer a different mix of speed, storage space, and data protection. Clear rules and service level agreements (SLAs) should define when to use each tier. You might even charge internal users differently based on the tier they use, to encourage them to pick the most appropriate and cost/energy-efficient option.",
          "complianceRequires": "- Create a tiered storage environment utilizing multiple physical media types.\n- These media types should deliver the required combinations of performance, capacity, and resilience for different classes of data.\n- Implement clear guidelines on the usage of these storage tiers.\n- Define Service Level Agreements (SLAs) for performance (e.g., IOPS, latency, throughput) and availability for each tier.\n- Consider implementing a tiered charging model (for internal chargeback/showback or external customers) based on usage at each performance/resilience level to incentivize appropriate tier selection.",
          "expertTip": "Automate data movement between storage tiers (Automated Storage Tiering - AST) based on access frequency and age. This ensures that data automatically migrates to the most cost-effective and energy-efficient tier without requiring constant manual intervention, while still providing high performance for frequently accessed 'hot' data.",
          "recommendedEvidence": "- Storage architecture document detailing the different physical storage tiers, media types used (e.g., SSD, SAS, NL-SAS, tape, cloud tiers), and their characteristics.\n- Service Level Agreements (SLAs) defined for each storage tier (e.g., performance, availability, RPO/RTO).\n- User guidelines or policies on how to select and utilize different storage tiers.\n- If implemented, documentation of a tiered charging model for storage usage.\n- Reports showing data distribution across different tiers.",
          "recommendedActions": "- Analyze data access patterns and performance requirements for different applications and datasets.\n- Design and implement a tiered storage architecture using appropriate physical media types to match these varying needs efficiently.\n- Define and publish clear SLAs and usage guidelines for each storage tier.\n- Implement tools for monitoring data placement and performance across tiers.\n- Consider automated storage tiering solutions to dynamically optimize data placement.\n- If applicable, develop a tiered charging model to influence user behavior towards more efficient storage choices.",
          "sampleResponseTemplate": "We have a [Number]-tiered storage environment: Tier 1 ([e.g., All-Flash SSDs] for [Use Case, e.g., databases, VDI]), Tier 2 ([e.g., Hybrid SAS] for [Use Case, e.g., general file services]), and Tier 3 ([e.g., High-capacity NL-SAS/Tape/Cloud Archive] for [Use Case, e.g., backups, long-term archive]). SLAs for IOPS, latency, and availability are defined for each tier in our '[Storage Service Definition Document ID]'. Guidelines on tier usage are provided to application owners. [Optional: We are exploring a tiered showback model based on cost and energy profile of each tier.]",
          "humanInputAnswer": "Our enterprise storage platform is a 3-tier architecture: Tier 0 (NVMe Flash for extreme performance applications), Tier 1 (SAS SSD for production VMs and databases), and Tier 2 (High-capacity SAS/SATA HDD for file shares, backups, and less active data). This is documented in 'Storage Architecture Standard ST-ARCH-001'. Each tier has defined performance (IOPS/latency) and availability SLAs. Automated tiering (FAST VP) moves data blocks between Tier 1 and Tier 2 based on access frequency. We are currently developing a showback model to reflect the different costs per TB of each tier.",
          "llmGeneratedAnswer": "Yes, our organization has implemented a tiered storage environment, as detailed in our 'Enterprise Storage Architecture Document (ESAD-2025)'. This environment utilizes:\n- Tier 1: All-Flash arrays (NVMe SSDs) for high-performance databases and latency-sensitive applications, offering sub-millisecond latency and 99.999% availability.\n- Tier 2: Hybrid arrays (SSD cache with SAS HDDs) for general virtual machine workloads and file services, providing balanced performance and capacity with 99.99% availability.\n- Tier 3: Capacity-optimized disk arrays (NL-SAS HDDs) and an LTO-8 tape library for backups, archiving, and infrequently accessed data, focused on cost-effective, energy-efficient long-term storage with appropriate RTO/RPOs.\nClear guidelines on tier usage, along with SLAs for IOPS, throughput, and availability, are provided to application owners via our 'Storage Service Catalog'. A tiered internal showback model, reflecting the cost and energy profile of each tier, is in effect to encourage appropriate data placement.",
          "badExampleAnswer": "We buy fast storage when we need performance and cheap, big drives for everything else. Users save files where they want.",
          "questions": {
            "question_58": {
              "id": 58,
              "old_question_text": null,
              "questionText": "When selecting new storage hardware, does the organization evaluate and prioritize energy efficiency in terms of the service delivered per Watt? Does this evaluation consider deployment-specific factors, including achieved performance (e.g., IOPS/Watt) and storage volume per Watt (e.g., TB/Watt), as well as additional factors like data protection levels, performance availability, and recovery capability required to meet business service level requirements defined in the data management policy? Are both in-use power draw and peak power of the configured storage device(s) evaluated? Is compliance with the EU Eco Design Directive and Lot 9 amendments for online storage systems ensured as a minimum?",
              "simplifiedQuestion": "When buying new storage, do you pick lower power devices based on performance/capacity per Watt, considering real-world needs and EU Eco Design rules?",
              "reference": "2025 CoC Guide 4.4.4",
              "priority": "High (Value: 4, Expected: Optional Practice)",
              "stepSection": "step_GuideSection4_ITEquipmentServices_Part4",
              "recommendationTitle": "Select lower power storage devices",
              "document_requirements": {
                "required": false,
                "accepted_types": [
                  "pdf",
                  "docx",
                  "xlsx"
                ],
                "description": "Storage procurement policies emphasizing energy efficiency (service/Watt). Evaluation criteria for storage hardware including IOPS/Watt, TB/Watt. Analysis of in-use and peak power for selected storage. Vendor declarations for EU Eco Design Lot 9 compliance."
              },
              "docRequired": "No",
              "acceptedFiles": "pdf, docx, xlsx",
              "documentDescriptor": "Optional: Procurement policies for storage hardware that prioritize energy efficiency (e.g., IOPS/Watt, TB/Watt). Vendor documentation confirming EU Eco Design Lot 9 compliance. Analysis showing evaluation of both in-use and peak power of selected storage solutions.",
              "verification_method": [
                "Document Review",
                "Procurement Record Audit"
              ],
              "verificationMethods": "Review of storage procurement policies and recent tender evaluation documents. Audit of specifications for newly acquired storage hardware.",
              "nonconformity": {
                "major": null,
                "minor": "New storage hardware is selected primarily on capacity and performance, without significant evaluation of energy efficiency metrics like service delivered per Watt, or EU Eco Design Lot 9 compliance is not verified (if practice is adopted)."
              },
              "majorNonconformity": null,
              "minorNonconformity": "If pursued: Procurement of new storage hardware does not systematically evaluate energy efficiency (e.g., performance/Watt, volume/Watt) or consider both in-use and peak power draw. Compliance with EU Eco Design Lot 9 for online storage is not consistently verified.",
              "whatThisMeans": "When buying new data storage systems, your organization should carefully look at how much energy they use for the performance and capacity they provide (e.g., how many operations per second per watt, or how much storage per watt). This includes considering data protection and availability needs defined by your data management policy. You should assess both the typical power use and the maximum power draw. As a minimum, ensure the storage meets EU Eco Design rules (Lot 9). Choosing lower power storage devices helps reduce overall energy consumption.",
              "complianceRequires": "- When selecting new storage hardware, evaluate energy efficiency in terms of service delivered per Watt.\n- Consider deployment-specific factors, including achieved performance per Watt (e.g., IOPS/Watt) and storage volume per Watt (e.g., TB/Watt).\n- Factor in data protection levels, performance availability, and recovery capability required by the data management policy.\n- Evaluate both the in-use power draw and the peak power of the storage device(s) as configured.\n- Ensure compliance with the EU Eco Design Directive and Lot 9 amendments for online storage systems as a minimum.",
              "expertTip": "Look beyond raw capacity (TB) and consider density (TB/U) and power density (TB/Watt or IOPS/Watt) when evaluating storage. Modern high-density drives and flash storage can offer significant energy savings per unit of storage or performance compared to older technologies. Also, assess the power consumption of storage controllers and chassis, not just the media.",
              "recommendedEvidence": "- Storage procurement policy specifying energy efficiency (performance/Watt, capacity/Watt) as a key selection criterion.\n- Vendor proposals or datasheets for storage solutions showing energy efficiency metrics and power consumption data (in-use and peak).\n- Evaluation documentation from recent storage procurements comparing different options based on service delivered per Watt.\n- Vendor declarations of conformity for EU Eco Design Directive Lot 9 for online storage systems.",
              "recommendedActions": "- Update storage procurement policies to include specific energy efficiency metrics (e.g., IOPS/Watt, TB/Watt) as primary evaluation criteria.\n- Require vendors to provide detailed power consumption data for their storage solutions under various load conditions and configurations.\n- Ensure all new online storage systems comply with EU Eco Design Directive Lot 9.\n- Train procurement and IT staff on how to evaluate and compare the energy efficiency of different storage hardware options.",
              "sampleResponseTemplate": "Our 'Storage Hardware Procurement Standard' ([Doc ID: SHP-003]) requires evaluation based on [TB/Watt and IOPS/Watt] to meet our service level needs defined in our data management policy. For our recent [Storage Array X] purchase, we evaluated its [in-use and peak power] and confirmed its compliance with EU Eco Design Lot 9. It offered a [Y]% better [TB/Watt] compared to the alternative.",
              "humanInputAnswer": "When procuring new storage, our 'Storage Acquisition Policy (SAP-002)' mandates an evaluation of watts/TB for capacity tiers and IOPS/watt for performance tiers. For our recent Tier 1 storage refresh, 'VendorA FlashArray' was selected due to its superior IOPS/watt rating (35% better than the runner-up) and compliance with EU Eco Design Lot 9. We assess both typical and peak power draw based on vendor-supplied tools and our workload projections.",
              "llmGeneratedAnswer": "Yes, our 'IT Storage Procurement Policy (ITSPP-2025, Section 4)' mandates the evaluation of energy efficiency for all new storage hardware. This assessment prioritizes 'service delivered per Watt,' considering metrics such as TB/Watt for capacity-focused tiers and IOPS/Watt for performance-intensive tiers, always factoring in our defined data protection and availability SLAs. Both typical in-use power draw and maximum configured peak power are evaluated. As a baseline, all new online storage systems must comply with the EU Eco Design Directive Lot 9 amendments; for instance, our recent 'SAN Upgrade Project Phase 2' exclusively selected Lot 9 compliant arrays that also demonstrated leading TB/Watt figures in their class.",
              "badExampleAnswer": "We buy storage based on how much capacity we need and the price per terabyte."
            },
            "question_59": {
              "id": 59,
              "old_question_text": null,
              "questionText": "Has the organization implemented an effective data identification and management policy and process aimed at reducing the total volume of data stored? Does this include initiatives such as 'clean up days' where users are encouraged or required to delete unnecessary data from storage systems?",
              "simplifiedQuestion": "Do you have policies and processes (like 'clean up days') to actively reduce the total amount of data you store?",
              "reference": "2025 CoC Guide 4.4.5",
              "priority": "High (Value: 4, Expected: Optional Practice)",
              "stepSection": "step_GuideSection4_ITEquipmentServices_Part4",
              "recommendationTitle": "Reduce total data volume",
              "document_requirements": {
                "required": false,
                "accepted_types": [
                  "pdf",
                  "docx",
                  "xlsx"
                ],
                "description": "Data reduction policy or guidelines. Procedures for data identification and deletion. Communication materials for 'clean up days' or similar initiatives. Reports showing data volume reduction achieved."
              },
              "docRequired": "No",
              "acceptedFiles": "pdf, docx, xlsx",
              "documentDescriptor": "Optional: Policy and procedures for identifying and reducing total stored data volume. Evidence of initiatives like 'data clean-up days' and their impact.",
              "verification_method": [
                "Document Review",
                "Interview",
                "Data Analysis"
              ],
              "verificationMethods": "Review of data reduction policies and procedures. Interviews with IT staff and data owners. Analysis of storage consumption trends and impact of reduction initiatives.",
              "nonconformity": {
                "major": null,
                "minor": "No proactive measures are taken to identify and reduce unnecessary stored data volumes, leading to continuous growth in storage demand and energy consumption (if practice is adopted)."
              },
              "majorNonconformity": null,
              "minorNonconformity": "If pursued: Lack of an effective data identification and management policy/process to reduce overall data volume, or initiatives like 'clean up days' are ineffective or not implemented.",
              "whatThisMeans": "Your organization should have a formal way to identify and get rid of data that's no longer needed. This helps reduce the total amount of data you're storing, which in turn reduces the energy needed for storage systems. Activities like organizing 'clean up days' where users delete their old, unnecessary files can be part of this.",
              "complianceRequires": "- Implement an effective data identification and management policy and process aimed at reducing the total volume of data stored.\n- Consider implementing initiatives such as 'clean up days' where users are actively encouraged or required to delete unnecessary data from storage systems.\n- This practice complements data lifecycle management and archival strategies by focusing on outright deletion of unneeded data.",
              "expertTip": "Implement automated tools for data discovery and classification that can identify redundant, obsolete, and trivial (ROT) data. Combine this with clear data ownership and a defined process for reviewing and approving data deletion. Gamification or departmental targets can also help encourage participation in data clean-up initiatives.",
              "recommendedEvidence": "- Policy document outlining data reduction goals and processes.\n- Procedures for identifying and deleting unnecessary data (e.g., ROT data removal process).\n- Communications or materials related to 'data clean up days' or similar user-focused initiatives.\n- Reports or metrics showing trends in total data volume and any reductions achieved through these efforts.\n- Data lifecycle management policies that include criteria for data deletion.",
              "recommendedActions": "- Develop and implement a data reduction policy that includes data identification, ownership, and deletion processes.\n- Launch awareness campaigns and initiatives like 'clean up days' to engage users in reducing unnecessary data.\n- Utilize data management tools to identify ROT data and automate deletion where appropriate and safe.\n- Set targets for reducing overall data volume or growth rate.\n- Integrate data reduction efforts with data retention policies and archival strategies.",
              "sampleResponseTemplate": "Our 'Data Minimization Policy' ([Policy ID: DMP-002]) includes an annual 'Data Clean-Up Week' where users are prompted to review and delete unnecessary files from their allocated network storage and collaboration platforms. We also run quarterly reports to identify and address large, aging datasets. Last year, these initiatives resulted in a [X TB / Y %] reduction in [specific storage area].",
              "humanInputAnswer": "Our 'Data Lifecycle Management Policy (DLMP-2024)' includes a 'Redundant, Obsolete, Trivial (ROT) Data Elimination' program. Annually, we run a 'Data Spring Clean' campaign, providing users with tools and reports to identify and delete unneeded personal and departmental data. In 2024, this initiative resulted in a 7% reduction (approx. 50TB) in user-managed file storage, delaying the need for an additional storage shelf.",
              "llmGeneratedAnswer": "Yes, our organization has implemented a 'Data Volume Reduction Initiative (DVRI-2025)' as part of our overall data management strategy, detailed in 'DMG-Policy-003: Data Lifecycle and Minimization'. This initiative includes a biannual 'Digital Clean-Up Drive' where employees are encouraged, through awareness campaigns and departmental challenges, to review and delete unnecessary data from their network shares and email archives. Additionally, automated scripts identify and flag files unaccessed for over 3 years for owner review and potential deletion. In the last cycle, this led to a reclamation of approximately 15TB of storage capacity.",
              "badExampleAnswer": "We just keep buying more storage as users need it. It's hard to get people to delete things."
            },
            "question_60": {
              "id": 60,
              "old_question_text": null,
              "questionText": "Has the organization implemented its data management policy to reduce the total storage volume consumed by minimizing the number of data copies (both logical and physical, such as mirrors)? Does this include implementing storage subsystem space-saving features like space-efficient snapshots/copies or compression, and utilizing storage subsystem thin provisioning features where possible and appropriate for the workload?",
              "simplifiedQuestion": "Do you actively work to reduce storage space used by minimizing data copies and using storage-saving features like thin provisioning, snapshots, and compression?",
              "reference": "2025 CoC Guide 4.4.6",
              "priority": "High (Value: 4, Expected: Optional Practice)",
              "stepSection": "step_GuideSection4_ITEquipmentServices_Part4",
              "recommendationTitle": "Reduce total storage volume",
              "document_requirements": {
                "required": false,
                "accepted_types": [
                  "pdf",
                  "docx",
                  "xlsx",
                  "screenshots"
                ],
                "description": "Storage management policies detailing strategies for reducing data copies and using space-saving features. Configuration details or reports from storage systems showing use of thin provisioning, efficient snapshots, compression, deduplication. Data on storage efficiency ratios achieved."
              },
              "docRequired": "No",
              "acceptedFiles": "pdf, docx, xlsx, screenshots",
              "documentDescriptor": "Optional: Documentation of strategies to reduce total storage volume, such as minimizing data copies and implementing space-saving storage features (thin provisioning, efficient snapshots, compression, deduplication). Evidence of their application and effectiveness.",
              "verification_method": [
                "Document Review",
                "System Configuration Review"
              ],
              "verificationMethods": "Review of storage management policies and procedures. Examination of storage system configurations and reports to verify use of space-saving features and efforts to reduce data copies.",
              "nonconformity": {
                "major": null,
                "minor": "Opportunities to reduce stored data volume through minimizing copies or using space-saving storage features are not systematically pursued (if practice is adopted)."
              },
              "majorNonconformity": null,
              "minorNonconformity": "If pursued: Failure to implement data management policies that actively reduce storage volume by minimizing data copies (logical and physical) or by not leveraging storage subsystem space-saving features like thin provisioning, efficient snapshots, compression, or deduplication where appropriate.",
              "whatThisMeans": "To save storage space (and thus energy), your organization should implement its data management policy in ways that cut down on how much storage is actually used. This means reducing the number of duplicate copies of data (both separate files and backup mirrors). It also involves using smart features built into modern storage systems, like thin provisioning (allocating space only as it's truly needed), space-efficient snapshots and copies (which don't duplicate all the data), and data compression or deduplication.",
              "complianceRequires": "- Implement the data management policy (from 4.4.1) in a way that aims to reduce the total storage volume consumed.\n- Minimize the number of copies of data, both logical (e.g., multiple versions where one would suffice) and physical (e.g., unnecessary mirrors or full clones).\n- Implement and utilize storage subsystem space-saving features where appropriate, such as:\n    - Space-efficient snapshots/copies.\n    - Data compression.\n    - Data deduplication (though not explicitly listed, it's a key space-saving feature often coupled with compression).\n- Implement storage subsystem thin provisioning features where possible and suitable for the workload.",
              "expertTip": "Regularly review your data protection strategies (backup, replication, snapshots) to ensure they meet RPO/RTO requirements without creating excessive, unnecessary data copies. Modern backup solutions often have integrated deduplication and efficient snapshot technologies that can significantly reduce storage footprint compared to traditional methods.",
              "recommendedEvidence": "- Storage provisioning policy or standards that mandate the use of thin provisioning and other space-saving features where appropriate.\n- Documentation of backup and replication strategies, highlighting efforts to minimize redundant data copies (e.g., use of differential/incremental backups, deduplication).\n- Reports from storage systems showing utilization of thin provisioning, compression ratios, deduplication rates, and efficient snapshot usage.\n- Examples of how storage capacity planning considers and leverages these space-saving features.",
              "recommendedActions": "- Standardize on storage platforms that offer robust space-saving features (thin provisioning, snapshots, compression, deduplication).\n- Implement these features as the default for new storage allocations where workloads permit.\n- Review existing storage allocations and identify opportunities to apply space-saving features retrospectively (where feasible and non-disruptive).\n- Educate application owners and users on the benefits and appropriate use of these features.\n- Regularly monitor and report on the effectiveness of space-saving features (e.g., achieved compression/deduplication ratios).",
              "sampleResponseTemplate": "Our storage provisioning standard ([Doc ID: STO-PROV-001]) mandates the use of [thin provisioning/compression/deduplication] for [all new LUNs/specific data types]. We also utilize space-efficient snapshots for our [VMware backups/database protection], reducing copy data by [X%]. Currently, our overall storage efficiency ratio (logical vs. physical used) is [Y:1] due to these measures.",
              "humanInputAnswer": "Our 'Storage Resource Management Policy (SRMP-002)' mandates thin provisioning for all new virtual server LUNs and enables inline compression and deduplication by default on our primary flash storage arrays (VendorX PureStorage). We also use space-efficient snapshots for daily operational recovery points, retaining them for 7 days. This has resulted in an average data reduction ratio of 3.5:1 on our primary storage, effectively tripling our usable capacity compared to raw provisioned space.",
              "llmGeneratedAnswer": "Yes, our data management policy (DMGP-001-2025, Section 6) is actively implemented to reduce total storage volume. This includes:\n1.  Minimizing data copies: Our backup strategy (Ref: 'Backup & Recovery SOP BR-003') leverages incremental backups and source-side deduplication to minimize redundant physical copies. Snapshot policies for development/test environments utilize space-efficient snapshot technology, retaining only changed blocks.\n2.  Storage subsystem features: All our primary storage arrays (e.g., 'Dell PowerStore', 'NetApp AFF') are configured with thin provisioning by default for all LUNs. Inline data compression and deduplication are enabled on these arrays, achieving an average data reduction ratio of 2.8:1. These settings are audited quarterly as per 'Storage Configuration Audit SCA-001'.",
              "badExampleAnswer": "We just allocate the storage LUNs as requested by the server team. We don't really look at how many copies of data they make."
            }
          }
        }
      }
    }
  },
  "step_6": {
    "description": "2025 Guide Section 5: Cooling (Practices 5.1.1 through 5.1.10 - Air Flow Management and Design, Part 1)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Cooling_1"
    ],
    "critical_questions": [],
    "questions": {
      "question_61": {
        "id": 61,
        "old_question_text": null,
        "questionText": "Has the organization designed its data centre layouts using the 'Hot / Cold Aisle' concept to manage airflow for IT equipment? Does this design ensure that IT equipment shares a common air flow direction (within cabinets, in adjacent cabinets, and across aisles) by creating dedicated cold aisles (fed with cold air for equipment intake) and hot aisles (with no cold air feed, for equipment exhaust)? Is this Hot / Cold aisle design reinforced by using empty but fully blanked cabinets (or cabinets with solid doors) rather than leaving gaps in aisles which can disrupt airflow patterns?",
        "simplifiedQuestion": "Do you use a Hot/Cold Aisle layout for your IT equipment, ensuring consistent airflow and filling empty rack spaces to maintain it?",
        "reference": "2025 CoC Guide 5.1.1",
        "priority": "High (Value: 5, Expected: Entire Data Centre)",
        "stepSection": "step_6",
        "recommendationTitle": "Design - Hot / Cold aisle",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "dwg",
            "jpg"
          ],
          "description": "Data centre layout drawings/diagrams showing Hot/Cold Aisle configuration. Photographic evidence of implemented Hot/Cold Aisles. Standards or guidelines for IT equipment installation regarding airflow. Procedures for managing empty rack spaces (e.g., use of blanked cabinets)."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, dwg, jpg",
        "documentDescriptor": "Mandatory: Data centre floor plans showing clear Hot/Cold Aisle layout. Photographic evidence. Documentation of policies for equipment installation ensuring correct orientation and for managing empty rack spaces (e.g., blanking or using blank cabinets).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of data centre layout diagrams and equipment installation guidelines. Physical inspection of data halls to verify Hot/Cold Aisle configuration, equipment orientation, and management of empty rack spaces.",
        "nonconformity": {
          "major": "No consistent Hot/Cold Aisle layout is implemented, leading to significant mixing of hot and cold air and reduced cooling efficiency.",
          "minor": "Hot/Cold Aisle layout is generally implemented but with inconsistencies, such as incorrectly oriented equipment, significant gaps in rows, or unblanked empty cabinets, partially compromising airflow separation."
        },
        "majorNonconformity": "Absence of a clearly defined and consistently implemented Hot/Cold Aisle layout, resulting in widespread mixing of equipment exhaust air with cold intake air.",
        "minorNonconformity": "A Hot/Cold Aisle layout is present but poorly maintained, with numerous gaps, incorrectly faced equipment, or unblanked empty rack spaces that degrade airflow separation.",
        "whatThisMeans": "Your data centre should be arranged so that IT equipment is lined up in rows, creating alternating 'cold aisles' where cool air is supplied to the front of the equipment, and 'hot aisles' where the hot air exhausts from the back. All equipment must face the correct way (e.g., front to cold aisle, back to hot aisle). Empty spaces in racks within an aisle should be filled with blanking panels or empty, fully blanked cabinets to prevent hot and cold air from mixing, which makes cooling less efficient.",
        "complianceRequires": "- Design and implement a 'Hot / Cold Aisle' layout for IT equipment.\n- Ensure all IT equipment within cabinets, in adjacent cabinets, and across aisles shares a common airflow direction.\n- Create dedicated cold aisles fed with cold air for equipment intake.\n- Create dedicated hot aisles (with no direct cold air feed) for equipment exhaust.\n- Reinforce the Hot / Cold Aisle design by using empty but fully blanked cabinets (or cabinets with solid doors if appropriate for the design) to fill gaps in aisles, rather than leaving empty spaces that allow air to bypass or recirculate.",
        "expertTip": "Ensure strict adherence to equipment installation orientation within the Hot/Cold Aisle layout. Even a few incorrectly installed devices can significantly degrade the effectiveness of the airflow separation. Use clear labeling on racks or aisles to indicate correct equipment orientation for installation teams.",
        "recommendedEvidence": "- Data centre floor plans and rack layout diagrams clearly showing the Hot/Cold Aisle configuration.\n- Photographic evidence of the implemented Hot/Cold Aisles, including proper equipment orientation.\n- Standard operating procedures or installation guidelines for IT equipment that specify correct airflow orientation.\n- Evidence of how empty rack spaces are managed (e.g., photos of blanked cabinets or installed blanking panels).",
        "recommendedActions": "- If not already implemented, redesign data hall layouts to establish clear Hot/Cold Aisles.\n- Develop and enforce strict IT equipment installation standards that ensure correct airflow orientation.\n- Implement a policy for managing empty rack spaces, requiring the use of blanking panels or fully blanked empty cabinets to maintain aisle integrity.\n- Regularly audit data halls for compliance with Hot/Cold Aisle discipline.",
        "sampleResponseTemplate": "Our data centres are designed with a strict Hot/Cold Aisle layout, as documented in our 'Data Centre Design Standard DC-DS-001'. All IT equipment is installed with front intakes facing the cold aisle and rear exhaust into the hot aisle. Empty rack positions are filled with [blanking panels / fully blanked decommissioned cabinets] to maintain airflow separation. This is verified through our quarterly data hall audits.",
        "humanInputAnswer": "All our data halls utilize a Hot/Cold Aisle configuration, as specified in 'DC Physical Layout Standard PLS-002'. Server inlets universally face the cold aisles, and exhausts into hot aisles. We maintain a 100% blanking panel policy for unused rack U-space, and any empty cabinet positions in a row are filled with either blank placeholder cabinets or fully doored and blanked empty racks to prevent air mixing. This is part of our standard equipment installation checklist.",
        "llmGeneratedAnswer": "Yes, all our data centre production environments adhere to a rigorously enforced Hot/Cold Aisle design, as stipulated in our 'Data Centre Build and Operations Manual (DCBOM-2025, Section 4.2)'. This involves aligning IT equipment so that server fronts (intakes) in one row face the server fronts in the adjacent row, creating a cold aisle for air supply, while server rears (exhausts) face each other to form a hot aisle for air return. To reinforce this, our policy (DCBOM-2025, Section 4.2.1) mandates that any gaps in cabinet rows are filled with either fully blanked empty cabinets or dedicated airflow management panels to prevent bypass airflow.",
        "badExampleAnswer": "Our servers are in rows, but some face different ways. We have some fans to move air around if it gets too hot in places."
      },
      "question_62": {
        "id": 62,
        "old_question_text": null,
        "questionText": "Has the organization implemented designs for new builds or retrofits that physically contain and separate the cold air from the heated return air on the data floor? Examples include Hot Aisle Containment, Cold Aisle Containment, contained cabinet supply, contained cabinet return (including cabinet chimneys), or contained room return and room supply. Is it understood that failure to contain airflow reduces cooling efficiency and increases risk, especially with variable airflow IT devices, and that this practice, when implemented, supersedes Practice 5.1.1 (basic Hot/Cold Aisle)?",
        "simplifiedQuestion": "For new/retrofitted data centres, do you use physical containment (like enclosed aisles or cabinet chimneys) to separate hot and cold air?",
        "reference": "2025 CoC Guide 5.1.2",
        "priority": "High (Value: 5, Expected: Entire Data Centre)",
        "stepSection": "step_6",
        "recommendationTitle": "Design - Contained hot or cold air",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "dwg",
            "jpg"
          ],
          "description": "Design documents for new builds/retrofits showing airflow containment strategy (Hot Aisle Containment, Cold Aisle Containment, cabinet-level containment). Specifications of containment systems used. Photographic evidence of implemented containment."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, dwg, jpg",
        "documentDescriptor": "Mandatory: Design documentation and evidence (e.g., photos, specifications) of implemented airflow containment systems (e.g., Hot Aisle Containment, Cold Aisle Containment, cabinet chimneys) for separating cold supply air from hot return air.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of data centre design documents showing containment strategy. Physical inspection of data halls to verify the type, extent, and integrity of airflow containment systems.",
        "nonconformity": {
          "major": "No airflow containment strategy is implemented in areas with significant IT load, leading to substantial mixing of hot and cold air and high cooling energy consumption.",
          "minor": "Containment is implemented but is incomplete, has significant leakage, or is not appropriately designed for the IT equipment and cooling system, reducing its effectiveness."
        },
        "majorNonconformity": "Absence of any physical containment system (aisle or cabinet level) to separate cold supply air from hot exhaust air in data halls, especially where Practice 5.1.1 (Hot/Cold Aisle) is already in place.",
        "minorNonconformity": "A containment system is implemented but has significant gaps, leaks, or design flaws that compromise its ability to effectively separate hot and cold air streams, or it is not applied to all relevant areas.",
        "whatThisMeans": "Beyond just having hot and cold aisles (Practice 5.1.1), your organization should use physical barriers to completely separate the cold air supplied to IT equipment from the hot air coming out of it. This can be done by enclosing the hot aisles (Hot Aisle Containment), enclosing the cold aisles (Cold Aisle Containment), or using special cabinets with built-in air containment (like cabinet chimneys or direct supply/return). This is crucial for cooling efficiency and managing risk, especially as modern IT gear can change its airflow and heat output rapidly. If you do this, it's a more advanced step than just basic hot/cold aisles.",
        "complianceRequires": "- Implement design concepts that physically contain and separate cold supply air from heated return air on the data floor.\n- Examples of such concepts include:\n    - Hot aisle containment.\n    - Cold aisle containment.\n    - Contained cabinet supply.\n    - Contained cabinet return (including cabinet chimneys).\n    - Contained room return and room supply.\n- Understand that failure to contain airflow reduces achievable cooling efficiency and increases operational risk due to unpredictable airflow patterns from variable airflow IT devices.\n- Recognize that implementing a containment practice (5.1.2) is an advancement and supersedes the basic Hot/Cold Aisle separation practice (5.1.1) in the areas where containment is applied.",
        "expertTip": "Ensure your chosen containment solution is well-sealed. Small gaps in aisle-end doors, between racks, or under cabinets can significantly reduce the effectiveness of containment. Regularly inspect and maintain the integrity of the containment barriers. Also, integrate containment with your fire suppression system design to ensure it functions correctly.",
        "recommendedEvidence": "- Design drawings and specifications for the implemented airflow containment system (e.g., HACS, CACS, cabinet chimneys).\n- Photographs of the installed containment solution in data halls.\n- Vendor documentation for any pre-fabricated containment components used.\n- Thermal imaging or airflow studies demonstrating the effectiveness of the containment in separating hot and cold air.",
        "recommendedActions": "- For new data centre builds or major retrofits, incorporate an airflow containment strategy (aisle or cabinet-level) into the design from the outset.\n- For existing facilities with Hot/Cold Aisles, evaluate the feasibility and benefits of retrofitting a containment solution (as per Practice 5.1.3).\n- Ensure that the containment solution is properly installed, sealed, and maintained to prevent air leakage.\n- Coordinate the containment design with fire detection and suppression systems, as well as lighting and access requirements.",
        "sampleResponseTemplate": "Our primary data halls utilize a [Hot Aisle Containment System (HACS) / Cold Aisle Containment System (CACS)] as detailed in our 'Airflow Management Standard AMS-001'. This was implemented in [Year/Project] and includes [specific components like aisle-end doors, roof panels]. We conduct quarterly inspections to ensure containment integrity. This supersedes basic hot/cold aisle separation in these areas.",
        "humanInputAnswer": "All our production data halls, built or retrofitted since 2022, employ Cold Aisle Containment (CAC) systems with rigid partitions, ceiling panels, and automated aisle-end doors (as per design standard DC-COOL-DES-004). For example, Data Hall 5 uses 'VendorX AisleContain' solution. This ensures complete separation of cold supply air from hot return air, and this practice effectively supersedes simple hot/cold aisle layouts in these areas.",
        "llmGeneratedAnswer": "Yes, our organization implements full Cold Aisle Containment (CAC) in all new data hall builds and major retrofits since 2023, as mandated by our 'Data Centre Cooling Design Standard (DCCDS-2025, Section 3.1)'. This involves enclosing the cold aisles with end-of-row doors and overhead ceiling panels to create a sealed cold air plenum. This practice physically separates the cold supply air from the hot equipment exhaust, which significantly improves cooling efficiency and predictability, especially with variable airflow IT devices. This containment strategy is understood to supersede the basic hot/cold aisle separation (Practice 5.1.1) in the areas it is deployed.",
        "badExampleAnswer": "We have hot and cold aisles, but we don't use any barriers or enclosures as it makes it harder to access the equipment."
      },
      "question_63": {
        "id": 63,
        "old_question_text": null,
        "questionText": "For existing data centres where a basic Hot / Cold Aisle separation is already in use but there is no full containment of hot or cold air (as per Practice 5.1.2), has the organization considered or implemented retrofitting measures to provide basic separation, for example, using curtains or other methods? If such retrofitting is undertaken, is care taken to assess and verify fire system requirements and regulations?",
        "simplifiedQuestion": "In existing data centres with Hot/Cold aisles, have you considered or added basic containment (like curtains) to improve airflow separation, ensuring fire safety is checked?",
        "reference": "2025 CoC Guide 5.1.3",
        "priority": "Medium (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_6",
        "recommendationTitle": "Design - Contained hot or cold air - Retrofit",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg"
          ],
          "description": "Evaluation reports for retrofitting containment. Design and specification of any retrofitted basic separation (e.g., curtains). Fire safety assessment related to the retrofit. Photographic evidence of retrofitted solutions."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, docx, jpg",
        "documentDescriptor": "Applicable to existing DCs with Hot/Cold Aisles: Documentation of feasibility studies for retrofitting containment or evidence of implemented basic separation (e.g., vinyl curtains, modular panels), including fire safety assessments for such modifications.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of any retrofit containment plans, designs, and fire safety assessments. Physical inspection of data halls to observe any retrofitted basic separation measures.",
        "nonconformity": {
          "major": "No consideration given to retrofitting basic containment in existing Hot/Cold Aisle setups where significant air mixing is evident and full containment (5.1.2) is not planned.",
          "minor": "Retrofitting of basic separation was considered but dismissed without thorough justification, or implemented measures are poorly designed/maintained, or fire safety implications were not adequately assessed."
        },
        "majorNonconformity": "In an existing data centre with established Hot/Cold aisles but no full containment, no consideration has been given to or no action taken to retrofit even basic separation measures (like curtains), despite clear opportunities for efficiency gains.",
        "minorNonconformity": "Retrofit basic separation measures have been implemented but are not well-maintained, have significant gaps, or the fire safety implications of the retrofit were not thoroughly assessed and verified.",
        "whatThisMeans": "If your existing data centre already has hot and cold aisles but doesn't have full physical barriers (like enclosed aisles from Practice 5.1.2), your organization should look into adding simpler forms of separation. This could involve using things like heavy-duty curtains or panels to better keep the hot and cold air from mixing. When doing this, it's very important to check and make sure it doesn't interfere with your fire safety systems.",
        "complianceRequires": "- For existing data centres already employing Hot/Cold Aisle separation (Practice 5.1.1) but lacking full containment (Practice 5.1.2), consider retrofitting measures to provide basic airflow separation.\n- Such retrofitting could involve, for example, using curtains (e.g., vinyl strips or sheets) or other modular panel methods to partially enclose aisles or redirect airflow.\n- If retrofitting basic separation, take due care to assess and verify fire system requirements and regulations to ensure continued compliance and safety.",
        "expertTip": "When retrofitting basic containment, prioritize the areas with the highest heat density or most significant airflow management challenges first. Even simple, low-cost solutions like vinyl curtains can yield noticeable improvements in temperature stability and cooling efficiency if properly installed and maintained. Always consult with fire safety experts before making modifications that could affect sprinkler coverage or smoke detection.",
        "recommendedEvidence": "- Feasibility study or assessment report on retrofitting basic containment in existing data halls.\n- Design sketches or specifications for any retrofitted basic separation solutions (e.g., curtain systems, modular panels).\n- Records of fire safety assessment or approval related to the retrofitted containment.\n- Photographic evidence of the implemented retrofitted basic separation measures.",
        "recommendedActions": "- Survey existing data halls with Hot/Cold Aisle layouts to identify areas where retrofitting basic airflow separation could be beneficial and feasible.\n- Evaluate different basic separation solutions (e.g., curtains, rigid panels) for cost-effectiveness and ease of installation.\n- Before implementing any retrofit, consult with fire safety professionals and local authorities to ensure compliance with all relevant regulations.\n- Implement the chosen solution and monitor its impact on temperature distribution and cooling energy consumption.",
        "sampleResponseTemplate": "In our legacy Data Hall 2, which utilizes a Hot/Cold Aisle layout, we retrofitted [e.g., heavy-duty vinyl curtains at the ends of cold aisles] in [Month, Year] to improve basic air separation. This project ([Project ID LH-Retrofit-01]) included a fire safety review by [Consultant/Authority Name] (Ref: [Report ID]). We observed a [quantifiable improvement, e.g., '2\u00b0C average increase in return air temperature'] post-implementation.",
        "humanInputAnswer": "For our older Data Hall B, which has a Hot/Cold aisle setup, we conducted a retrofit assessment in Q3 2024 (Ref: 'DHB_Containment_Retrofit_Study.pdf'). Based on this, we installed flexible strip curtains at the ends of all cold aisles in Q1 2025. The local fire marshal reviewed and approved the installation method (Approval #FMA-2025-017) ensuring no impact on fire suppression system operation. This has helped reduce cold air bypass.",
        "llmGeneratedAnswer": "Yes, in our existing 'Data Centre North' facility, which employs a Hot/Cold Aisle configuration but predates our full containment standards, we have retrofitted basic cold aisle separation. This was implemented in 2024 using modular, fire-retardant polycarbonate panels at the ends of each cold aisle and flexible sealing strips above the cabinets to reduce bypass airflow. A comprehensive fire system review, including smoke testing, was conducted and approved by our certified fire safety consultant (Ref: 'FireSafety_Retrofit_Cert_DSN_2024') prior to final installation, ensuring no compromise to existing fire detection or suppression capabilities. This measure has improved temperature consistency within the cold aisles.",
        "badExampleAnswer": "Our old data centre has hot and cold aisles. We haven't added any curtains because it might be a fire hazard."
      },
      "question_64": {
        "id": 64,
        "old_question_text": null,
        "questionText": "Has the organization implemented a policy and practice of installing blanking plates in all empty rack U-spaces within IT cabinets to reduce or prevent hot air from re-circulating from the hot aisle, through the gaps in the cabinet, and back into the cold aisle or equipment inlets? Is this practice consistently applied to minimize heated air being ingested by other devices, which can increase their intake temperature and reduce cooling efficiency?",
        "simplifiedQuestion": "Do you install blanking plates in all unused rack spaces in your IT cabinets to prevent hot air recirculation?",
        "reference": "2025 CoC Guide 5.1.4",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_6",
        "recommendationTitle": "Cabinet air flow management - Blanking Plates",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg"
          ],
          "description": "Data centre operational procedures or standards for cabinet setup, specifically requiring the installation of blanking plates. Photographic evidence of cabinets with blanking plates installed in empty U-spaces. Audit checklists or reports that verify blanking plate installation."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, jpg",
        "documentDescriptor": "Mandatory: Policy or procedure requiring blanking plate installation in all empty rack U-spaces. Photographic evidence or audit reports demonstrating consistent implementation.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of data centre standards for cabinet build-outs. Physical inspection of IT cabinets to verify the consistent installation of blanking plates in all unused rack spaces.",
        "nonconformity": {
          "major": "Blanking plates are generally not used, or their use is very inconsistent, leading to significant opportunities for hot air recirculation within cabinets and between aisles.",
          "minor": "Blanking plates are used but not in all empty U-spaces, or some cabinets have missing plates, allowing some hot air recirculation."
        },
        "majorNonconformity": "Systematic absence of blanking plates in empty rack U-spaces across a significant portion of IT cabinets, leading to widespread hot air recirculation.",
        "minorNonconformity": "Inconsistent installation of blanking plates, with many gaps remaining in cabinets, or use of makeshift/ineffective blanking solutions.",
        "whatThisMeans": "Any empty 'U' spaces in your server racks should be covered with blanking plates. This is a simple but very effective way to stop hot exhaust air from servers from circulating back to the front of the rack and being sucked into other equipment. If hot air gets into the cold aisle or equipment intakes, your cooling systems have to work harder and less efficiently.",
        "complianceRequires": "- Implement the installation of blanking plates in all empty U-spaces within IT cabinets.\n- This practice aims to reduce hot air re-circulating through gaps in the cabinet from the hot aisle to the cold aisle.\n- Consistent application helps prevent air heated by one device from being ingested by another, which would increase intake temperatures and reduce overall cooling efficiency.",
        "expertTip": "Make blanking plate installation a mandatory part of your server deployment and decommissioning checklists. Keep a stock of various U-size blanking plates readily available in the data centre. Consider tool-less blanking plates for ease of installation and removal.",
        "recommendedEvidence": "- Data centre operational standard or policy document requiring the installation of blanking plates in all unused rack U-spaces.\n- Photographic evidence showcasing consistent use of blanking plates in IT cabinets across data halls.\n- Checklists for new server installations or cabinet audits that include verification of blanking plate installation.\n- Purchase records for blanking plates.",
        "recommendedActions": "- Procure an adequate supply of blanking plates in various sizes (1U, 2U, etc.).\n- Implement a strict policy requiring the installation of blanking plates in all empty rack U-spaces immediately after equipment is removed or if a space is intentionally left empty.\n- Include checks for blanking plates in regular data centre walkthroughs or audits.\n- Train all data centre staff on the importance and proper installation of blanking plates.",
        "sampleResponseTemplate": "Our 'Data Centre Cabinet Standard' ([Doc ID: DC-CAB-STD-002], Section 3.1) mandates the installation of blanking plates in all unoccupied rack U-spaces. This is verified during our [weekly/monthly] data hall inspections. We maintain a stock of 1U and 2U blanking plates for immediate use. Photos from our last audit on [Date] confirm widespread compliance.",
        "humanInputAnswer": "Yes, our 'Rack & Cabinet Build Standard (RCBS-001)' requires that all unpopulated U-space in active racks must be filled with blanking plates. Our technicians install these as part of any server installation or decommissioning process. We conduct weekly visual inspections of data halls, and missing blanking plates are rectified within 24 hours. Our target is 100% compliance.",
        "llmGeneratedAnswer": "Affirmative. The organization enforces a strict policy, detailed in 'Data Centre Operations Manual (DCOM-2025, Section 5.2.1: Cabinet Airflow Integrity)', requiring the installation of blanking plates in all unused rack unit (U) spaces within IT cabinets. This measure is consistently applied to prevent hot air recirculation from the hot aisle to the cold aisle through cabinet gaps, thereby maintaining intake air temperatures for IT equipment and optimizing cooling system efficiency. Compliance is verified through quarterly data hall audits, with photographic evidence maintained.",
        "badExampleAnswer": "We have some blanking plates, but we don't always use them. Sometimes it's easier to leave spaces open for future servers."
      },
      "question_65": {
        "id": 65,
        "old_question_text": null,
        "questionText": "In addition to blanking plates in empty U-spaces, has the organization implemented measures to seal other air leakage opportunities in each IT cabinet? Does this include addressing floor openings at the base of the cabinet (especially in raised floor environments) and any gaps at the sides, top, and bottom of the cabinet between equipment or mounting rails and the cabinet's perimeter, for instance, by using aperture brushes (draught excluders) or cover plates?",
        "simplifiedQuestion": "Do you seal other air leakage spots in IT cabinets (around cables, sides, top/bottom) using brushes or covers?",
        "reference": "2025 CoC Guide 5.1.5",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_6",
        "recommendationTitle": "Cabinet air flow management - Other openings",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg"
          ],
          "description": "Data centre standards for cabinet sealing and cable management. Specifications for aperture brushes, cover plates, or other sealing materials used. Photographic evidence of sealed openings in cabinets."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, jpg",
        "documentDescriptor": "Mandatory: Documentation or evidence (e.g., photos, installation standards) of measures taken to seal air leakage paths in IT cabinets, such as floor openings, and gaps around/between equipment and cabinet frames, using items like brush grommets or cover plates.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of cabinet installation standards and cable management guidelines. Physical inspection of IT cabinets for sealing of floor openings (especially under-cabinet cable entry points) and other gaps.",
        "nonconformity": {
          "major": "Significant unsealed openings exist in IT cabinets (e.g., large gaps at the base for cable entry in raised floors, substantial side/top/bottom gaps) allowing extensive air leakage.",
          "minor": "Some effort is made to seal openings, but it's inconsistent, materials used are ineffective, or many smaller gaps remain unaddressed."
        },
        "majorNonconformity": "Systematic failure to seal significant air leakage pathways in IT cabinets, particularly large openings at the base of cabinets in raised floor environments or extensive gaps around equipment and cabinet perimeters.",
        "minorNonconformity": "Cabinet sealing efforts are inconsistent or incomplete; for example, some cable openings are sealed with brushes, but many other gaps around rails or between equipment remain open.",
        "whatThisMeans": "Besides covering empty U-spaces in racks with blanking plates, your organization should also seal other holes and gaps in your IT cabinets where air can leak. This includes openings in the floor at the bottom of the cabinet (especially if you have a raised floor for cables) and any gaps around the sides, top, or bottom of the cabinet frame or between your equipment and the mounting rails. Using things like brush strips (like a draught excluder) or special cover plates helps stop air from leaking where it shouldn't.",
        "complianceRequires": "- Implement measures to cover all significant air leakage opportunities in each IT cabinet, beyond just empty U-spaces.\n- Specifically address and seal floor openings at the base of the cabinet, particularly in raised floor environments where cables enter.\n- Seal gaps at the sides, top, and bottom of the cabinet that may exist between installed IT equipment or mounting rails and the perimeter of the cabinet structure.\n- Utilize appropriate sealing mechanisms such as aperture brushes (draught excluders) or custom cover plates.",
        "expertTip": "Pay close attention to under-cabinet cable entry points in raised floor environments, as these are common and significant sources of uncontrolled airflow. Use appropriately sized brush grommets or modular sealing systems that can adapt to varying cable bundle sizes. Regularly inspect these seals, especially after new cable installations or removals.",
        "recommendedEvidence": "- Data centre build or cabinet installation standards that specify requirements for sealing all cabinet openings.\n- Photographs showing the use of aperture brushes, cover plates, or other sealing methods for floor openings at the base of cabinets and other gaps.\n- Specifications or purchase records for cabinet sealing accessories (e.g., brush grommets, blanking panels for non-U space gaps).\n- Thermal imaging reports that indicate good sealing around cabinets with minimal air leakage.",
        "recommendedActions": "- Conduct an audit of existing IT cabinets to identify and inventory all air leakage paths (base openings, side gaps, etc.).\n- Procure and install appropriate sealing solutions (e.g., brush grommets for cable openings, custom panels for larger gaps) for all identified leakage points.\n- Integrate cabinet sealing checks into new equipment installation and cabinet maintenance procedures.\n- Include sealing of these \"other openings\" as part of overall airflow management and containment strategies.",
        "sampleResponseTemplate": "Our 'Cabinet Installation Standard' ([Doc ID: CAB-INST-003]) requires sealing of all cabinet openings. Floor openings at the base of racks are sealed using [Type of brush grommet/seal]. Side and top/bottom gaps between rails and the cabinet perimeter are addressed with [custom foam/cover plates]. This is verified during our standard data hall audits.",
        "humanInputAnswer": "In addition to U-space blanking, our 'Cabinet Sealing Procedure (CSP-001)' mandates the use of brush strip grommets for all cable entry points at the base and rear of cabinets. Any gaps larger than 1/4 inch between the mounting rails and the cabinet sides/top/bottom are sealed with custom-cut fire-retardant foam or blanking material. This is checked as part of our 'New Rack Commissioning Checklist'.",
        "llmGeneratedAnswer": "Yes, our organization implements comprehensive cabinet airflow management beyond just U-space blanking plates. As per 'Data Centre Build Standard (DCBS-2025, Section 4.3: Cabinet Sealing)', all potential air leakage points are addressed. This includes installing high-density brush grommets on all floor openings at the base of cabinets for cable pass-through in our raised floor environments. Additionally, any perceivable gaps at the sides, top, or bottom of cabinets, particularly between mounting rails and the cabinet frame, are sealed using either pre-fabricated magnetic sealing strips or custom-cut fire-retardant blanking panels. This ensures maximum airflow integrity within our contained aisles.",
        "badExampleAnswer": "We put blanking panels in the front of the racks. We don't worry too much about small gaps around the cables at the bottom."
      },
      "question_66": {
        "id": 66,
        "old_question_text": null,
        "questionText": "Has the organization ensured that for IT cabinets requiring air cooling, solid doors (if used) are replaced with perforated doors to ensure adequate cooling airflow, and that such perforated doors have a sufficient free area? Is it understood that ISO/IEC 14763-2 recommends a minimum of at least 66% perforated area, with 80% considered an ideal target by other authorities, to prevent impeded airflow and recirculation within the cabinet which increases equipment inlet temperatures?",
        "simplifiedQuestion": "Do your IT cabinet doors have enough holes (perforation, e.g., 66-80%) to allow proper airflow for cooling?",
        "reference": "2025 CoC Guide 5.1.6",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_6",
        "recommendationTitle": "Provide adequate free area on cabinet doors",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg"
          ],
          "description": "Cabinet procurement specifications detailing required door perforation percentages. Manufacturer datasheets for installed cabinets confirming perforation levels. Photographic evidence of perforated cabinet doors in use."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, jpg",
        "documentDescriptor": "Mandatory: Specifications for IT cabinet doors showing minimum perforation percentage (e.g., 66% or higher). Evidence that cabinets in use meet these specifications.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of cabinet procurement specifications and manufacturer datasheets. Physical inspection of cabinet doors to assess perforation levels and ensure no solid doors are impeding airflow where air cooling is required.",
        "nonconformity": {
          "major": "IT cabinets housing air-cooled equipment predominantly use solid doors or doors with very low perforation (<50%), significantly impeding airflow and causing equipment overheating or excessive fan power use.",
          "minor": "Some cabinets use doors with perforation below recommended levels (e.g., <66%), or perforated doors are partially obstructed, potentially limiting airflow for densely packed equipment."
        },
        "majorNonconformity": "Widespread use of solid doors or doors with significantly insufficient perforation (e.g., less than 50% free area) on cabinets housing air-cooled IT equipment, leading to severely restricted airflow and cooling inefficiency.",
        "minorNonconformity": "Cabinet doors have some perforation but it is consistently below the recommended minimum of 66% free area, or perforated areas are frequently obstructed, potentially impacting airflow to densely populated cabinets.",
        "whatThisMeans": "If your IT equipment needs air to cool it, the doors on your server racks/cabinets must have enough holes (perforations) to let air pass through easily. Solid doors block airflow and cause equipment to overheat or its fans to run much harder. Standards like ISO/IEC 14763-2 suggest at least 66% of the door area should be perforated (open), with 80% being even better. This helps ensure your equipment gets enough cool air.",
        "complianceRequires": "- If solid doors are present on cabinets requiring airflow for IT equipment cooling, replace them with adequately perforated doors.\n- Ensure that new and existing cabinets used for air-cooled IT equipment have perforated doors with sufficient free area to allow adequate cooling airflow.\n- Aim for door perforation levels that meet or exceed industry recommendations, such as ISO/IEC 14763-2's minimum of at least 66% perforated area, with 80% being an ideal target.\n- Understand that solid doors impede cooling airflow, promote recirculation within the cabinet, and increase equipment inlet temperatures, thereby reducing efficiency.",
        "expertTip": "When procuring new cabinets, specify a high perforation percentage (e.g., 70-80%) for both front and rear doors as a standard requirement. For existing cabinets with poor perforation, evaluate the cost-benefit of replacing doors versus replacing entire cabinets, especially if airflow issues are contributing to higher cooling energy costs.",
        "recommendedEvidence": "- IT cabinet procurement specifications that mandate minimum door perforation percentages (e.g., \u226566%).\n- Manufacturer datasheets for installed IT cabinets, verifying the perforation percentage of front and rear doors.\n- Photographic evidence of IT cabinets in data halls clearly showing perforated doors.\n- Any internal standards or guidelines related to cabinet selection that address airflow and door perforation.",
        "recommendedActions": "- Audit all existing IT cabinets to identify any with solid doors or doors with insufficient perforation that house air-cooled equipment.\n- Develop a plan to replace inadequate doors with highly perforated alternatives (meeting or exceeding 66% free area).\n- Ensure all new cabinet procurements specify a high door perforation percentage as a mandatory requirement.\n- Regularly inspect cabinet doors for any obstructions (e.g., items placed against them) that could impede airflow.",
        "sampleResponseTemplate": "Our standard for IT cabinets ([Cabinet Spec Doc ID: CAB-SPEC-001]) requires both front and rear doors to have a minimum of [e.g., 75%] perforated free area, aligning with industry best practices (referencing ISO/IEC 14763-2). All cabinets currently deployed in our data halls, such as [Cabinet Manufacturer/Model], meet or exceed this specification. We do not use solid doors on cabinets housing air-cooled IT equipment.",
        "humanInputAnswer": "Our data centre standard (DC-STD-RACK-003) mandates that all IT equipment cabinets must have front and rear doors with at least 70% perforation. The 'VendorXYZ' cabinets we predominantly use are certified by the manufacturer to provide 78% open area. We do not permit the use of solid doors on any racks containing actively cooled IT equipment. This is verified during new rack installation.",
        "llmGeneratedAnswer": "Yes, our organization's 'Data Centre Cabinet Standard (DCCS-2025, Section 3.2)' mandates that all IT cabinets housing air-cooled equipment must be equipped with perforated front and rear doors providing a minimum of 70% open area for adequate airflow. This requirement exceeds the ISO/IEC 14763-2 recommendation of 66%. For example, all cabinets in our 'Alpha Data Hall' (e.g., 'RackModel SuperAir 42U') have manufacturer-certified 75% perforated doors. Solid doors are only permissible for specific applications like network distribution racks with passive equipment or fully sealed cabinet systems with dedicated cooling, and these are exceptions documented and approved separately.",
        "badExampleAnswer": "Most of our racks have doors with some holes in them. Some older ones might be solid, but the equipment seems okay."
      },
      "question_67": {
        "id": 67,
        "old_question_text": null,
        "questionText": "Has the organization implemented comprehensive raised floor airflow management by closing all unwanted apertures in the raised floor, reviewing and optimizing the placement and opening factors of vented tiles to reduce bypass airflow, and maintaining unbroken rows of cabinets (or filling gaps with empty, fully blanked cabinets) to prevent re-circulated air, especially in hot/cold aisle environments where any opening between aisles can degrade air separation?",
        "simplifiedQuestion": "If you use a raised floor for cooling, do you seal unwanted holes, optimize vent tile placement, and keep cabinet rows complete to manage airflow effectively?",
        "reference": "2025 CoC Guide 5.1.7",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_6",
        "recommendationTitle": "Raised floor air flow management",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "dwg"
          ],
          "description": "Procedures for raised floor airflow management (sealing openings, tile management). Data centre layout plans showing vented tile placement. Audit reports or checklists verifying raised floor sealing and tile optimization. Policies on maintaining unbroken cabinet rows."
        },
        "docRequired": "Yes, if raised floor used for air distribution",
        "acceptedFiles": "pdf, docx, jpg, dwg",
        "documentDescriptor": "Mandatory (if raised floor is used for cooling): Documentation of raised floor airflow management practices, including sealing of unwanted openings, vented tile placement strategy, and ensuring unbroken cabinet rows. Evidence of regular checks and maintenance.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of data centre operational procedures for raised floor management and vented tile strategy. Physical inspection of the raised floor for unsealed openings, placement of vented tiles, and integrity of cabinet rows.",
        "nonconformity": {
          "major": "Significant unsealed openings in the raised floor, haphazard placement of vented tiles, or many gaps in cabinet rows, leading to substantial bypass airflow and poor cooling distribution.",
          "minor": "Some unsealed openings exist, vented tile placement is not fully optimized, or minor gaps in cabinet rows are present, leading to some level of bypass or recirculation."
        },
        "majorNonconformity": "Widespread unsealed openings in the raised floor, incorrect or unmanaged placement of vented floor tiles, and/or frequent gaps in cabinet rows, leading to significant bypass or recirculation of conditioned air.",
        "minorNonconformity": "Inconsistent sealing of raised floor openings, some misplacement of vented tiles, or occasional gaps in cabinet rows that minimally impact airflow management but indicate a lack of rigorous control.",
        "whatThisMeans": "If your data centre uses a raised floor to deliver cold air, you need to manage it carefully. This means sealing any unnecessary holes in the floor panels. You also need to strategically place your vented tiles (the ones with holes that let cold air out) to deliver air where it's needed and reduce air that bypasses the IT equipment. Keeping rows of server racks complete (or filling gaps with blanked-off empty racks) is also important to prevent hot and cold air from mixing where it shouldn't, especially if you have a hot/cold aisle setup.",
        "complianceRequires": "- Close all unwanted apertures (holes, gaps) in the raised floor.\n- Regularly review and optimize the placement and opening factors (e.g., % open area, damper settings) of vented floor tiles to reduce bypass airflow (air that doesn't go through IT equipment).\n- Maintain unbroken rows of IT cabinets to prevent re-circulated air from hot aisles mixing with cold air in cold aisles.\n- If cabinet rows have gaps, fill these with empty, fully blanked cabinets or equivalent barriers.\n- Understand that managing unbroken rows is especially important in hot and cold aisle environments, as any opening between aisles will degrade the separation of hot and cold air.",
        "expertTip": "Use tools like pressure sensors or even simple smoke pencils to identify areas of bypass airflow or unintended air leakage from the raised floor. Regularly audit vented tile placement \u2013 they often get moved during maintenance or equipment changes and not returned to their optimal positions. Consider using variable-air-volume (VAV) diffusers or intelligently controlled vented tiles that can adjust airflow based on demand.",
        "recommendedEvidence": "- Standard operating procedures for managing raised floor integrity (sealing openings, managing cable cutouts).\n- Data centre layout diagrams showing planned placement of vented tiles and actual configurations.\n- Audit reports or logs from regular inspections of the raised floor for unsealed openings and correct tile placement.\n- Policy or guideline on maintaining full cabinet rows or using blanked cabinets to fill gaps.\n- Photographic evidence of well-sealed raised floor and appropriately placed vented tiles.",
        "recommendedActions": "- Conduct a thorough survey of the raised floor to identify and seal all unnecessary openings using appropriate grommets, brushes, or blank panels.\n- Develop a vented tile strategy based on IT equipment cooling requirements and airflow patterns, and ensure tiles are correctly placed and managed.\n- Implement procedures to ensure gaps in cabinet rows are filled promptly with blanked cabinets or suitable barriers.\n- Regularly inspect and maintain the raised floor plenum, including sealing cable cutouts and other penetrations properly.",
        "sampleResponseTemplate": "Our 'Raised Floor Management Protocol' ([Doc ID: RFMP-001]) mandates that all unused openings are sealed with [e.g., fire-rated grommets]. Vented tile placement is optimized based on [CFD analysis/equipment load] and reviewed [Frequency]. Gaps in cabinet rows are filled with blanked cabinets. Last audit on [Date] confirmed [X]% compliance with sealing requirements.",
        "humanInputAnswer": "Our data centre utilizes a raised floor for cold air distribution. As per 'Raised Floor Airflow Procedure RFAP-002', all cable cutouts are sealed with dual-brush grommets. We conduct quarterly inspections to identify and seal any other unwanted openings. Vented tile placement (25% open area tiles) is aligned with IT equipment locations, and any gaps in cabinet rows are immediately filled with blanking panels or placeholder cabinets. This ensures minimal bypass airflow.",
        "llmGeneratedAnswer": "Yes, our organization implements rigorous raised floor airflow management. Our 'Data Centre Operations Standard (DCOS-5.1.7)' mandates:\n1.  All unintended apertures in the raised floor are sealed using fire-rated grommets and blanking panels.\n2.  Vented tile placement is strategically determined based on CFD modeling and IT equipment load, with regular audits (quarterly) to ensure optimal positioning and prevent bypass.\n3.  Unbroken rows of cabinets are maintained. Any planned gaps are filled with fully blanked placeholder cabinets to preserve aisle integrity, especially critical in our hot/cold aisle contained environments. Our latest audit in Q1 2025 (Ref: 'RF_Audit_Q12025.pdf') confirmed 98% compliance with these standards.",
        "badExampleAnswer": "We have a raised floor and some vented tiles are placed in front of the server racks. There are some holes for cables."
      },
      "question_68": {
        "id": 68,
        "old_question_text": null,
        "questionText": "Has the organization reviewed the placement and level of obstruction created by cabling, cable trays, and other structures in under-floor airflow paths (if a raised floor is used for air distribution)? Are measures taken to ensure that the under-floor area is as free of obstructions as possible to minimize airflow resistance, turbulence, and potential negative pressure zones, with consideration given to using overhead cabling trays to substantially reduce under-floor obstruction?",
        "simplifiedQuestion": "If you use a raised floor, do you ensure under-floor cables and structures don't block airflow, possibly by using overhead cable trays instead?",
        "reference": "2025 CoC Guide 5.1.8",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_6",
        "recommendationTitle": "Raised floor air flow management - Obstructions",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "dwg"
          ],
          "description": "Cable management plan for under-floor areas. Design guidelines for under-floor infrastructure. Photographic evidence of under-floor cable management. If overhead trays are used, design and photos."
        },
        "docRequired": "Yes, if raised floor used for air distribution",
        "acceptedFiles": "pdf, docx, jpg, dwg",
        "documentDescriptor": "Mandatory (if raised floor is used for cooling): Documentation of under-floor cable management practices aimed at minimizing airflow obstruction. Evidence (e.g., photos, design standards) of organized cabling, minimal obstructions, or use of overhead cable trays.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of cable management standards and under-floor design guidelines. Physical inspection of the under-floor plenum (where accessible and safe) to assess the level of obstruction from cabling and other infrastructure.",
        "nonconformity": {
          "major": "Under-floor plenum is severely obstructed by disorganized cabling, cable trays, or other structures, significantly impeding airflow and increasing fan energy requirements.",
          "minor": "Some under-floor obstructions exist that create localized airflow issues or moderate increases in airflow resistance, or overhead cabling is not considered where beneficial."
        },
        "majorNonconformity": "Significant and widespread obstructions (e.g., dense, disorganized cabling, improperly routed pipework) in the under-floor plenum, severely restricting airflow, creating high static pressure, and leading to inefficient cooling distribution.",
        "minorNonconformity": "Some under-floor obstructions exist that cause localized airflow issues or increased resistance, or the benefits of overhead cabling have not been fully leveraged where under-floor congestion is a known issue.",
        "whatThisMeans": "If your data centre uses a raised floor to distribute cold air, it's important that the space under the floor isn't cluttered with messy cables, cable trays, or other things that can block the airflow. Blockages increase the effort your cooling fans need to move air, wasting energy, and can create areas with poor cooling. Using overhead cable trays instead of under-floor ones can greatly help reduce these obstructions.",
        "complianceRequires": "- Regularly review the placement and level of obstruction created by cabling, cable trays, and other structures within under-floor airflow paths (if a raised floor is used for air distribution).\n- Implement measures to ensure that the under-floor area is kept as free of obstructions as possible.\n- This aims to reduce airflow resistance, minimize turbulence, and prevent the creation of negative pressure zones that can draw hot air into the cold plenum.\n- Consider the use of overhead cabling trays as a strategy to substantially reduce the level of under-floor obstruction.",
        "expertTip": "When performing work under raised floors, enforce strict cable management discipline. Use appropriate cable tray systems, maintain separation between power and data cables, and remove any abandoned cables immediately. Periodically inspect the under-floor plenum with airflow measurement tools (anemometers) or smoke tests to identify and remediate obstructions.",
        "recommendedEvidence": "- Cable management standards for under-floor installations.\n- Data centre design guidelines specifying maximum allowable obstruction levels or preferred routing for under-floor services.\n- Photographic evidence of well-managed under-floor cabling or the use of overhead cable tray systems.\n- Reports from airflow assessments or thermal audits that include evaluation of under-floor obstructions.",
        "recommendedActions": "- Establish and enforce clear cable management standards for all under-floor installations, including removal of abandoned cables.\n- During new builds or major retrofits, prioritize overhead cable management solutions to keep the under-floor plenum clear for airflow.\n- For existing facilities with under-floor cabling, conduct periodic audits to identify and remediate significant obstructions.\n- Ensure any new under-floor installations (e.g., piping, conduits) are routed to minimize airflow impact.",
        "sampleResponseTemplate": "Our 'Under-Floor Cable Management Standard' ([Doc ID: UFCM-001]) specifies [e.g., maximum fill ratios for cable trays, mandatory use of specific tray types] to minimize airflow obstruction. We primarily use [overhead cable trays] for new data and power distribution to keep the plenum clear. Regular inspections are conducted [Frequency] to ensure compliance. The last inspection on [Date] found [brief finding].",
        "humanInputAnswer": "Our standard practice for new data hall builds is to use overhead ladder racks for all data cabling and power distribution to cabinets, keeping the under-floor plenum clear for optimal airflow. For our legacy 'Site A' which has under-floor cabling, we conducted a cable mining project in 2023, removing over 2,000 lbs of abandoned cables. All active under-floor cabling is now managed in dedicated, elevated trays, ensuring at least 12 inches of clear space below for airflow. This is documented in 'Site A Underfloor Remediation Report 2023'.",
        "llmGeneratedAnswer": "Yes, the organization prioritizes unobstructed under-floor airflow paths. Our 'Data Centre Cabling Standard (DCCS-2025, Section 6)' mandates the use of overhead cable trays for all network and power cabling to IT cabinets in new buildouts and major retrofits. For existing raised floor installations with under-floor cabling, a biannual 'Under-Floor Obstruction Audit' is conducted. The latest audit (Ref: 'UFA_Audit_H2_2024') identified and scheduled the removal of 150m of abandoned cabling, and rerouting of one section of chilled water piping to reduce airflow restriction by approximately 20% in that zone. The aim is to keep the under-floor plenum as clear as possible to minimize airflow resistance and turbulence.",
        "badExampleAnswer": "We run cables under the floor because it's convenient. It can get a bit crowded down there sometimes."
      },
      "question_69": {
        "id": 69,
        "old_question_text": null,
        "questionText": "Has the organization considered the use of return plenum(s) in its data centre design to return heated air from the IT equipment directly to the air conditioning units, potentially improving airflow management and cooling efficiency?",
        "simplifiedQuestion": "Have you considered using a dedicated return air path (plenum) to guide hot air directly back to your AC units?",
        "reference": "2025 CoC Guide 5.1.9",
        "priority": "Low (Value: 3, Expected: Optional Practice)",
        "stepSection": "step_6",
        "recommendationTitle": "Design - Return plenums",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "dwg",
            "docx"
          ],
          "description": "Data centre design documents showing the use of return plenums (e.g., ducted return, ceiling plenum). Feasibility studies or evaluations considering return plenums. Performance data if return plenums are in use."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, dwg, docx",
        "documentDescriptor": "Optional: Data centre design documents illustrating the use of dedicated return air plenums (e.g., ceiling void used as a hot air return, ducted returns from hot aisles or cabinet chimneys).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of data centre HVAC design drawings and specifications. Physical inspection to observe the return air path and any dedicated plenum construction.",
        "nonconformity": {
          "major": null,
          "minor": "Return air management is inefficient, relying on open room return without consideration of dedicated plenums where this could improve efficiency and airflow control (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: In data centre designs where a dedicated return plenum could significantly improve airflow control and efficiency (e.g., with hot aisle containment or cabinet chimneys), this option was not considered or was dismissed without adequate justification.",
        "whatThisMeans": "When designing your data centre's cooling, your organization should think about creating a specific, enclosed path (a plenum) for the hot air coming out of your IT equipment to travel directly back to the air conditioning units. This can be a space above a false ceiling, or actual ducts. Using a return plenum can help manage airflow better and make your cooling more efficient by preventing hot air from mixing with cold air before it gets back to the ACs.",
        "complianceRequires": "- Consider the use of a return plenum or plenums in the data centre design.\n- The purpose of such a plenum is to return heated air from the IT equipment directly to the air conditioning units (CRACs/CRAHs).\n- This approach can improve airflow management and potentially enhance cooling efficiency by ensuring a more controlled and direct path for hot exhaust air, reducing mixing with conditioned cold air.",
        "expertTip": "When using a ceiling void as a return air plenum, ensure it is well-sealed to prevent air leakage into or out of adjacent spaces. Also, keep the plenum clear of obstructions (like unrelated pipework or cabling) that could impede airflow and increase static pressure for the return fans in the cooling units.",
        "recommendedEvidence": "- Data centre HVAC design documents or drawings showing the design and construction of a return air plenum (e.g., ducted return from hot aisles, use of ceiling void as a plenum with sealed penetrations).\n- Airflow diagrams illustrating the intended path of hot return air via the plenum to the cooling units.\n- If not implemented, documentation showing that return plenums were considered during the design phase, with justification if not adopted.",
        "recommendedActions": "- During the data centre design or major retrofit planning, evaluate the benefits of incorporating a dedicated return air plenum (e.g., ceiling plenum, ducted returns).\n- If implemented, ensure the return plenum is appropriately sized, sealed, and free from obstructions to facilitate efficient airflow.\n- Integrate the return plenum design with hot aisle containment or cabinet chimney systems for maximum effectiveness.",
        "sampleResponseTemplate": "Our data centre [Facility Name/Data Hall X] utilizes a [ducted return system from hot aisles / sealed ceiling void as a return plenum] to direct hot exhaust air from IT equipment back to the CRAC units. This design, detailed in [HVAC Design Doc ID], helps maintain consistent air pressure differentials and improves cooling efficiency by [specific benefit, e.g., 'increasing return air temperature to the cooling units'].",
        "humanInputAnswer": "Our 'Data Centre Design Standard DC-STD-HVAC-005' specifies the use of a dedicated hot air return plenum, typically utilizing the ceiling void above our hot aisle containment systems. This ensures that exhaust air is directly channeled back to the CRAH units, maximizing return air temperature and preventing re-circulation. This is standard in all our builds since 2022.",
        "llmGeneratedAnswer": "Yes, our current data centre design standard (Ref: 'DC_Design_Guidelines_Rev5_2024', Section 4.3.2) mandates the use of a dedicated return plenum for managing heated air. In our facilities, this is typically achieved by using the ceiling void above the hot aisles as a sealed plenum, with IT equipment exhaust directed into this space either from contained hot aisles or via cabinet chimneys. This ensures that hot air is directly and efficiently returned to the Computer Room Air Handler (CRAH) units, minimizing mixing with the cold supply air and thereby improving the overall thermal efficiency of the cooling system.",
        "badExampleAnswer": "The hot air just goes up to the ceiling and eventually finds its way back to the AC units in the room."
      },
      "question_70": {
        "id": 70,
        "old_question_text": null,
        "questionText": "When designing new data centres or undertaking major retrofits where raised floors or suspended ceilings are used as air plenums (for supplying cold air or returning hot air), has the organization considered increasing the height/depth of these voids? Is it understood that larger plenum depths can reduce fan energy losses associated with moving air by lowering air velocity and static pressure requirements?",
        "simplifiedQuestion": "If using raised floors/suspended ceilings for airflow, did you consider making them deeper/higher to improve fan efficiency?",
        "reference": "2025 CoC Guide 5.1.10",
        "priority": "Low (Value: 3, Expected: Optional Practice)",
        "stepSection": "step_6",
        "recommendationTitle": "Design - Raised floor or suspended ceiling height",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "dwg",
            "docx"
          ],
          "description": "Data centre design documents (for new builds/retrofits) showing specifications for raised floor or suspended ceiling plenum heights. Any analysis or justification for the chosen plenum depths, especially if increased height was considered or implemented for airflow efficiency."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, dwg, docx",
        "documentDescriptor": "Optional: Design documentation for new builds/major retrofits specifying raised floor/suspended ceiling plenum depths. Evidence that increased plenum height was considered or implemented to reduce fan energy losses.",
        "verification_method": [
          "Document Review",
          "Site Inspection (if new build/retrofit)"
        ],
        "verificationMethods": "Review of design documents for new builds or major retrofits that utilize raised floors or suspended ceilings as air plenums, specifically looking for plenum height specifications and any design considerations for airflow efficiency. Physical measurement during site inspection if applicable and accessible.",
        "nonconformity": {
          "major": null,
          "minor": "For new designs using plenums, the opportunity to optimize plenum height for reduced fan energy was not considered, or unnecessarily shallow plenums were implemented leading to higher air velocities and pressure drops (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued for new builds/retrofits using air plenums: Failure to consider or optimize raised floor or suspended ceiling void heights, potentially leading to increased fan energy due to higher air velocities and static pressure, where greater plenum depth was feasible.",
        "whatThisMeans": "If your data centre design uses the space under a raised floor or above a suspended ceiling to move air for cooling, making these spaces deeper (higher) can help save energy. Deeper plenums mean air can move more slowly and with less resistance, so your cooling fans don't have to work as hard.",
        "complianceRequires": "- When using voids in raised floors or suspended ceilings as air plenums (for cold air supply or hot air return), consider increasing the size (height/depth) of these spaces during data centre design or major retrofits.\n- Understand that larger plenum depths can reduce the fan energy losses associated with moving air through these plenums by lowering air velocities and static pressure requirements.",
        "expertTip": "During the early design phase of a new data centre or significant retrofit, model the impact of different plenum depths on fan energy consumption versus the construction cost implications. Optimal plenum height can vary based on data hall size, cooling distribution method, and IT load density.",
        "recommendedEvidence": "- Design drawings or specifications for new data centres or major retrofits showing the specified height of raised floor or suspended ceiling plenums.\n- Engineering calculations or CFD analysis (if performed) that evaluated the impact of different plenum heights on airflow and fan energy.\n- Justification for the chosen plenum height, particularly if it was increased to improve airflow efficiency.",
        "recommendedActions": "- In the design phase of new data centres or large-scale retrofits that will use under-floor or above-ceiling air plenums, evaluate the impact of plenum height on fan power requirements.\n- Where structurally and economically feasible, opt for greater plenum depths to reduce air velocity and static pressure.\n- Ensure that any plenums are kept clear of unnecessary obstructions (as per Practice 5.1.8) to maximize the benefit of their designed height.",
        "sampleResponseTemplate": "For our new [Data Centre Wing Name] design, the raised floor plenum height was specified at [e.g., 900mm], an increase from our older standard of [e.g., 600mm]. This decision, documented in '[Design Review Minutes ID: DR2024-05]', was based on an analysis showing potential fan energy savings of [X]%. We considered this as part of optimizing airflow for Practice 5.1.10.",
        "humanInputAnswer": "In our 'Data Centre Design Standard Revision 2024 (DCDS-R2024)', the minimum raised floor plenum depth for new builds was increased from 600mm to 800mm. This change was based on CFD modeling (Ref: 'CFD Study Plenum Depth v1.2') which indicated a potential 12% reduction in CRAH fan energy due to lower static pressure. Our 'North Wing' expansion, currently under construction, implements this 800mm standard.",
        "llmGeneratedAnswer": "Yes, during the design of our 'Facility B Expansion' (completed Q4 2024), which utilizes a raised floor for cold air distribution, the plenum height was a specific design consideration. The final specified height is 750mm, increased from our previous standard of 600mm. This decision, as documented in 'Facility B Design Basis - HVAC Addendum (FB-HVAC-DB-A01)', was made following an airflow modeling study that projected reduced fan energy consumption by approximately 8-10% due to lower air velocities and static pressure within the deeper plenum. This aligns with the intent of Practice 5.1.10.",
        "badExampleAnswer": "We just use a standard height raised floor, whatever the architect specified. We haven't really thought about making it deeper."
      }
    }
  },
  "step_7": {
    "description": "2025 Guide Section 5: Cooling (Practices 5.1.11-5.1.12 from Sec 5.1; and 5.2.1-5.2.8 from Sec 5.2 - Air Flow Management & Cooling Management)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Cooling_2"
    ],
    "critical_questions": [],
    "questions": {
      "question_71": {
        "id": 71,
        "old_question_text": null,
        "questionText": "Has the organization implemented a strategy to deploy groups of IT, mechanical, or electrical equipment with substantially different environmental requirements (e.g., narrower temperature/humidity tolerances) or different equipment airflow directions into separate, dedicated areas? Where such equipment has different environmental needs, is it preferable and practiced to provide separate environmental controls for these segregated zones to avoid compromising the efficiency of the entire data centre by catering to the most restrictive equipment?",
        "simplifiedQuestion": "Do you group equipment with different cooling/airflow needs into separate areas with their own dedicated environmental controls?",
        "reference": "2025 CoC Guide 5.1.11",
        "priority": "Medium (Value: 3, Expected: New IT Equipment and New build or retrofit)",
        "stepSection": "step_7",
        "recommendationTitle": "Equipment segregation",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Data centre zoning plans showing segregated areas for equipment with special environmental or airflow needs. Specifications for separate environmental controls for these zones. Inventory of equipment with special requirements and their location."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory (if deploying equipment with varied environmental/airflow needs): Design documents or operational plans showing segregation of IT, M&E equipment with different environmental requirements or airflow patterns into dedicated zones. Evidence of separate environmental controls for these zones.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of data centre layout plans, HVAC zoning diagrams, and IT/M&E equipment inventory with environmental specifications. Physical inspection of segregated zones and their dedicated control systems.",
        "nonconformity": {
          "major": "Equipment with significantly different and more restrictive environmental needs (or conflicting airflow) is co-located with standard IT equipment, forcing the entire data hall to operate inefficiently to meet the needs of the most sensitive items.",
          "minor": "Segregation is attempted but is not fully effective, or separate environmental controls for segregated zones are inadequate or poorly optimized."
        },
        "majorNonconformity": "Failure to segregate groups of IT, mechanical, or electrical equipment with substantially different environmental requirements or airflow directions, resulting in the entire data centre's cooling being constrained by the most demanding (and potentially smallest group of) equipment.",
        "minorNonconformity": "Equipment segregation is practiced, but the separate environmental controls for these zones are not fully independent or optimized, or some equipment with differing needs is still mixed, partially compromising overall efficiency.",
        "whatThisMeans": "If you have different types of equipment (IT, power, or cooling gear) that need very different temperature or humidity levels, or have different airflow patterns (e.g., some blow air side-to-side), you should put them in separate, dedicated areas. Each of these areas should ideally have its own environmental controls. This stops a few sensitive pieces of equipment from forcing your whole data centre to run at less efficient conditions just to cater to them.",
        "complianceRequires": "- Deploy groups of IT, mechanical, and electrical equipment with substantially different environmental requirements (e.g., tighter temperature/humidity tolerances) into separate, dedicated areas.\n- Similarly, deploy equipment with substantially different airflow directions into separate areas.\n- Where equipment in segregated zones has different environmental needs, provide separate environmental controls for these zones.\n- The objective is to prevent the data centre's overall cooling plant settings from being constrained by the equipment with the most restrictive environmental range or problematic airflow, which would compromise the efficiency of the entire data centre.",
        "expertTip": "During the data centre design phase, identify any known equipment with special environmental needs (e.g., legacy systems, specific storage types like tape libraries, UPS batteries) or non-standard airflow. Plan dedicated zones or rooms for this equipment with independent environmental controls from the outset. For colocation, offer these as specialized zones.",
        "recommendedEvidence": "- Data centre floor plans or design documents showing clearly demarcated zones for equipment with different environmental or airflow requirements.\n- Specifications for the separate environmental control systems (e.g., dedicated CRACs, different setpoints) for these segregated zones.\n- An inventory list of equipment with special requirements and their designated locations within these segregated areas.\n- Policy or standard for equipment placement that mandates segregation based on environmental/airflow needs.",
        "recommendedActions": "- Audit existing IT, M&E equipment to identify items with substantially different environmental needs or airflow patterns.\n- For new equipment purchases, assess environmental and airflow characteristics and plan for segregation if they differ significantly from data hall standards.\n- In new builds or major retrofits, design dedicated zones with independent environmental controls for such equipment.\n- For existing facilities, explore options for creating segregated areas, even if through simpler means like partitioning and localized cooling adjustments where feasible.",
        "sampleResponseTemplate": "Our 'Data Centre Zoning Standard' ([Doc ID: DC-ZONE-001]) mandates segregation of equipment with unique environmental needs. For example, our [Tape Library System] which requires a cooler, drier environment, is housed in a dedicated room ([Room ID]) with its own environmental controls, separate from our main data halls which operate at ASHRAE A2 recommended ranges. Similarly, network switches with side-airflow are grouped in specific racks with airflow management kits.",
        "humanInputAnswer": "Our legacy 'System Nightingale' requires a stricter temperature range (18-22\u00b0C) than our standard A2 data halls. As per our 'Special Equipment Hosting Policy SEHP-001', Nightingale is deployed in a dedicated, partitioned 'Cold Zone' within Data Hall 3, served by two downsized, independently controlled CRAC units. This prevents the entire hall's temperature from being dictated by this single legacy system. This zone is clearly marked on our DC floor plan (Rev 4.2).",
        "llmGeneratedAnswer": "Yes, our organization practices equipment segregation. As detailed in 'Data Centre Design Standard (DCDS-2025, Section 5.2)', equipment with environmental requirements significantly different from our standard ASHRAE A2 target range (e.g., certain legacy storage arrays requiring lower temperatures or specific humidity controls) are deployed in 'Special Environmental Zones' (SEZs). These SEZs are physically distinct areas, often modular pods, equipped with independent environmental controls (dedicated cooling units, humidifiers/dehumidifiers as needed) to meet their specific needs without impacting the efficiency of the broader data hall. For instance, our tape library archive is in an SEZ maintained at 20\u00b0C and 45% RH, while the main data hall operates at 25\u00b0C.",
        "badExampleAnswer": "All our equipment is in the same room. We just try to make sure the room temperature is okay for everything."
      },
      "question_72": {
        "id": 72,
        "old_question_text": null,
        "questionText": "Has the organization investigated and implemented strategies for operating cooling unit fans to ensure a slight oversupply of air compared to IT equipment flow demand, particularly in contained systems? Is this done to minimize recirculation whilst simultaneously avoiding excessive oversupply of air volume (which results in bypass and fan energy wastage)? Specifically, in contained air systems, is care taken to ensure that there is a slightly positive pressure (preferably no more than 5 Pascals) in the cold air stream with respect to the hot air stream?",
        "simplifiedQuestion": "Do you fine-tune cooling fan operation to supply just slightly more air than IT gear needs (especially in contained aisles) to prevent recirculation, while avoiding wasteful oversupply and maintaining a slight positive pressure in cold areas (max 5 Pa)?",
        "reference": "2025 CoC Guide 5.1.12",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_7",
        "recommendationTitle": "Control of supplied air flow volume minimizing over pressure",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "csv"
          ],
          "description": "Cooling system control philosophy or sequence of operations detailing airflow volume control strategies. Pressure sensor data from contained systems. Reports from commissioning or optimization activities related to airflow balancing and pressure control."
        },
        "docRequired": "Yes, if contained systems used",
        "acceptedFiles": "pdf, docx, csv",
        "documentDescriptor": "Mandatory for new builds/retrofits with contained systems: Control strategy documentation for cooling unit fans demonstrating management of supplied air volume to achieve slight oversupply/positive pressure (max 5 Pa in cold stream relative to hot) while minimizing bypass. Pressure differential monitoring data.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "Site Inspection"
        ],
        "verificationMethods": "Review of cooling control strategies and BMS configurations. Analysis of pressure sensor data from contained aisles. Observation of airflow dynamics and fan speed controls during site inspection.",
        "nonconformity": {
          "major": "In contained systems, airflow volume is grossly mismatched with IT demand, leading to significant recirculation due to undersupply, or excessive bypass and fan energy waste due to extreme oversupply/overpressure.",
          "minor": "Airflow volume control is not precisely managed, leading to some recirculation or moderate bypass; positive pressure in cold contained aisles significantly exceeds 5 Pa or is not consistently maintained."
        },
        "majorNonconformity": "In contained systems (new build/retrofit), no effective control of supplied air volume relative to IT equipment demand, leading to either significant hot air recirculation (due to undersupply) or excessive bypass and fan energy wastage (due to gross oversupply/overpressure).",
        "minorNonconformity": "Airflow volume is managed, but not optimally, leading to some bypass or minor recirculation. Positive pressure in cold aisles is not consistently maintained within the target of ~5Pa (e.g., significantly higher, causing leakage, or often negative, causing hot air ingress).",
        "whatThisMeans": "Your data centre's cooling fans should be controlled to supply just a little more cold air than your IT equipment actually needs, especially if you use containment (like enclosed aisles). This small amount of extra air (oversupply) helps stop hot air from sneaking back into the cold areas (recirculation). However, too much oversupply is wasteful, as it just bypasses the IT gear and wastes fan energy. If you have contained cold aisles, they should have a slightly positive air pressure compared to the hot aisles \u2013 ideally no more than 5 Pascals (a very gentle pressure) \u2013 to help keep hot air out.",
        "complianceRequires": "- Investigate and operate cooling unit fans to ensure a slight oversupply of air compared to the IT equipment's actual airflow demand.\n- This is particularly applicable to contained systems (e.g., cold aisle or hot aisle containment).\n- The goal is to minimize hot air recirculation while avoiding excessive oversupply of air volume (which results in bypass airflow and wasted fan energy).\n- In contained air systems (e.g., cold aisle containment), ensure that there is a slightly positive pressure in the cold air stream with respect to the hot air stream.\n- This positive pressure should preferably be no more than 5 Pascals (Pa) to prevent excessive leakage through minor gaps in containment.",
        "expertTip": "Use differential pressure sensors to monitor and control the pressure difference between contained cold aisles and the surrounding ambient or hot aisles. Integrate these sensors with your cooling unit fan speed controls (VFDs) to dynamically maintain the target positive pressure (e.g., 2-5 Pa) as IT airflow demand changes.",
        "recommendedEvidence": "- Control logic or sequence of operations for cooling unit fans, detailing how airflow volume is matched to IT demand with a slight oversupply.\n- For contained systems: documentation of the target differential pressure for cold aisles and records from pressure sensors showing actual maintained pressures.\n- Commissioning reports or balancing reports for the airflow systems.\n- Data showing correlation between IT load/airflow demand and cooling unit fan speeds/airflow delivery.",
        "recommendedActions": "- Implement variable speed fan control on cooling units.\n- For contained systems, install differential pressure sensors between cold aisles and adjacent spaces.\n- Develop control algorithms that modulate fan speed to maintain a slight positive pressure (target ~2-5 Pa) in contained cold aisles or to ensure slight oversupply in other contained configurations.\n- Regularly monitor IT equipment airflow demand (e.g., via server fan speeds or \u2206T) and adjust cooling unit airflow delivery accordingly to minimize recirculation and bypass.\n- Periodically verify pressure differentials and inspect containment for leaks.",
        "sampleResponseTemplate": "Our cooling units in the contained cold aisles (CCA) are controlled to maintain a slight positive pressure of [e.g., 3 \u00b11 Pa] relative to the hot aisles, as monitored by [Sensor Type/System Name] and documented in our '[BMS Control Strategy ID: CCA-PressureCtrl]'. This minimizes recirculation while preventing excessive bypass. Fan speeds are modulated by VFDs based on this pressure differential and overall cooling demand.",
        "humanInputAnswer": "For our cold aisle containment pods, we use differential pressure sensors (Setra Model 267) at the top and bottom of each aisle, referenced to the adjacent hot aisle. Our BMS (Schneider EcoStruxure) controls CRAH fan VFDs to maintain a target positive pressure of 2.5 Pa (\u00b11 Pa) within the cold aisle. This ensures sufficient airflow to IT equipment while minimizing bypass through containment leaks. This is documented in 'Airflow Control SOP AC-SOP-005'.",
        "llmGeneratedAnswer": "Yes, our organization implements controlled supplied air flow volume. For our new build data halls utilizing cold aisle containment (CAC), the cooling unit fans (equipped with VFDs) are dynamically modulated by our Building Management System (BMS). The control strategy, outlined in 'HVAC Control Sequence HCS-5.1.12', aims to maintain a slight positive pressure of 2-4 Pascals within the contained cold aisle relative to the ambient/hot aisle. This is continuously monitored by differential pressure sensors. This strategy minimizes hot air recirculation into the cold aisle while preventing excessive air bypass that would result from significant over-pressurization, thereby optimizing fan energy consumption.",
        "badExampleAnswer": "Our CRAC fans run at a constant speed that we set based on the total heat load. We make sure there's plenty of cold air."
      },
      "question_73": {
        "id": 73,
        "old_question_text": null,
        "questionText": "Has the organization implemented a scalable or modular installation and operational strategy for its cooling equipment, allowing operators to shut down unnecessary cooling plant components when not needed (e.g., during partial data centre fill or low load periods)? Is this strategy part of a regular review process triggered by changes in cooling load? Does the cooling system design also aim to maximize part-load efficiency for active components, as described in Practices 3.3.4, 5.4.2.3, and 5.4.2.4?",
        "simplifiedQuestion": "Is your cooling system modular, allowing you to turn off parts when not needed, and is it designed for good efficiency even at partial loads?",
        "reference": "2025 CoC Guide 5.2.1",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_7",
        "recommendationTitle": "Scalable or modular installation and use of cooling equipment",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Design documents for cooling systems showing modularity. Operational procedures for scaling cooling capacity up/down and shutting down unused components. Capacity management plans linking cooling deployment to load. Specifications for equipment showing good part-load efficiency."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits: Cooling system design documentation showing modular installation. Operational procedures for shutting down unnecessary cooling equipment based on load. Evidence of design for part-load efficiency (Practices 3.3.4, 5.4.2.3, 5.4.2.4).",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Review"
        ],
        "verificationMethods": "Review of cooling system design documents, operational procedures for capacity management, and equipment specifications. Inspection of modular cooling installations. Review of operational data showing scaling of cooling plant with load.",
        "nonconformity": {
          "major": "New build/retrofit cooling plant is monolithically sized for maximum future load, with no modularity or ability to efficiently scale down, leading to significant inefficiency at partial loads.",
          "minor": "Cooling plant has some modularity, but operational procedures for shutting down unnecessary components are lacking or not followed, or part-load efficiency was not a key design consideration."
        },
        "majorNonconformity": "For new builds or retrofits, cooling plant is installed as a single, large system with limited or no modularity, preventing sections from being shut down during periods of low load, and lacking design for part-load efficiency.",
        "minorNonconformity": "Cooling equipment is modular, but there's no clear operational procedure or practice to shut down unnecessary units based on load, or the part-load efficiency of the chosen modular components is not optimized.",
        "whatThisMeans": "Your data centre's cooling system should be designed in a modular way, like building blocks. This means you can add or remove cooling capacity as needed and shut down parts of the cooling system that aren't required when the data centre isn't full or when the IT load is low. This should be a regular consideration. The system should also be designed so that the active components run efficiently even when they are not at full capacity (good part-load efficiency).",
        "complianceRequires": "- Install cooling plant in a modular arrangement that allows operators to shut down unnecessary equipment/components.\n- Implement operational procedures to review cooling capacity and shut down unnecessary components in response to changes in cooling load (e.g., reduced IT load, partially populated facility).\n- Ensure the cooling system design aims to maximize part-load efficiency of active components, as detailed in Practices 3.3.4 (Design infrastructure to maximise part load efficiency), 5.4.2.3 (Efficient part load operation - for cooling plant), and 5.4.2.4 (Variable speed drives - for cooling plant).",
        "expertTip": "When designing modular cooling, ensure that the control system can effectively manage the staging of modular components (e.g., chillers, pumps, CRACs) to keep the operating units within their most efficient load range. Also, consider the energy consumption of standby units \u2013 ensure they are truly off or in a very deep standby mode.",
        "recommendedEvidence": "- Cooling system design documents showing a modular architecture (e.g., phased deployment plans, specifications for modular chillers/CRACs).\n- Standard Operating Procedures (SOPs) for scaling cooling capacity up or down, including criteria for shutting down redundant or unused cooling units.\n- Records or logs demonstrating instances where cooling equipment was shut down or scaled back due to reduced load.\n- Design considerations or equipment specifications highlighting features for maximizing part-load efficiency (e.g., variable speed drives, compressor modulation).",
        "recommendedActions": "- For new builds and major retrofits, prioritize modular cooling system designs that allow capacity to be closely matched to actual demand.\n- Develop and implement clear operational procedures for shutting down or idling cooling components that are not needed due to current load conditions.\n- Select cooling equipment (chillers, pumps, fans) with high part-load efficiency and features like variable speed drives.\n- Regularly review cooling capacity utilization and adjust the number of active cooling units accordingly.",
        "sampleResponseTemplate": "Our new data hall (DH5) features a modular cooling design with [e.g., N+1 CRAC units deployed per IT pod, and modular chillers]. Our 'Cooling Capacity Management Procedure' ([Proc ID: COOL-CAPMAN-001]) requires a review of active cooling units if IT load drops below [X]% for [Y hours], triggering shutdown of surplus units. All primary cooling components utilize VFDs for optimal part-load efficiency as per design [Design Doc ID].",
        "humanInputAnswer": "Our primary cooling plant consists of three 500kW chillers, designed for N+1 redundancy for current load, but installed in a modular fashion allowing for future expansion. Our BMS automatically stages chillers based on load, ensuring operational units are above 40% capacity for optimal efficiency. During low load winter periods, one chiller is often completely shut down, as per our 'Seasonal Cooling Ops Guide'. This strategy (Doc: 'Cooling_Modularity_Strategy_v1.pdf') also emphasizes part-load efficiency for individual components, referencing practices 3.3.4, 5.4.2.3, and 5.4.2.4.",
        "llmGeneratedAnswer": "Yes, our organization implements a scalable and modular approach to cooling equipment. As detailed in our 'Data Centre Design Standard (DCDS-2025, Section 5.1)', cooling plant for new halls is installed in modules sized for N+1 redundancy based on projected 18-month IT load increments. For example, Data Hall 4 was initially fitted with two 300kW cooling modules. Operational procedures (Ref: 'Cooling_Ops_SOP_CH4_Rev2') dictate that one module is placed in active standby and only brought online when load exceeds 70% of a single module's capacity for a sustained period, or for maintenance rotation. This approach, combined with equipment selected for high part-load efficiency (compliant with practices 3.3.4, 5.4.2.3, 5.4.2.4), ensures we avoid prolonged inefficient operation of oversized plant.",
        "badExampleAnswer": "We installed enough cooling for the entire data centre's final planned IT load so we don't have to add more later. The units are very efficient at full load."
      },
      "question_74": {
        "id": 74,
        "old_question_text": null,
        "questionText": "If the data centre facility is not yet fully populated with IT equipment, or if space has been cleared through IT consolidation efforts, are non-variable cooling plants (such as fixed speed fan CRAC/CRAH units) in these empty or sparsely populated areas turned off? Is it understood that this should not be applied if operating more plant at a lower, more efficient load (e.g., variable speed drive CRAC/CRAH units) is demonstrably more energy efficient overall?",
        "simplifiedQuestion": "Do you turn off fixed-speed cooling units in empty or nearly empty data centre areas (unless running more variable-speed units at low load is better)?",
        "reference": "2025 CoC Guide 5.2.2",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_7",
        "recommendationTitle": "Shut down unnecessary cooling equipment",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "log"
          ],
          "description": "Operational procedures for managing cooling in partially filled or de-populated areas. Logs or records showing when fixed-speed cooling units were turned off in such areas. If variable speed units are used, analysis showing why running more units at lower load is more efficient than turning some off."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, log",
        "documentDescriptor": "Mandatory: Procedures for identifying and shutting down unnecessary fixed-speed cooling units in underutilized areas. Evidence (logs, BMS data) of this practice. If VSD units are kept running at low load, justification demonstrating superior overall efficiency.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Review"
        ],
        "verificationMethods": "Review of operational procedures for cooling management in low-load areas. Physical inspection of empty/sparsely populated data centre areas to check status of cooling units. Review of BMS logs or operational data.",
        "nonconformity": {
          "major": "Fixed-speed cooling units are left running continuously in empty or significantly underutilized data centre areas without justification.",
          "minor": "Procedures for shutting down unnecessary fixed-speed cooling units are unclear or not consistently followed, or the efficiency trade-off for variable speed units is not documented if they are left running at low load in multiple."
        },
        "majorNonconformity": "Systematic operation of non-variable (fixed speed) cooling units in data centre areas that are empty or have very low IT load, without any attempt to power them down.",
        "minorNonconformity": "Inconsistent application of shutting down unnecessary fixed-speed cooling units in low-load areas, or if variable speed units are used, no clear analysis supports keeping more units running at lower individual loads versus shutting some down.",
        "whatThisMeans": "If parts of your data centre are empty or have very little IT equipment (either because it's new and not filled yet, or because you've consolidated IT gear), you should turn off any cooling units (especially older, fixed-speed fan types) serving those specific areas. This saves a lot of energy. However, if you have modern variable-speed cooling units, it might sometimes be more efficient to keep several of them running at a low speed rather than turning some off completely \u2013 you'd need to check which approach is better for your specific setup.",
        "complianceRequires": "- If the facility is not yet fully populated or space has been cleared through consolidation, shut down non-variable cooling plant (e.g., fixed speed fan CRAC/CRAH units) in these empty or sparsely populated areas.\n- This practice aims to eliminate unnecessary energy consumption by cooling units serving no or minimal load.\n- Caveat: This practice should *not* be applied if operating more (e.g., variable speed drive) cooling units at a lower individual load is demonstrably more energy efficient overall than turning some units completely off. This requires an understanding of the part-load efficiency curves of the equipment.",
        "expertTip": "Use your Building Management System (BMS) or DCIM to monitor IT load and environmental conditions in different zones. Implement automated sequences or clear operator procedures to power down fixed-speed cooling units in zones where the load drops below a defined threshold for a sustained period. For VSD units, model their combined efficiency at different numbers of active units versus load to determine the optimal staging strategy.",
        "recommendedEvidence": "- Standard Operating Procedures (SOPs) for managing cooling in partially occupied or de-populated data centre zones.\n- BMS logs, operator logs, or change records demonstrating that fixed-speed cooling units in empty/low-load areas have been turned off.\n- If variable speed units are used and more are kept running at lower load: an energy efficiency analysis or manufacturer data justifying this operational mode over shutting down units.",
        "recommendedActions": "- Regularly survey data centre spaces to identify areas with no or very low IT load.\n- Develop and implement a procedure to power down fixed-speed cooling units serving these identified areas.\n- For areas with variable speed cooling units, analyze the system efficiency curves to determine the optimal number of units to run at different load levels (it may be more efficient to run more units at lower speeds than fewer units at higher speeds).\n- Ensure that when areas are re-populated with IT equipment, the cooling is appropriately reactivated.",
        "sampleResponseTemplate": "Our 'Low-Load Area Management Procedure' ([Proc ID: LLAM-002]) dictates that fixed-speed CRAC units in zones with less than [X]% IT capacity utilization are powered down. In [Data Hall Y, Zone Z], which is currently [empty/partially filled], [Number] fixed-speed CRACs are currently off. This is logged in our [BMS/Operator Log]. For our variable-speed CRAHs, we follow [Vendor X]'s recommended staging for optimal part-load efficiency, keeping [N] units active at reduced speed.",
        "humanInputAnswer": "Our operational procedure 'DC-OPS-COOL-007: Low Load Zone Management' requires that any data hall quadrant with less than 10% of its designed IT load, and served by fixed-speed CRACs, must have at least 50% of its dedicated CRACs powered off. In our new North Wing, currently at 20% IT build-out, 3 out of 6 fixed-speed CRACs in the unoccupied sections are currently offline. This is verified and logged weekly.",
        "llmGeneratedAnswer": "Yes, our organization implements a policy for managing cooling in underutilized data centre spaces. As per 'Energy Conservation Protocol ECP-003: Partial Occupancy Cooling', if a data centre zone is less than 25% populated with IT equipment, or if an area has been significantly depopulated through consolidation, any fixed-speed fan CRAC/CRAH units serving that specific area are powered down. For instance, in Data Hall 3, Bay D, which is currently being prepared for a future deployment and is empty, the two dedicated fixed-speed CRAC units are turned off. This status is reviewed monthly. This protocol does not apply where our analysis (Ref: 'VSD_Cooling_Efficiency_Study_2024') shows that operating a larger number of our variable-speed CRAH units at lower, optimized speeds provides greater overall energy efficiency for the given load.",
        "badExampleAnswer": "All our cooling units run all the time to make sure the room is ready for new servers and to keep some airflow going."
      },
      "question_75": {
        "id": 75,
        "old_question_text": null,
        "questionText": "Before any IT equipment changes (installation, removal, or relocation) are made, does the organization have a process to review the availability of cooling, including the placement and flow characteristics (e.g., CFM, % open area, damper settings) of vented floor tiles or other air delivery mechanisms in the affected area, to optimize the use of cooling resources and ensure adequate cooling for the planned IT configuration?",
        "simplifiedQuestion": "Before changing IT equipment, do you check and adjust cooling (like vent tile placement) to ensure it's optimized for the new setup?",
        "reference": "2025 CoC Guide 5.2.3",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_7",
        "recommendationTitle": "Review of cooling before IT equipment changes",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "dwg"
          ],
          "description": "Change management procedures that include a cooling review step for IT equipment changes. Checklists or forms used for this cooling review. Data centre layout plans showing vented tile configurations and airflow design."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx, dwg",
        "documentDescriptor": "Mandatory: Documented procedure within the change management process requiring a review of cooling availability and air delivery (e.g., vented tile placement/settings) before IT equipment installations, moves, or changes.",
        "verification_method": [
          "Document Review",
          "Process Verification",
          "Site Inspection"
        ],
        "verificationMethods": "Review of change management procedures and associated cooling review checklists. Observation of a recent IT equipment change process. Inspection of vented tile placement in relation to IT equipment.",
        "nonconformity": {
          "major": "IT equipment changes are made without any review or adjustment of local cooling delivery, leading to potential hotspots, overcooling, or inefficient use of cooling resources.",
          "minor": "Cooling review before IT changes is informal, inconsistent, or does not adequately consider optimization of air delivery mechanisms like vented tile placement and settings."
        },
        "majorNonconformity": "Absence of a formal process to review and adjust cooling delivery (e.g., vented tile placement and settings) in conjunction with IT equipment changes, leading to suboptimal cooling resource utilization.",
        "minorNonconformity": "A cooling review process exists but is not consistently applied before all IT equipment changes, or it focuses only on gross capacity without optimizing local air delivery mechanisms like vented tile configuration.",
        "whatThisMeans": "Before your organization installs, removes, or moves any IT equipment, there should be a standard step to check the cooling in that specific area. This includes looking at where vented floor tiles are placed (if you use a raised floor), how much air they are delivering, and if they are correctly configured for the new or changed IT setup. This helps make sure the equipment will be properly cooled and that you're using your cooling resources efficiently.",
        "complianceRequires": "- Implement a process to review the availability and configuration of cooling before any IT equipment changes (installation, removal, relocation).\n- This review should include checking the placement and flow characteristics of vented floor tiles (e.g., % open area, damper settings if adjustable) or other air delivery mechanisms in the affected area.\n- The aim is to optimize the use of existing cooling resources and ensure adequate cooling for the planned IT equipment configuration.",
        "expertTip": "Integrate a 'cooling assessment' step into your IT change management workflow. This should include verifying that the right type and number of vented tiles are in place for the specific IT equipment being installed, and removing or replacing tiles with blank ones where equipment is removed. Use airflow measurement tools to confirm adequate airflow post-change.",
        "recommendedEvidence": "- Change management procedure documentation that includes a mandatory step for 'cooling review' or 'airflow assessment' before IT equipment changes.\n- Checklists or forms used by data centre staff to assess cooling adequacy (including vented tile placement/type) for new/changed IT deployments.\n- Examples of change records that show this cooling review was performed.\n- Data centre airflow design guidelines that specify appropriate vented tile configurations for different IT load densities.",
        "recommendedActions": "- Incorporate a mandatory cooling review into the standard process for all IT equipment installations, moves, adds, and changes (IMACs).\n- Train data centre operations staff on how to assess local cooling conditions and optimize vented tile placement and settings for different IT equipment types and densities.\n- Maintain an inventory of different types of vented floor tiles (e.g., varying % open area, directional flow) to allow for optimized placement.\n- After IT equipment changes, verify that inlet temperatures are within desired ranges.",
        "sampleResponseTemplate": "Our 'IT Equipment Change Management Procedure' ([Doc ID: ITCM-003], Section 4.2) requires a 'Cooling and Airflow Verification' step before any server installation or relocation. This involves [e.g., 'checking for correct vented tile placement (e.g., 25% open tiles for loads <5kW, 50% for >5kW), ensuring no obstructions, and verifying sufficient local airflow using an anemometer']. Adjustments are made as needed. For example, during the recent [Project X] deployment, [Number] vented tiles were [relocated/changed] to optimize cooling.",
        "humanInputAnswer": "Our Change Management process (Ref: CM-PROC-001) includes a mandatory 'Facilities Impact Assessment' for all IT hardware changes. This requires the data centre operations team to verify and adjust, if necessary, the local cooling delivery, including confirming correct perforated tile (typically 25% open for <5kW racks, 50% for 5-10kW racks) placement and quantity before equipment power-on. For example, for CR12345, installing a new blade chassis, two 25% tiles were replaced with 50% tiles.",
        "llmGeneratedAnswer": "Yes, a cooling review is an integral part of our IT equipment change process. As detailed in the 'Data Centre Change Management Policy (DCCMP-2025, Section 6.3: Pre-Deployment Checks)', before any IT equipment is installed, moved, or significantly altered, a 'Cooling Availability Assessment' must be completed. This assessment, performed by the facilities team, verifies adequate cooling capacity and optimal airflow delivery for the proposed configuration. It includes checking the placement, type (e.g., % open area), and damper settings of vented floor tiles in the target location to ensure efficient use of cooling resources. The assessment form (Form: DCCMP-F007) must be signed off before the change can proceed.",
        "badExampleAnswer": "When we put in new servers, we just make sure there's a vented tile in front of the rack. If it's a lot of new gear, we might add another one."
      },
      "question_76": {
        "id": 76,
        "old_question_text": null,
        "questionText": "Has the organization established a formally defined cooling strategy, and is there a process to periodically review the IT equipment deployment and cooling system deployment against this defined strategy to ensure ongoing alignment, efficiency, and adequacy?",
        "simplifiedQuestion": "Do you have a defined cooling strategy, and do you regularly review your IT and cooling setups against it?",
        "reference": "2025 CoC Guide 5.2.4",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_7",
        "recommendationTitle": "Define and review of cooling strategy",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx"
          ],
          "description": "Documented cooling strategy for the data centre(s). Records of periodic reviews of IT and cooling deployment against this strategy (e.g., meeting minutes, review reports, action plans)."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx",
        "documentDescriptor": "Mandatory: A documented data centre cooling strategy. Evidence of periodic reviews (e.g., quarterly, annually) that compare current IT and cooling deployments against this strategy and identify any necessary adjustments.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of the documented cooling strategy and records of strategy review meetings or reports. Interviews with data centre facilities management and IT planning teams.",
        "nonconformity": {
          "major": "No defined cooling strategy exists, or if one exists, it is not periodically reviewed against actual IT and cooling deployments.",
          "minor": "A cooling strategy is defined but reviews are infrequent, informal, or do not consistently lead to actionable improvements or adjustments."
        },
        "majorNonconformity": "Absence of a formally defined cooling strategy, or a complete lack of periodic reviews of IT equipment and cooling deployment against any existing strategy.",
        "minorNonconformity": "A cooling strategy is documented, but the review process is ad-hoc, not comprehensive, or fails to result in actionable outcomes to maintain or improve alignment and efficiency.",
        "whatThisMeans": "Your organization needs a clear, written plan (a strategy) for how you cool your data centre. This isn't just about the equipment, but also how you deploy IT gear in relation to your cooling setup. You then need to regularly check if what you're actually doing (your current IT layout and cooling operation) still matches that strategy and if it's still the most efficient and effective approach.",
        "complianceRequires": "- Formally define a cooling strategy for the data centre.\n- Establish a process to periodically review the existing IT equipment deployment and the cooling system deployment against this defined strategy.\n- The review should assess alignment, identify deviations, and propose corrective actions or strategy updates to ensure ongoing cooling efficiency and effectiveness.",
        "expertTip": "Your cooling strategy should be a living document, updated to reflect changes in IT technology (e.g., higher density equipment, liquid cooling adoption), data centre load, energy costs, and available cooling technologies. Schedule formal strategy reviews at least annually, or when significant changes to the IT or facility infrastructure are planned.",
        "recommendedEvidence": "- The documented data centre cooling strategy, outlining key principles, technologies, and operational targets.\n- Agendas, minutes, or reports from periodic meetings where the cooling strategy and its alignment with current deployments are reviewed.\n- Action plans or change records resulting from these strategy reviews, showing adjustments made to IT or cooling deployments to improve efficiency or alignment.\n- Capacity planning documents that refer to the cooling strategy.",
        "recommendedActions": "- Develop and document a comprehensive cooling strategy that addresses design, technology selection, operational parameters, and management practices.\n- Schedule and conduct regular reviews (e.g., annually or biannually) of this strategy, involving both facilities and IT teams.\n- As part of the review, compare the current state of IT deployments and cooling system performance against the strategy's objectives.\n- Document any identified gaps or areas for improvement and create action plans to address them.",
        "sampleResponseTemplate": "Our 'Data Centre Cooling Strategy' ([Doc ID: DC-CS-2025]) defines our approach to [e.g., airflow management, temperature setpoints, free cooling utilization]. We conduct an annual review of IT and cooling deployments against this strategy, chaired by the [e.g., Head of DC Operations]. The last review on [Date] resulted in [Example action, e.g., 'a plan to re-balance airflow in Hall B due to new high-density racks'].",
        "humanInputAnswer": "Our 'Data Centre Cooling Strategy & Roadmap (DC-COOL-STRAT-V3)' was last updated in Jan 2025. It details our approach to airflow management, containment, and free cooling. Annually, our cross-functional 'Thermal Efficiency Review Board' (IT & Facilities) reviews current IT rack layouts, cooling capacity, and PUE sub-component data against this strategy. The Q4 2024 review (Minutes: TERB-2024-Q4) led to a project to optimize vented tile placement in our older data hall.",
        "llmGeneratedAnswer": "Yes, our organization maintains a 'Data Centre Cooling Strategy Document (DCCS_Rev_2025.1)'. This document outlines our strategic approach to cooling, including design philosophies, preferred technologies, operational setpoints, and efficiency targets. As per Section 7 of this document, a formal review of IT equipment deployment and cooling system performance against this strategy is conducted biannually by the Data Centre Operations Committee. The most recent review, completed in March 2025, (Ref: 'Cooling_Strategy_Review_H1_2025.pdf'), confirmed alignment and identified two minor optimization opportunities for CRAC unit scheduling, which are now being implemented.",
        "badExampleAnswer": "We have a cooling system that works. We don't really have a written strategy for it beyond keeping the room cool."
      },
      "question_77": {
        "id": 77,
        "old_question_text": null,
        "questionText": "Has the organization ensured that Computer Room Air Conditioning (CRAC) / Computer Room Air Handler (CRAH) units in occupied data centre areas have appropriate and consistent temperature and relative humidity setpoints, and that these setpoints are properly calibrated? Is care taken to avoid units working against each other (e.g., one humidifying while another dehumidifies, or one cooling while another heats if reheat is active), for instance by networking their controls to run together when installed in the same area, while also ensuring that any such interconnected control does not introduce new single points of failure?",
        "simplifiedQuestion": "Are your AC unit (CRAC/CRAH) setpoints for temperature and humidity consistent, calibrated, and set up to prevent units from fighting each other (e.g., by linking their controls)?",
        "reference": "2025 CoC Guide 5.2.5",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_7",
        "recommendationTitle": "Review CRAC / CRAH Settings",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "CRAC/CRAH operational settings documentation (temperature, humidity setpoints). Calibration records for CRAC/CRAH sensors. Description or diagrams of control system networking/coordination for CRAC/CRAH units in the same zone. Analysis of potential single points of failure from interconnected controls."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Mandatory: Documented temperature and humidity setpoints for CRAC/CRAH units. Sensor calibration records. Evidence of strategies (e.g., control networking, deadbands) to ensure units do not work against each other. Assessment of any new failure modes introduced by linked controls.",
        "verification_method": [
          "Document Review",
          "System Data Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of CRAC/CRAH setpoint documentation, control logic, and sensor calibration records. Analysis of BMS data for setpoint consistency and evidence of units conflicting. Inspection of CRAC/CRAH control panels and networked configurations.",
        "nonconformity": {
          "major": "CRAC/CRAH units in the same zone have widely varying or uncalibrated setpoints, leading to simultaneous heating/cooling or humidification/dehumidification and significant energy waste.",
          "minor": "Setpoints are generally consistent but not optimally coordinated, leading to occasional minor conflicts between units, or sensor calibration is infrequent."
        },
        "majorNonconformity": "CRAC/CRAH units within the same control zone have significantly different temperature or humidity setpoints, or uncalibrated sensors, leading to units actively working against each other (e.g., simultaneous cooling and humidification by different units in the same space).",
        "minorNonconformity": "Setpoints are not precisely coordinated, leading to occasional inefficient operation where units might briefly counteract each other. Control system networking for coordination is absent or potential single points of failure from it are not assessed.",
        "whatThisMeans": "All the air conditioning units (CRACs or CRAHs) in a specific area of your data centre should be set to the same temperature and humidity targets, and their sensors must be accurate (calibrated). It's very wasteful if one unit is trying to cool while another is heating (if it has a heater), or if one is adding humidity while another is trying to remove it. Modern systems often allow AC units in the same zone to be linked so they work together efficiently. If you link them, make sure this connection itself doesn't become a single point where if it fails, everything stops working.",
        "complianceRequires": "- Ensure CRAC/CRAH units in occupied data centre areas have appropriate and consistent temperature and relative humidity setpoints.\n- Ensure these setpoints and the sensors that control them are properly calibrated.\n- Take specific care to avoid units working against each other (e.g., one unit humidifying while another in the same zone is dehumidifying, or simultaneous heating and cooling).\n- Consider solutions such as networking the controls of CRAC/CRAH units installed in the same area so they run in a coordinated manner.\n- If interconnecting controls, carefully assess and mitigate any potential new single points of failure that might be introduced by such linking.",
        "expertTip": "Implement control deadbands for temperature and humidity between CRAC/CRAH units in the same zone. For example, set slightly different start/stop points for cooling or humidification to prevent multiple units from fighting over a very precise setpoint. Ensure sensor calibration is part of your regular preventive maintenance schedule.",
        "recommendedEvidence": "- Documented standard setpoints for temperature and humidity for CRAC/CRAH units in different data centre zones.\n- Calibration certificates or logs for CRAC/CRAH temperature and humidity sensors.\n- Description or diagrams of the control system strategy used to coordinate multiple CRAC/CRAH units in a zone (e.g., networked controls, lead/lag sequences, deadband settings).\n- Risk assessment or design review documentation addressing potential single points of failure if controls are networked.",
        "recommendedActions": "- Establish and document standard temperature and humidity setpoints for all CRAC/CRAH units, ensuring consistency within each control zone.\n- Implement a regular calibration schedule for all CRAC/CRAH sensors.\n- Configure control systems (e.g., via BMS or networked unit controllers) to ensure coordinated operation of units within the same zone, preventing simultaneous opposing actions.\n- When networking controls, perform a failure modes and effects analysis (FMEA) to identify and mitigate potential new single points of failure.",
        "sampleResponseTemplate": "All CRAC/CRAH units within each defined zone (as per '[Zone Plan ID]') have consistent temperature ([X]\u00b0C \u00b1[Y]\u00b0C) and humidity ([A]% \u00b1[B]% RH) setpoints, documented in '[BMS Settings Doc ID]'. Sensors are calibrated [Annually/Biannually] (Ref: '[Calibration Schedule]'). Units within a zone are networked using [Control System/Protocol] with [e.g., lead-lag and deadband logic] to prevent conflicting operation. A risk assessment for the networked control was conducted on [Date].",
        "humanInputAnswer": "Our CRAC units in each data hall operate as a networked group with coordinated setpoints (e.g., Data Hall A: 25\u00b0C supply, 40-60% RH). Temperature and humidity sensors are calibrated annually by a third party (calibration records: 'CAL-2024-SITEA.pdf'). The BMS employs a 1.5\u00b0C deadband between cooling stages and a 10% RH deadband for humidification/dehumidification to prevent units from fighting. This master/slave control logic is detailed in our 'HVAC Control Philosophy Document'.",
        "llmGeneratedAnswer": "Yes, CRAC/CRAH settings are standardized and reviewed. Our 'Environmental Control Standard (ECS-2025, Section 4.1)' specifies consistent temperature (24\u00b0C \u00b11\u00b0C) and relative humidity (50% \u00b110%) setpoints for all CRAH units within a given data hall zone. All unit sensors undergo biannual calibration (records maintained in 'CalibrationLog_HVAC.xlsx'). Units within each zone are networked via our BMS, employing a lead-lag-assist control strategy and widened deadbands to prevent simultaneous cooling/heating or humidifying/dehumidifying cycles. This coordinated control prevents units from working antagonistically. A Failure Mode and Effects Analysis (FMEA) was conducted on the networked control system (Ref: 'FMEA_CRAC_Network_2024') to identify and mitigate potential single points of failure.",
        "badExampleAnswer": "Each CRAC unit has its own thermostat, and the technicians set them to keep the room cool. Sometimes they adjust them if one area feels warmer."
      },
      "question_78": {
        "id": 78,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented dynamic control systems for building cooling that take multiple factors into account (e.g., current cooling load, data floor air temperature, external ambient air temperature and humidity) to optimize the overall cooling system (e.g., chilled water loop temperature, economizer operation) in real-time, according to an agreed and documented strategy?",
        "simplifiedQuestion": "Do you use dynamic/smart controls for your main cooling system that adjust in real-time based on IT load, room temperature, and outside weather to stay efficient?",
        "reference": "2025 CoC Guide 5.2.6",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_7",
        "recommendationTitle": "Dynamic control of building cooling",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "screenshots"
          ],
          "description": "Cooling system control philosophy document detailing dynamic control strategies. BMS configuration showing inputs (IT load, internal/external temperatures/humidity) and control logic for optimizing chilled water temperature or economizer modes. Reports showing system performance under dynamic control."
        },
        "docRequired": "Yes, if implemented",
        "acceptedFiles": "pdf, docx, screenshots",
        "documentDescriptor": "Mandatory for new builds/retrofits (if implemented): Documentation of the dynamic cooling control strategy. BMS configuration showing how factors like IT load, internal temperatures, and external ambient conditions are used to optimize cooling system parameters (e.g., chilled water temperature, economizer mode) in real-time.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of cooling control strategy documents and BMS sequence of operations. Analysis of BMS trend data showing how cooling system parameters (e.g., chilled water setpoints) respond to changes in IT load and ambient conditions.",
        "nonconformity": {
          "major": "Cooling system operates on fixed setpoints without dynamic adjustment based on load or ambient conditions, despite opportunities for optimization.",
          "minor": "Some dynamic control elements are present but not fully optimized, or the control strategy does not comprehensively use available data (load, external conditions) for real-time optimization."
        },
        "majorNonconformity": "For new builds or retrofits, cooling system parameters (e.g., chilled water temperature, economizer engagement) operate on static setpoints without real-time dynamic optimization based on multiple factors like cooling load and external ambient conditions.",
        "minorNonconformity": "Dynamic control of building cooling is implemented but does not utilize all relevant inputs (e.g., ignores IT load feedback or external temperature), or the control strategy and algorithms are not fully optimized for maximum energy efficiency.",
        "whatThisMeans": "Your organization should consider using (or for new facilities, implement) smart control systems for your main building cooling. These systems should automatically adjust how the cooling system runs in real-time, based on several factors like how much cooling is actually needed by the IT equipment, the current temperature in the data hall, and the temperature and humidity outside. For example, it might raise the chilled water temperature or switch to 'free cooling' (economizer) mode more effectively when conditions allow. This should all follow a pre-agreed plan.",
        "complianceRequires": "- Consider implementing dynamic control systems for the building's cooling infrastructure.\n- These control systems should take multiple factors into account to optimize the cooling system in real-time, such as:\n    - Current cooling load (from IT equipment).\n    - Data floor air temperature(s).\n    - External (outside ambient) air temperature and humidity.\n- Optimization targets could include adjusting chilled water loop temperature, maximizing economizer operation, or optimizing heat rejection.\n- The dynamic control should operate according to an agreed and documented strategy.",
        "expertTip": "Implement a chilled water temperature reset strategy that dynamically adjusts the setpoint based on outside air temperature and/or current IT load. Higher chilled water temperatures significantly improve chiller efficiency and maximize economizer hours. Ensure the control strategy has appropriate safeguards to maintain adequate cooling under all conditions.",
        "recommendedEvidence": "- Documented cooling system control philosophy or sequence of operations that describes the dynamic control strategy.\n- BMS screenshots or configuration files showing how inputs like IT load, data floor temperatures, and external ambient conditions are used in control loops.\n- Trend data from the BMS illustrating how parameters like chilled water temperature or economizer mode are dynamically adjusted in response to changing conditions.\n- Documentation of the agreed strategy that guides these dynamic controls.",
        "recommendedActions": "- For new builds or major retrofits, specify and implement dynamic control capabilities within the Building Management System (BMS) for the cooling plant.\n- Ensure sensors are in place to provide the necessary input data (IT load, internal/external temperatures and humidity).\n- Develop and document a clear control strategy for how the cooling system will be dynamically optimized.\n- Regularly review and fine-tune the dynamic control algorithms and setpoints based on operational data and performance analysis.",
        "sampleResponseTemplate": "Our new data centre ([Facility Name]) utilizes a dynamic cooling control strategy, documented in '[BMS Control Philosophy ID: BMS-COOL-DYN-001]'. The system adjusts [e.g., chilled water supply temperature and economizer mode selection] in real-time based on inputs including [IT load (from PDU monitoring), average cold aisle temperatures, and external wet-bulb temperature]. This strategy aims to maximize free cooling and optimize chiller plant efficiency.",
        "humanInputAnswer": "Our 'Chiller Plant Optimization Protocol (CHOP-003)', implemented in our new facility, uses a dynamic control strategy. The BMS (Siemens Desigo) adjusts the chilled water setpoint based on a combination of outside air wet-bulb temperature and overall IT load (derived from UPS output). For every 1\u00b0C rise in IT return air temperature, the chilled water setpoint can float up by 0.5\u00b0C, within defined limits, maximizing chiller efficiency and economizer hours. This strategy is documented and reviewed annually.",
        "llmGeneratedAnswer": "Yes, our Building Management System (BMS) for the 'Site Alpha' data centre, commissioned in 2024, implements dynamic control of the cooling infrastructure. As detailed in the 'Cooling System Control Strategy (CSCS-SA-2024)', the BMS continuously optimizes the chilled water loop temperature and air-side economizer operation. This optimization is based on real-time inputs including: total IT power consumption (from PDU monitoring), average server inlet temperatures across multiple zones, and external ambient dry-bulb and wet-bulb temperatures. The agreed strategy prioritizes maximizing free cooling hours and then optimizing chiller efficiency by adjusting chilled water setpoints according to load and ambient conditions.",
        "badExampleAnswer": "Our chillers have a fixed temperature setting that we know works well for our IT equipment year-round."
      },
      "question_79": {
        "id": 79,
        "old_question_text": null,
        "questionText": "Has the organization implemented an effective and regular maintenance program for its entire cooling system (including chillers, pumps, cooling towers, CRAC/CRAH units, heat exchangers, etc.)? Is this maintenance aimed at conserving or restoring the equipment to a 'like new' condition to maintain its designed cooling efficiency? Does this include tasks such as checking belt tension, addressing condenser coil fouling (water or air side), and evaporator fouling? Does it also encompass regular air filter changes in air handling units to maintain air quality and reduce friction losses, along with routine monitoring of air quality and a regular technical cleaning regime for relevant areas (including under-floor plenums if applicable)?",
        "simplifiedQuestion": "Do you have a thorough, regular maintenance program for your entire cooling system to keep it running at peak efficiency and cleanliness?",
        "reference": "2025 CoC Guide 5.2.7",
        "priority": "High (Value: 2, Expected: Entire Data Centre)",
        "stepSection": "step_7",
        "recommendationTitle": "Effective regular maintenance of cooling plant",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "log"
          ],
          "description": "Cooling system maintenance plan and schedule. Maintenance logs and records for all cooling components. Procedures for specific tasks (coil cleaning, filter changes, belt tensioning). Air quality monitoring records and technical cleaning schedules/logs."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx, log",
        "documentDescriptor": "Mandatory: Documented cooling system preventative maintenance program. Records of maintenance activities (e.g., coil cleaning, filter changes, belt checks). Air quality monitoring results and technical cleaning logs.",
        "verification_method": [
          "Document Review",
          "Maintenance Record Audit",
          "Site Inspection"
        ],
        "verificationMethods": "Review of preventative maintenance plans, schedules, and completion records for all cooling system components. Inspection of equipment condition (coils, filters, belts) and general cleanliness of plant rooms and air pathways.",
        "nonconformity": {
          "major": "No formal or regular preventative maintenance program for the cooling system, leading to visible degradation of equipment condition (e.g., heavily fouled coils, dirty filters) and likely reduced efficiency.",
          "minor": "Maintenance program is informal, inconsistent, or does not cover all critical aspects (e.g., coil cleaning, air quality, technical cleaning) necessary to maintain designed efficiency."
        },
        "majorNonconformity": "Absence of an effective, regular maintenance program for the cooling system, leading to significant degradation in designed cooling efficiency due to issues like fouled coils, dirty filters, or poor air quality.",
        "minorNonconformity": "Maintenance of the cooling system is performed but is reactive rather than proactive, or the program is incomplete (e.g., lacks regular filter changes, coil cleaning, or technical cleaning of air pathways), impacting optimal efficiency.",
        "whatThisMeans": "Your organization must have a good, regular maintenance plan for every part of your cooling system (chillers, pumps, AC units, etc.) to keep it running as efficiently as it was designed to. This means checking things like fan belts, cleaning coils and evaporators if they get dirty, changing air filters regularly to keep air flowing well and clean, and routinely cleaning technical areas (like under raised floors if they are used for air) to maintain air quality.",
        "complianceRequires": "- Implement an effective and regular maintenance program for the entire cooling system.\n- The aim of this maintenance should be to conserve or achieve a 'like new' condition for equipment to maintain its designed cooling efficiency.\n- Include specific tasks such as checking belt tension, addressing condenser coil fouling (both water and air side), and managing evaporator fouling.\n- Incorporate regular air filter changes in air handling units to maintain air quality and reduce friction losses (airflow resistance).\n- Include routine monitoring of air quality and a regular technical cleaning regime for relevant areas, including under-floor plenums if they are part of the air distribution system.",
        "expertTip": "Use a Computerized Maintenance Management System (CMMS) to schedule, track, and document all cooling system maintenance activities. Include efficiency-related checks in your PM tasks, such as measuring temperature differentials across coils or pressure drops across filters, to proactively identify performance degradation.",
        "recommendedEvidence": "- Documented preventative maintenance plan for the cooling system, specifying tasks, frequencies, and responsibilities for all major components.\n- Maintenance logs or CMMS records showing completion of scheduled maintenance tasks (e.g., coil cleaning dates, filter replacement dates, belt tension checks).\n- Specifications for air filters used and records of air quality monitoring, if performed.\n- Schedule and logs for technical cleaning of data halls and air plenums.",
        "recommendedActions": "- Develop a comprehensive preventative maintenance program for all cooling system components based on manufacturer recommendations and operational experience.\n- Ensure maintenance tasks specifically address factors affecting efficiency, such as heat exchanger cleanliness, filter condition, and air/water flow rates.\n- Implement regular air filter replacement schedules and maintain high standards of cleanliness in plant rooms and air pathways.\n- Train maintenance staff on best practices for maintaining cooling system efficiency.",
        "sampleResponseTemplate": "We have a 'Cooling System Preventative Maintenance Program' ([Doc ID: COOL-PM-001]) managed via our [CMMS Name]. It includes [e.g., quarterly coil cleaning for CRAHs, monthly filter changes, annual chiller overhaul]. For example, condenser coils on [Chiller X] were last cleaned on [Date], and air filters in [Data Hall Y] were changed on [Date]. Air quality is monitored via [Method] and underfloor cleaning occurs [Frequency].",
        "humanInputAnswer": "Our 'Cooling Infrastructure PM Program (CIPP-003)' outlines weekly, monthly, quarterly, and annual maintenance for all cooling components. For example, CRAC unit filters are replaced monthly, and condenser coils on our chillers are cleaned quarterly. Last month (April 2025), all 48 CRAC filters were changed (Log Ref: CRACFILT-202504). Belt tensions are checked monthly. We also conduct bi-annual technical cleaning of the data hall sub-floor plenum.",
        "llmGeneratedAnswer": "Yes, our organization adheres to an 'Effective Regular Maintenance Protocol for Cooling Plant (ERMP-CP-2025)'. This protocol mandates scheduled maintenance tasks designed to keep the cooling system operating at or near its original design efficiency. Key activities include: \n- Monthly: Air filter inspection and replacement (based on differential pressure or max 3 months), belt tension checks on fan motors.\n- Quarterly: Condenser coil cleaning (air-cooled chillers), evaporator coil inspection, and water quality testing for water-cooled systems.\n- Annually: Comprehensive technical cleaning of CRAC/CRAH internals and associated under-floor/ceiling plenums. \nAll activities are logged in our CMMS (Ref: 'Maximo PM Records - Cooling'). Air quality is also monitored, informing filter selection and cleaning frequency.",
        "badExampleAnswer": "We fix the AC units when they break. The filters are changed sometimes if they look really bad."
      },
      "question_80": {
        "id": 80,
        "old_question_text": null,
        "questionText": "Has the organization reviewed and, if possible and beneficial, increased its chilled water temperature setpoints to maximize the use of free cooling and reduce chiller compressor energy consumption? Is seasonal impact taken into account for these setpoints? Are chilled water setpoints raised in conjunction with supply air flow setpoints to avoid reducing cooling capacity? Furthermore, has the organization reviewed and, if useful, increased the chilled water temperature difference (Delta T between supply and return) to reduce water flow rates and thereby pump energy consumption? For DX systems, are evaporator temperatures similarly reviewed and optimized, considering that Electronic Expansion Valves (EEVs) can allow better control and higher evaporator temperatures than Thermostatic Expansion Valves (TEVs)?",
        "simplifiedQuestion": "Have you reviewed and optimized your chilled water temperatures (and DX evaporator temps) to save energy on chillers/pumps, considering seasonal changes and using EEVs if you have DX systems?",
        "reference": "2025 CoC Guide 5.2.8",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_7",
        "recommendationTitle": "Review and optimise chilled water temperature",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "Cooling system operational parameters showing chilled water/DX evaporator temperature setpoints. Analysis or reports from reviews of these setpoints. BMS trend data showing seasonal adjustments or changes in Delta T. Documentation on EEV use if applicable."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Mandatory: Documentation of current chilled water (or DX evaporator) temperature setpoints. Evidence of reviews and optimizations made to these setpoints to maximize free cooling, reduce compressor use, and optimize pumping energy (e.g., by increasing Delta T). Records of seasonal setpoint adjustments.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of cooling system operational setpoints and control strategies. Analysis of BMS trend data for chilled water temperatures, Delta T, and DX evaporator temperatures. Comparison with design specifications and free cooling operational hours.",
        "nonconformity": {
          "major": "Chilled water or DX evaporator temperature setpoints are unnecessarily low and are not reviewed or optimized for energy efficiency, significantly limiting free cooling potential and increasing compressor/pumping energy.",
          "minor": "Setpoint reviews are infrequent or not comprehensive; opportunities to increase chilled water temperature, widen Delta T, or optimize DX evaporator temperatures (e.g., with EEVs) are not fully exploited."
        },
        "majorNonconformity": "Chilled water temperature setpoints (or DX evaporator temperatures) are maintained at unnecessarily low levels without periodic review or optimization, significantly curtailing free cooling opportunities and increasing compressor energy consumption.",
        "minorNonconformity": "Reviews of chilled water/DX evaporator temperatures occur, but setpoints are not aggressively optimized, seasonal adjustments are minimal, chilled water Delta T is not actively managed to reduce pump energy, or EEV capabilities in DX systems are not fully leveraged.",
        "whatThisMeans": "Your organization should regularly check and adjust the temperature of the chilled water your cooling system produces. By carefully increasing this temperature (while still ensuring adequate cooling), you can use 'free cooling' (like outside air) more often and make your chillers work less, saving energy. This needs to consider seasonal changes. Also, increasing the temperature difference between the water going to and coming from the cooling units can reduce the amount of water that needs to be pumped, saving pump energy. If you use Direct Expansion (DX) cooling systems, their evaporator temperatures should also be optimized, and using Electronic Expansion Valves (EEVs) can help with this.",
        "complianceRequires": "- Regularly review and, if possible and beneficial, increase chilled water temperature setpoints to:\n    - Maximize the use of free cooling.\n    - Reduce chiller compressor energy consumption.\n- Take seasonal impacts into account when setting chilled water temperatures.\n- When raising chilled water setpoints, ensure they are raised in conjunction with supply air flow setpoints to avoid reducing overall cooling capacity.\n- Review and, if useful, increase the chilled water temperature difference (Delta T between supply and return water) to reduce water flow rates and thereby save pump energy.\n- For Direct Expansion (DX) systems: similarly review and optimize evaporator temperatures.\n- Recognize that Electronic Expansion Valves (EEVs) generally allow better control and permit higher evaporator temperatures (leading to better efficiency) than Thermostatic Expansion Valves (TEVs).",
        "expertTip": "Implement a dynamic chilled water temperature reset strategy that adjusts the setpoint based on outside air temperature and/or current IT load. For Delta T optimization, monitor flow rates and temperatures across the chilled water loop to identify areas where flow can be reduced without impacting cooling performance, often in conjunction with variable speed pumps.",
        "recommendedEvidence": "- Documented chilled water (or DX evaporator) temperature setpoint policy, including any seasonal adjustments.\n- BMS trend data showing historical chilled water supply/return temperatures and calculated Delta T.\n- Analysis or reports from reviews of chilled water/evaporator temperature setpoints and resulting energy impacts (e.g., increased free cooling hours, reduced chiller/pump energy).\n- Documentation of CRAC/CRAH supply air temperature adjustments made in conjunction with chilled water temperature changes.\n- If EEVs are used in DX systems, evidence of their configuration and impact on evaporator temperatures and efficiency.",
        "recommendedActions": "- Establish a process for periodically reviewing and optimizing chilled water (or DX evaporator) temperature setpoints.\n- Analyze the impact of raising chilled water temperatures on free cooling hours and chiller efficiency, considering seasonal variations.\n- Evaluate and optimize the chilled water Delta T to reduce pumping energy, ensuring adequate flow to all cooling units.\n- For DX systems, investigate upgrading from TEVs to EEVs if not already in place, and optimize evaporator temperature settings.\n- Ensure that changes to chilled water temperatures are coordinated with adjustments to air-side systems (e.g., CRAC/CRAH fan speeds or supply air temperatures) to maintain cooling capacity.",
        "sampleResponseTemplate": "Our chilled water supply temperature setpoint is reviewed [quarterly/seasonally] and currently operates at [X\u00b0C] during [Summer] and [Y\u00b0C] during [Winter] to maximize free cooling. This is documented in '[Cooling Ops Manual ID]'. We aim for a system Delta T of [Z\u00b0C] by managing flow rates with VSD pumps. Our DX units [are EEV equipped and optimized / have TEVs and evaporator temps are set to P\u00b0C].",
        "humanInputAnswer": "Our chilled water setpoint is dynamically adjusted by our BMS based on outside air temperature, ranging from 7\u00b0C in peak summer to 12\u00b0C during winter to maximize economizer use (Ref: 'BMS CHW Control Logic v2.1'). We also actively manage for a minimum 8\u00b0C Delta T across the chilled water loop by controlling variable speed pumps, which has reduced pumping energy by an estimated 15%. For our few remaining DX CRACs, we have upgraded them to EEVs and operate with an average evaporator temperature of 10\u00b0C.",
        "llmGeneratedAnswer": "Yes, our organization regularly reviews and optimizes chilled water temperatures. As per 'Chilled Water System Operational Protocol (CWOP-2025)', the chilled water supply temperature is set to a baseline of 10\u00b0C but is dynamically reset upwards based on ambient temperature and current cooling load to maximize free cooling hours via our waterside economizer, sometimes reaching up to 15\u00b0C. This strategy is reviewed seasonally. We also aim to maintain a chilled water Delta T of at least 7\u00b0C by optimizing flow with variable speed pumps to reduce pumping energy. Our newer package DX units are equipped with EEVs, and their evaporator temperatures are optimized by the unit controllers for maximum part-load efficiency, a significant improvement over our older TEV-based units.",
        "badExampleAnswer": "We keep our chilled water at 6\u00b0C all year round because that's what the chiller was designed for and it keeps everything very cold."
      }
    }
  },
  "step_8": {
    "description": "2025 Guide Section 5: Cooling (Practices 5.3.1-5.3.5 from Sec 5.3 'Temperature and Humidity Settings'; and 5.4.1.1-5.4.1.5 from Sec 5.4.1 'Free Cooling / Economised Cooling')",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Cooling_3"
    ],
    "critical_questions": [],
    "questions": {
      "question_81": {
        "id": 81,
        "old_question_text": null,
        "questionText": "Has the organization designed and is it operating its data centres at their highest efficiency to deliver IT equipment intake air within the ASHRAE Class A2 allowable environmental range (or a wider range if equipment permits and is appropriately managed)? Is this practice implemented to enable energy savings by reducing or eliminating overcooling? Is it understood that some data centres may contain equipment with legacy environmental ranges (as per Practice 4.1.2) which might restrict the maximum temperature until segregation (Practices 5.1.11, 5.3.4, 5.3.5) is achieved, and that additional airflow management practices (Section 5.1) may need to be implemented concurrently? Is it also validated that IT equipment fan power consumption does not increase more than the cooling system energy saved when intake temperatures are raised?",
        "simplifiedQuestion": "Do you operate your data centre with IT intake air temperatures within ASHRAE A2 (or wider if safe) to save energy by avoiding overcooling, considering legacy gear and IT fan power?",
        "reference": "2025 CoC Guide 5.3.1",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_8",
        "recommendationTitle": "Review and if possible, raise target IT equipment intake air temperature",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "Cooling policy or operational guidelines specifying target IT intake air temperature ranges (e.g., ASHRAE A2). BMS trend data showing actual intake temperatures. Documentation of any assessments for raising temperatures (including IT fan power impact). Plans for managing legacy equipment with restrictive ranges."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Mandatory: Documented target IT equipment intake air temperature range (e.g., aligned with ASHRAE Class A2). BMS/monitoring data showing actual operating temperatures. Analysis of impact on IT fan power if temperatures are raised. Strategy for managing any legacy equipment incompatible with these ranges.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "Site Inspection"
        ],
        "verificationMethods": "Review of environmental setpoint policies and operational data (BMS temperature logs). Assessment of any analysis done on IT fan power impact. Inspection for management of legacy equipment.",
        "nonconformity": {
          "major": "Data centre IT intake air temperatures are consistently maintained significantly below ASHRAE Class A2 lower recommended limits without justification, leading to substantial overcooling and energy waste.",
          "minor": "Target intake temperatures are within A2, but opportunities to operate towards the higher end of the allowable range are not explored, or the impact of raised temperatures on IT fan power is not assessed."
        },
        "majorNonconformity": "Systematic operation of the data centre with IT equipment intake air temperatures significantly below the ASHRAE Class A2 allowable range without valid justification, leading to overcooling and energy waste. No consideration for raising temperatures.",
        "minorNonconformity": "While operating within A2, no proactive effort to operate towards the upper end of the allowable range is made, or potential negative impacts on IT equipment fan energy from raised temperatures are not evaluated. Management of legacy equipment constraints is unclear.",
        "whatThisMeans": "Your data centre should be run so that the air going into your IT equipment is within the temperature range recommended by ASHRAE Class A2 (or even wider if your equipment allows). This helps save energy by not cooling the room more than necessary. If you have older equipment that needs cooler temperatures, that might limit how warm you can run the whole area, unless you put that gear in a separate zone. When you raise temperatures, also check that your IT equipment fans don't start using up all the energy you saved on cooling.",
        "complianceRequires": "- Design and operate data centres to deliver IT equipment intake air within the ASHRAE Class A2 allowable range (typically 18\u00b0C to 27\u00b0C or 64.4\u00b0F to 80.6\u00b0F) at their highest efficiency.\n- Understand this enables energy savings by reducing or eliminating overcooling.\n- Acknowledge constraints from equipment with legacy environmental ranges (Practice 4.1.2) and plan for their segregation (Practices 5.1.11, 5.3.4, 5.3.5) to allow the rest of the facility to operate at wider ranges.\n- Implement necessary airflow management best practices (Section 5.1) concurrently to ensure successful operation at higher temperatures.\n- Validate that any increase in IT equipment fan power consumption (due to higher intake temperatures) does not negate the energy savings achieved in the cooling system.\n- Refer to ASHRAE environmental guidelines for detailed specifications.",
        "expertTip": "Gradually increase your cold aisle temperature setpoints by 0.5\u00b0C or 1\u00b0C at a time, closely monitoring IT equipment inlet temperatures and server fan responses using your monitoring system. This allows you to find the optimal balance for your specific environment and equipment mix. Ensure robust airflow management (containment, blanking plates) is in place before raising temperatures significantly.",
        "recommendedEvidence": "- Documented target operating temperature range for IT equipment inlets, aligned with ASHRAE Class A2 or a defined wider range.\n- BMS or sensor data logs showing actual IT equipment intake temperatures over time.\n- Reports or studies analyzing the potential for raising intake air temperatures and the associated energy savings, including any assessment of IT equipment fan power impact.\n- Documentation or plans for segregating any legacy IT equipment that restricts operating at higher ambient temperatures.",
        "recommendedActions": "- Review and document the current operating temperature setpoints for IT equipment intake air.\n- Compare current setpoints against the ASHRAE Class A2 allowable range and identify opportunities to safely raise temperatures.\n- Before raising temperatures, ensure airflow management best practices (Section 5.1) are effectively implemented.\n- Conduct a trial or phased increase of temperature setpoints, monitoring IT equipment behavior (inlet temps, fan speeds, error logs).\n- Address any legacy equipment constraints through segregation or planned replacement.",
        "sampleResponseTemplate": "Our data halls are operated to maintain IT equipment intake air temperatures within the ASHRAE A2 allowable range, with a target supply air temperature of [e.g., 24-25\u00b0C]. This is documented in our '[Cooling Operations SOP ID]'. We have [Number] legacy systems with tighter requirements; these are [segregated in Zone X / scheduled for replacement by Y date]. A 2024 review confirmed that our current operating temperature provides net energy savings, considering IT fan power (Ref: '[Temp_Optimization_Study_2024]').",
        "humanInputAnswer": "Our standard operating procedure for data halls (DCSOP-ENV-001) specifies maintaining server inlet temperatures between 22\u00b0C and 26\u00b0C, aligning with ASHRAE A2. We currently target 25\u00b0C. Before implementing this, we conducted an analysis (Ref: 'IT_Fan_Power_Study_2023.pdf') which confirmed that the cooling energy savings outweighed any minor increases in server fan power. Legacy equipment incompatible with this range is isolated in a separately controlled pod, as per practice 5.1.11.",
        "llmGeneratedAnswer": "Yes, the organization designs and operates its data centres to deliver IT equipment intake air within the ASHRAE Class A2 allowable range (18\u00b0C - 27\u00b0C). Our 'Environmental Control Policy (ECP-DC-2025)' mandates a target cold aisle temperature of 24\u00b0C, reviewed seasonally. This policy aims to reduce overcooling. We have identified two legacy storage systems (Ref: AssetDB# LSS001, LSS002) with a maximum inlet temperature of 22\u00b0C; these are located in a dedicated, separately cooled zone (Zone D, as per 'DC_ZonePlan_Rev3') to comply with Practices 5.1.11 and 5.3.4. An internal study ('IT_Fan_vs_Cooling_Energy_2024.docx') confirmed net energy savings at our current operating temperatures, even accounting for IT fan response.",
        "badExampleAnswer": "We keep our data centre very cold, around 18-20\u00b0C, to make sure the servers never overheat. It's safer that way."
      },
      "question_82": {
        "id": 82,
        "old_question_text": null,
        "questionText": "Has the organization reviewed and widened the working humidity range for its data centres by reducing the lower humidity setpoint and, if practical, increasing the upper humidity setpoint, both within the ASHRAE Class A2 allowable range (or a wider range if equipment permits)? Is this done to reduce energy demand for humidification and dehumidification? Is it understood that legacy equipment constraints (Practice 4.1.2) might restrict this until segregation (Practices 5.1.11, 5.3.4, 5.3.5) is achieved, and that controlling humidity within a wider range of humidity ratio or relative humidity can reduce energy consumption?",
        "simplifiedQuestion": "Do you use a wider humidity range (within ASHRAE A2 or better) to save energy on humidifiers/dehumidifiers, considering legacy gear?",
        "reference": "2025 CoC Guide 5.3.2",
        "priority": "High (Value: 4, Expected: Entire Data Centre)",
        "stepSection": "step_8",
        "recommendationTitle": "Review and widen the working humidity range",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "Cooling policy or operational guidelines specifying target IT intake humidity ranges (e.g., ASHRAE A2). BMS trend data showing actual humidity levels and setpoints. Documentation of any assessments for widening humidity ranges. Plans for managing legacy equipment with restrictive humidity needs."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Mandatory: Documented target operating relative humidity range for IT equipment, aligned with ASHRAE Class A2 (or wider). Evidence of efforts to reduce lower humidity setpoints and potentially increase upper setpoints to save humidification/dehumidification energy. Strategy for legacy equipment.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of environmental setpoint policies for humidity and operational data (BMS humidity logs). Assessment of humidification/dehumidification system operation and energy use.",
        "nonconformity": {
          "major": "Humidity is tightly controlled to a narrow range (e.g., 45-55% RH) without justification, leading to excessive humidification/dehumidification energy use, when equipment could tolerate wider ASHRAE A2 ranges.",
          "minor": "The potential to widen humidity operating bands within ASHRAE A2 limits has not been fully explored or implemented, or legacy equipment unnecessarily restricts the entire facility's humidity range."
        },
        "majorNonconformity": "Systematic operation with unnecessarily tight relative humidity control bands (e.g., much narrower than ASHRAE Class A2 allowable), leading to significant energy consumption for humidification and/or dehumidification.",
        "minorNonconformity": "The working humidity range is within ASHRAE A2, but no effort has been made to optimize it by adjusting lower/upper setpoints to minimize humidification/dehumidification, or legacy equipment constraints are not effectively managed through segregation.",
        "whatThisMeans": "Your data centre's humidity levels should be allowed to fluctuate within a wider range, as long as it's safe for the IT equipment (e.g., within ASHRAE Class A2 guidelines). This means you might lower the minimum humidity setting (to reduce humidification) and potentially raise the maximum setting (to reduce dehumidification). This can save a lot of energy. If you have old equipment that needs very specific humidity, it might need to be in a separate area so it doesn't limit the whole data centre.",
        "complianceRequires": "- Review and reduce the lower relative humidity setpoint(s) of the data centre, within the ASHRAE Class A2 allowable range (or wider if equipment permits), to reduce the demand for humidification.\n- Review and, if practical, increase the upper relative humidity setpoint(s) of the data floor, within the ASHRAE Class A2 allowable range (or wider), to decrease dehumidification loads.\n- Understand that equipment with legacy environmental ranges (Practice 4.1.2) may constrain the humidity range until segregation (Practices 5.1.11, 5.3.4, 5.3.5) is achieved.\n- Recognize that controlling humidity within a wider range of humidity ratio or relative humidity reduces humidification and dehumidification energy consumption.\n- Refer to ASHRAE environmental guidelines for detailed specifications on allowable ranges.",
        "expertTip": "Focus on controlling to dew point temperature rather than just relative humidity, as dew point is a more direct indicator of the risk of condensation (at high levels) or static discharge (at very low levels). Widening the acceptable RH range often results in significant energy savings, especially if it avoids simultaneous humidification and dehumidification by different units or systems.",
        "recommendedEvidence": "- Documented target operating relative humidity range (e.g., 20%-80% RH or specific dew point limits) for IT equipment, aligned with ASHRAE Class A2 or wider.\n- BMS or sensor data logs showing actual relative humidity levels and humidification/dehumidification system setpoints and activity.\n- Analysis or reports justifying the selected humidity range and demonstrating consideration of energy savings from reduced humidification/dehumidification.\n- Documentation or plans for segregating any legacy IT equipment that requires a tighter humidity range.",
        "recommendedActions": "- Review current humidity setpoints and compare them against the allowable ranges in ASHRAE Class A2 (or equipment specifications).\n- Gradually adjust lower and (if practical) upper humidity setpoints towards the A2 limits, monitoring for any adverse effects.\n- Ensure cooling coil temperatures are managed to avoid unnecessary dehumidification (often a result of overly cold chilled water or DX evaporator temperatures).\n- Address any legacy equipment with restrictive humidity needs through segregation or planned replacement.\n- Educate operations staff on the energy impact of humidity control.",
        "sampleResponseTemplate": "Our data centres operate within the ASHRAE A2 allowable humidity range, with specific setpoints documented in '[Cooling Ops SOP ID]'. We target a lower RH setpoint of [e.g., 25%] to minimize humidification and an upper setpoint of [e.g., 75%] to reduce dehumidification. Legacy equipment requiring tighter humidity control is [segregated/managed as per X]. This strategy has reduced our humidification energy by [Y%].",
        "humanInputAnswer": "Our 'Environmental Standards Policy ESP-001' defines the operating humidity range as 20% - 80% RH, consistent with ASHRAE A2 allowable. Humidification is only activated below 25% RH and dehumidification (primarily via cooling coil temperature management) above 75% RH. This wider band, implemented in 2023, has reduced our humidifier energy consumption by over 70% and dehumidification load significantly. All new equipment is A2 compliant for humidity.",
        "llmGeneratedAnswer": "Yes, the organization has reviewed and widened its working humidity range. Our 'Data Centre Environmental Parameters Standard (DCEPS-2025)' now specifies an allowable relative humidity range of 25% to 75% RH, which is within ASHRAE A2 guidelines. The lower setpoint for active humidification has been reduced from 40% to 25% RH, and active dehumidification (primarily managed by avoiding overcooling) is targeted only when RH exceeds 75%. This strategy, documented in 'Humidity Control Protocol HCP-002', significantly reduces energy demand for both humidification and dehumidification. Legacy equipment with stricter humidity requirements is managed in a segregated zone as per Practice 5.1.11.",
        "badExampleAnswer": "We try to keep humidity around 50% all the time because we heard that's best for servers."
      },
      "question_83": {
        "id": 83,
        "old_question_text": null,
        "questionText": "Where appropriate and deemed effective for the specific data centre environment and IT equipment, has the organization designed and operated its data centres (or zones within) to allow IT equipment inlet environmental conditions (temperature and humidity) to expand to even wider ranges, such as 5\u00b0C to 40\u00b0C and 5% to 80% RH (non-condensing), or exceptionally up to +45\u00b0C, as described in standards like ETSI EN 300 019-1-3 Class 3.1? Is this considered with the aim of potentially eliminating mechanical refrigeration completely in most climates, thereby also removing its associated capital and maintenance costs?",
        "simplifiedQuestion": "Do you operate (or design for) even wider temperature/humidity ranges (e.g., 5-40\u00b0C, up to 45\u00b0C like ETSI Class 3.1) to potentially eliminate mechanical cooling entirely?",
        "reference": "2025 CoC Guide 5.3.3",
        "priority": "High (Value: 5, Expected: Optional Practice)",
        "stepSection": "step_8",
        "recommendationTitle": "Expanded IT equipment inlet environmental conditions (temperature and humidity)",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "dwg"
          ],
          "description": "Design documents or operational policies specifying use of expanded environmental ranges (e.g., ETSI Class 3.1). Feasibility studies or risk assessments for operating at such wide ranges. Specifications of IT equipment certified for these expanded ranges. Data showing periods of operation without mechanical refrigeration."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, dwg",
        "documentDescriptor": "Optional: Documentation supporting the design and operation within expanded IT inlet environmental conditions (e.g., ETSI EN 300 019-1-3 Class 3.1, 5\u00b0C-40\u00b0C or up to 45\u00b0C). Evidence of IT equipment suitability and any elimination/reduction of mechanical refrigeration.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "Site Inspection"
        ],
        "verificationMethods": "Review of design philosophy, IT equipment specifications (for expanded range compatibility), and operational data (temperature/humidity logs, cooling system mode). Interviews with design and operations staff.",
        "nonconformity": {
          "major": null,
          "minor": "Opportunities to operate with significantly expanded environmental ranges (e.g., ETSI Class 3.1) are not explored or implemented, despite having suitable IT equipment and climatic conditions that could allow for elimination of mechanical refrigeration (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: While wider ranges are considered, the full potential (e.g., up to 40/45\u00b0C as per ETSI Class 3.1) is not leveraged despite IT equipment compatibility and suitable climate, or the move to such ranges is not supported by adequate risk assessment and equipment selection.",
        "whatThisMeans": "For some data centres and types of IT equipment, it might be possible and very energy efficient to allow even wider temperature and humidity ranges than ASHRAE A2 \u2013 for example, letting temperatures go from 5\u00b0C up to 40\u00b0C, or even 45\u00b0C in some cases (like ETSI Class 3.1). If your IT gear can handle this, you might be able to completely stop using traditional, energy-hungry mechanical cooling (refrigeration-based chillers/ACs) for most, if not all, of the year in many climates. This would save a lot on equipment costs and maintenance too.",
        "complianceRequires": "- Where appropriate and effective for the specific data centre and IT equipment, design and operate for expanded IT equipment inlet environmental conditions.\n- This means allowing air inlet temperature and relative humidity to range, for example, from 5\u00b0C to 40\u00b0C and 5% to 80% RH (non-condensing).\n- Under exceptional conditions, temperatures up to +45\u00b0C might be considered, as described in standards like ETSI EN 300 019-1-3 Class 3.1.\n- The aim of operating at such expanded ranges is the potential to completely eliminate mechanical refrigeration in most climates, thereby saving both operational energy and the capital/maintenance costs of cooling systems.",
        "expertTip": "This practice requires careful selection of IT equipment specifically designed and warranted for these wider environmental ranges (e.g., compliant with ETSI Class 3.1 or NEBS Level 3). It also necessitates a robust facility design that can manage potential challenges like rapid temperature changes, dust, and humidity extremes when relying heavily on ambient air for cooling.",
        "recommendedEvidence": "- Data centre design philosophy or operational policy explicitly stating the adoption of expanded environmental ranges (e.g., referencing ETSI Class 3.1).\n- Specifications and warranty documents for IT equipment confirming its capability to operate reliably within these expanded ranges.\n- Design documentation for the cooling system showing reliance on free cooling technologies and minimal or no mechanical refrigeration.\n- Operational data logs showing periods where the data centre operates within these expanded ranges without mechanical cooling active.\n- Risk assessment considering the implications of operating at such expanded ranges.",
        "recommendedActions": "- Assess the feasibility of adopting expanded environmental operating ranges based on local climate, IT equipment capabilities, and business risk tolerance.\n- If pursuing this, select IT equipment specifically designed and warranted for operation within ranges like ETSI EN 300 019-1-3 Class 3.1.\n- Design the facility and cooling systems to maximize the use of free cooling and minimize or eliminate reliance on mechanical refrigeration.\n- Implement robust monitoring and control systems to manage conditions within the expanded ranges and protect IT equipment.",
        "sampleResponseTemplate": "For our [Specific application/facility type, e.g., 'edge micro-sites' / 'containerized data centres in X climate'], we have adopted an expanded operational environmental range of [e.g., 5\u00b0C to 40\u00b0C and 5-80% RH], aligning with [ETSI EN 300 019-1-3 Class 3.1]. IT equipment ([Vendor/Model]) is specified for these conditions. This allows us to operate with [e.g., 100% direct free air cooling] for approximately [Y]% of the year, eliminating mechanical refrigeration. This is documented in '[Design Spec Z]'.",
        "humanInputAnswer": "Our new 'Desert Edge' modular data centres are designed to operate within the ETSI EN 300 019-1-3 Class 3.1E environmental range, allowing inlet temperatures up to 40\u00b0C. All IT equipment selected for these modules (Ref: 'Edge Hardware Spec EHS-001') is certified by vendors for these conditions. This design uses indirect air-to-air heat exchangers and evaporative cooling, eliminating mechanical refrigeration entirely, which is crucial for these remote, arid locations.",
        "llmGeneratedAnswer": "Yes, our organization has implemented expanded IT equipment inlet environmental conditions for specific, newly deployed Edge computing modules. These modules (Ref: 'EdgePod_DesignSpec_Rev2') are designed to operate within a temperature range of 5\u00b0C to 40\u00b0C and a relative humidity of 5% to 80% RH (non-condensing), broadly aligning with ETSI EN 300 019-1-3 Class 3.1. This strategy allows these modules to utilize 100% free air cooling with high-efficiency filtration in most of our deployment climates, thereby eliminating the need for mechanical refrigeration. All IT equipment within these modules is specifically procured to meet these extended environmental tolerances, as verified by vendor conformance statements.",
        "badExampleAnswer": "We always keep our data centre at a steady 21\u00b0C because that's what we've always done, and it seems to work for all the equipment."
      },
      "question_84": {
        "id": 84,
        "old_question_text": null,
        "questionText": "For new data centre builds or retrofits, where the facility houses both IT equipment compliant with wider environmental ranges (e.g., ASHRAE Class A2 as per Practice 4.1.3) and other equipment requiring more restrictive temperature or humidity control (e.g., legacy IT as per Practice 4.1.2, certain UPS batteries, or archival tape media), has the organization ensured that separate physical areas or zones are provided for this restrictive equipment? Do these separate areas have their own independent environmental controls and potentially separate cooling systems to facilitate the optimization of cooling efficiency for each zone, thereby preventing the entire data centre's efficiency from being compromised by the equipment with the most restrictive needs?",
        "simplifiedQuestion": "For new/retrofitted DCs, do you create separate zones with independent climate controls for equipment that needs tighter temperature/humidity than your general IT gear (e.g., legacy systems, batteries, tapes)?",
        "reference": "2025 CoC Guide 5.3.4",
        "priority": "High (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_8",
        "recommendationTitle": "Separate environmental zones",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Data centre design documents for new builds/retrofits showing segregated environmental zones. Specifications for independent environmental controls/cooling systems for these zones. Inventory list of equipment with restrictive environmental needs and their placement in these zones."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits housing equipment with mixed environmental needs: Design documentation clearly showing physically separate zones for equipment requiring restrictive environmental controls (e.g., tighter temperature/humidity for UPS batteries, tape media, legacy IT). Specifications for independent environmental controls for these zones.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of data centre design plans, HVAC zoning diagrams, and equipment placement schedules. Physical inspection of any implemented separate environmental zones and their dedicated control systems.",
        "nonconformity": {
          "major": "In new builds/retrofits, equipment with restrictive environmental needs is co-mingled with general IT equipment, forcing the entire data hall to operate at less efficient, tighter environmental parameters to satisfy the most sensitive items.",
          "minor": "Separate zones are considered or partially implemented, but controls are not fully independent, or the segregation is not effective in allowing the main data hall to operate at wider, more efficient environmental ranges."
        },
        "majorNonconformity": "For new builds or retrofits, failure to provide physically separate areas with independent environmental controls for equipment that requires more restrictive temperature or humidity conditions (e.g., some UPS batteries, tape archives, legacy IT as per 4.1.2) when co-located with IT equipment that can tolerate wider ranges (e.g., ASHRAE A2 as per 4.1.3).",
        "minorNonconformity": "Separate environmental zones are designated, but the environmental controls are not fully independent or optimized for the specific needs of the restrictive equipment, or the segregation does not effectively allow the main IT areas to operate at more energy-efficient wider environmental parameters.",
        "whatThisMeans": "When building a new data centre or doing a major upgrade, if you have some equipment that needs very specific, tight temperature or humidity control (like old IT gear, some UPS batteries, or tape storage for archives) alongside modern IT gear that can handle wider conditions, you should put the sensitive equipment in its own separate area. This area needs its own independent climate controls. This way, the sensitive gear gets what it needs without forcing the entire data centre to run inefficiently just for a few items.",
        "complianceRequires": "- For new builds or retrofits: If housing both IT equipment compliant with wider environmental ranges (Practice 4.1.3) and other equipment requiring more restrictive temperature/humidity control (Practice 4.1.2, or specific M&E like UPS batteries, or media like tape archives).\n- Provide separate physical areas or zones for the equipment requiring more restrictive controls.\n- These separate areas must have independent environmental controls.\n- Consider using entirely separate cooling systems for these zones if necessary.\n- The objective is to optimize cooling efficiency for each zone and prevent the entire data centre's efficiency from being compromised by the equipment with the most restrictive environmental range.",
        "expertTip": "During the design phase, clearly identify all equipment types and their specific environmental requirements. Plan for modular 'pods' or dedicated rooms for equipment groups with unique needs. This not only helps with energy efficiency but can also simplify management and maintenance of those specialized systems.",
        "recommendedEvidence": "- Data centre design documents (floor plans, HVAC schematics) for new builds/retrofits showing designated separate environmental zones.\n- Specifications of the independent environmental control systems (e.g., dedicated small CRACs, specific BMS control loops) for these zones.\n- An inventory list of equipment types that require restrictive environmental controls and their location within these designated zones (e.g., UPS battery rooms, tape library areas, legacy system pods).\n- Operational setpoints for these separate zones compared to the general data hall areas.",
        "recommendedActions": "- During the planning and design of new data centres or major retrofits, conduct a thorough inventory of all planned equipment and their specific environmental operating requirements.\n- Identify any equipment (IT, M&E, storage media) that requires tighter temperature or humidity control than the general IT equipment (which should ideally meet Practice 4.1.3 for ASHRAE A2).\n- Design and create physically separate zones or rooms with independent, appropriately sized environmental control systems for this restrictive equipment.\n- Ensure that the main data halls can then be operated at wider, more energy-efficient environmental parameters without being constrained by these few items.",
        "sampleResponseTemplate": "Our new data centre build ([Project Name], completed [Date]) includes separate environmental zones for specific equipment. As documented in '[Design Specification DC-SPEC-005]', our [e.g., UPS battery room / tape library] is maintained at [X\u00b0C and Y% RH] by a dedicated [cooling system type], while the main server halls operate within ASHRAE A2 ranges. This prevents the [battery/tape] requirements from dictating overall data hall conditions.",
        "humanInputAnswer": "Our new data centre facility, 'Site Gamma', commissioned in Q1 2025, incorporates three 'Special Environment Rooms' (SERs) as per design document 'Gamma-Design-Rev2.pdf'. SER1 houses our VRLA UPS batteries and is maintained at 21\u00b0C by a dedicated mini-split system. SER2 contains our LTO tape library, kept at 20\u00b0C and 45% RH. This allows our main data halls to operate at up to 27\u00b0C (ASHRAE A2), maximizing free cooling potential for the bulk of the IT load.",
        "llmGeneratedAnswer": "Yes, during the recent retrofit of our 'Alpha Wing' (completed Q4 2024), we established separate environmental zones. The main IT equipment, compliant with ASHRAE Class A2 (as per Practice 4.1.3), is housed in the primary data halls. However, a dedicated 'Legacy Systems Pod' (LSP-AW-01) was created for specific older servers (compliant with Practice 4.1.2 - ASHRAE A1) and our archival tape library, which require tighter temperature (20\u00b0C \u00b11\u00b0C) and humidity (45% \u00b15% RH) controls. This LSP is served by an independent, right-sized precision cooling unit, as detailed in 'Alpha Wing Retrofit Design - HVAC Section (AWR-HVAC-D003)'. This segregation ensures the main data halls can operate at more energy-efficient A2 parameters without being compromised.",
        "badExampleAnswer": "All our equipment, including batteries and old servers, is in the main data hall. We just set the room temperature to be safe for the most sensitive piece of gear."
      },
      "question_85": {
        "id": 85,
        "old_question_text": null,
        "questionText": "For Colocation or Managed Service Providers designing new builds or retrofits, if customers may have equipment requiring extremely tight environmental control or legacy equipment with restrictive needs, does the provider design the facility in such a way that discrete areas can be offered to these customers with additional 'close control' cooling equipment? Is this done to match these specific customer requirements and offer a tighter SLA, while preventing such specific needs from compromising the energy efficiency of the entire data centre which might otherwise operate at wider, more efficient environmental parameters? Are these legacy/close-control support areas potentially differentially priced to reflect the higher capital and operational cost overhead, thereby incentivizing customers to install IT equipment in more efficient, standard areas?",
        "simplifiedQuestion": "If you're a colo/MSP, for new/retrofitted DCs, do you offer special zones with tighter climate control for customers needing it, to avoid making the whole DC less efficient, and possibly charge more for these zones?",
        "reference": "2025 CoC Guide 5.3.5",
        "priority": "High (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_8",
        "recommendationTitle": "Separate environmental zones - Colocation or Managed Service Provider",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Colocation/MSP service offering documentation detailing different environmental zone options and SLAs. Design documents for new/retrofitted facilities showing these discrete 'close control' areas. Pricing schedules that may differentiate costs for these specialized zones."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits by Colocation/MSPs: Service offerings and design plans showing discrete, separately controlled environmental zones for customers with legacy or tight-control equipment needs. Differential pricing models for such zones, if applied.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "Interview"
        ],
        "verificationMethods": "Review of colocation/MSP service level agreements, marketing materials for different zone offerings, and facility design documents. Inspection of any 'close control' zones. Interviews with sales and operations staff.",
        "nonconformity": {
          "major": "Colo/MSP new build/retrofit provides only a single, tightly controlled environment for all customers, even if many could use wider, more efficient ranges, or fails to offer options for customers with specific legacy needs, thereby limiting overall facility efficiency.",
          "minor": "Discrete zones are offered, but the environmental differentiation is minimal, or the pricing structure does not adequately reflect the increased cost/inefficiency of close-control zones."
        },
        "majorNonconformity": "For Colocation or Managed Service Providers' new builds/retrofits: Failure to design the facility with the capability to offer discrete, separately controlled environmental zones for customers requiring tighter environmental controls (e.g., for legacy equipment), thus forcing the entire data centre or large portions of it to operate at less efficient parameters.",
        "minorNonconformity": "While the concept of separate zones may be acknowledged, the practical design or service offering for customers with specific environmental needs is not well-developed, or differential pricing reflecting the true cost of such specialized zones is not implemented.",
        "whatThisMeans": "If you run a colocation or managed service data centre, and you're building a new one or doing a major upgrade, you should design it so you can offer special, separate areas for customers who have equipment needing very strict temperature and humidity control (like old gear). These areas would have their own dedicated 'close control' cooling. This way, these special customer needs don't make your whole data centre less energy efficient (as the rest could run at wider, more efficient settings). You might also charge more for these special zones to cover their higher running costs and encourage customers to use more efficient equipment in standard areas.",
        "complianceRequires": "- For Colocation or Managed Service Providers designing new builds or retrofits: Design the facility to offer discrete areas for customers whose equipment requires extremely tight environmental control or for legacy equipment.\n- These discrete areas should be equipped with additional 'close control' cooling equipment to match specific customer requirements and enable offering a tighter Service Level Agreement (SLA) for those zones.\n- This approach prevents specific customer needs from compromising the energy efficiency of the entire data centre, which could otherwise operate at wider environmental parameters.\n- Consider differentially pricing these legacy equipment support areas to reflect the capital and operational (metered) cost overhead of supporting a less energy-efficient legacy environment.\n- This differential pricing can act as an incentive for customers to install IT equipment in more efficient, standard areas and to consider more energy-efficient IT service delivery options.",
        "expertTip": "Clearly define the specifications, SLAs, and pricing for your standard and 'close control' environmental zones in your service catalogue. Use modular cooling solutions for the close control zones to right-size capacity to actual customer demand within those specific areas, avoiding unnecessary oversizing.",
        "recommendedEvidence": "- Service catalogue or marketing materials from the Colocation/MSP detailing different environmental zone offerings (e.g., standard ASHRAE A2 zones, premium close-control zones).\n- Design documents for new/retrofitted facilities showing the physical layout and dedicated M&E for these discrete environmental zones.\n- Example SLAs for different zone types, highlighting environmental parameter differences.\n- Pricing schedule that differentiates costs for standard versus close-control or legacy support zones, if applicable.\n- Technical specifications of the 'close control' cooling equipment used in specialized zones.",
        "recommendedActions": "- During the design of new colocation/MSP facilities or major retrofits, identify the likely need for specialized environmental zones based on target customer profiles and market demand.\n- Engineer discrete areas with independent, right-sized cooling systems capable of providing tighter environmental control.\n- Develop clear service offerings, SLAs, and potentially differential pricing for these specialized zones.\n- Educate sales and account teams on these offerings to guide customers to the most appropriate and efficient hosting environment for their needs.",
        "sampleResponseTemplate": "Our new colocation facility, [Facility Name], offers 'Premium Environment Pods' for customers with legacy or tight-control equipment needs, as detailed in our [Service Offering Document ID]. These pods ([Number] available, [Size] each) have dedicated [Cooling System Type] maintaining [e.g., 20\u00b0C \u00b11\u00b0C, 50% RH \u00b15%] and are priced at a [X]% premium over our standard ASHRAE A2 compliant space. This allows the majority of our facility to operate at more energy-efficient wider parameters.",
        "humanInputAnswer": "Our 'Colocation Service Specification for Site Delta (CSS-SD-2025)' includes an option for 'Precision Environment Suites'. These are separately built-out caged areas with dedicated Liebert CRVs providing customer-specified temperature and humidity control (e.g., 20-22\u00b0C, 45-55% RH) for an additional monthly fee that covers the increased capex and estimated opex. This allows our general colocation space to operate at an energy-efficient 25\u00b0C supply temperature.",
        "llmGeneratedAnswer": "Yes, as a Managed Service Provider, our latest facility design (Ref: 'MSP_DC_Gamma_Design_v2.0') incorporates 'Custom Environment Modules' (CEMs). These are discrete, physically segregated areas equipped with supplementary 'close control' cooling units (e.g., in-row precision coolers). CEMs are offered to clients with legacy IT equipment or specific applications requiring environmental parameters tighter than our standard ASHRAE A2 data hall conditions. This design, as per section 5.3.5 of the CoC, prevents the entire facility's efficiency from being compromised. These CEMs are subject to a differential pricing model that reflects the higher operational and capital costs, thereby incentivizing clients to migrate to more modern, environmentally tolerant IT equipment where feasible.",
        "badExampleAnswer": "We provide the same cooling for all our customers in the data hall to keep things simple. If they have old gear, it just has to cope."
      },
      "question_86": {
        "id": 86,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented direct air free cooling (direct air economization), where external ambient air is used directly to cool the facility? Does the system design include different operating modes, such as recirculating and mixing exhaust air with intake air during cold outdoor conditions to control supply air temperature and humidity? Is there an understanding and provision for backup mechanical cooling capacity for periods of high airborne pollutants (e.g., from external fires) or extreme ambient conditions, with special attention to external air quality monitoring and filtration? Is it noted that this design tends to have the lowest temperature difference between external temperature and IT supply air, and that IT equipment is likely to be exposed to a large humidity range, which constrains achievable free cooling hours by the chosen upper humidity limit?",
        "simplifiedQuestion": "Do you use (or have you considered) direct outside air for cooling, managing air quality, humidity, and backup cooling needs?",
        "reference": "2025 CoC Guide 5.4.1.1",
        "priority": "High (Value: 5, Expected: Optional Practice)",
        "stepSection": "step_8",
        "recommendationTitle": "Direct air free cooling",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "Feasibility studies for direct air free cooling. Design documents if implemented, including air filtration, mixing damper controls, backup cooling integration, and air quality monitoring strategy. Operational data showing hours of direct air economization."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Optional: If implemented or assessed: Design documentation for direct air free cooling systems, including filtration, air mixing controls, air quality monitoring, and backup cooling integration. Operational data on free cooling hours achieved and humidity range experienced by IT equipment.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Analysis"
        ],
        "verificationMethods": "Review of feasibility studies or design documents for direct air free cooling. Inspection of air intake, filtration, mixing dampers, and control systems if implemented. Analysis of operational data on free cooling hours and internal environmental conditions.",
        "nonconformity": {
          "major": null,
          "minor": "Direct air free cooling implemented without adequate air filtration, air quality monitoring, or robust backup/bypass for extreme conditions, potentially risking IT equipment (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Implementation of direct air free cooling lacks adequate air filtration, air quality monitoring, or effective control over air mixing and humidity, potentially exposing IT equipment to harmful contaminants or wide humidity swings. Backup mechanical cooling may be insufficient or poorly integrated.",
        "whatThisMeans": "Your organization should explore using outside air directly to cool your data centre if your climate is suitable. This 'direct air free cooling' can save a lot of energy. The system should be smart enough to mix warm return air with cold outside air if it's too cold outside. You'll need good air filters and a plan for monitoring outside air quality (e.g., for smoke from fires) and have backup cooling for when outside air can't be used or is too polluted. Be aware that this method means your IT gear might experience a wider range of humidity, and your ability to use free cooling will depend on the maximum humidity your equipment can handle.",
        "complianceRequires": "- Consider or implement direct air free cooling (direct air-side economization) where external ambient air is used directly to cool the facility.\n- Design the system with different operating modes, including recirculation and mixing of exhaust air with intake air to control supply conditions during cold outdoor weather.\n- Provide for backup mechanical cooling capacity for periods of high airborne pollutants or extreme ambient temperature/humidity.\n- Pay special attention to external air quality monitoring and implement robust filtration systems.\n- Understand that this design typically has the lowest temperature difference between external air and IT supply air.\n- Acknowledge that IT equipment will likely be exposed to a large humidity range, and the chosen upper humidity limit for IT equipment will directly constrain achievable free cooling hours.",
        "expertTip": "When implementing direct air free cooling, invest in high-quality multi-stage air filtration (including gaseous filtration if external air quality is a concern). Continuously monitor both external and internal air quality. Ensure your IT equipment is rated to handle the potential wider humidity fluctuations common with this cooling method.",
        "recommendedEvidence": "- Feasibility study or design documentation for a direct air free cooling system.\n- Specifications for air filtration systems, air quality monitoring sensors, and air mixing damper controls.\n- Control logic for managing different operating modes (full free cooling, mixed mode, full mechanical).\n- Design and capacity of backup mechanical cooling systems.\n- Operational data showing hours of direct air free cooling achieved, internal temperature/humidity stability, and any periods where backup cooling was used due to external air quality issues.",
        "recommendedActions": "- Conduct a detailed climate analysis and air quality assessment to determine the feasibility and potential benefits of direct air free cooling.\n- If viable, design the system with robust filtration, air quality monitoring, and intelligent controls for air mixing and mode transition.\n- Ensure adequate backup mechanical cooling capacity is available and can be seamlessly integrated.\n- Select IT equipment tolerant of the expected wider humidity range.\n- Implement operational procedures for managing the system based on external air quality alerts.",
        "sampleResponseTemplate": "We have implemented a direct air free cooling system at our [Facility Name/Location], as documented in '[HVAC Design Spec ID: HVAC-DAC-001]'. The system includes [e.g., MERV 13 + Carbon filters], [Air Quality Sensor Type] monitoring, and automated mixing dampers. Full mechanical backup is provided by [Backup System Type]. In [Year], we achieved [X hours] of direct free cooling. The IT equipment is rated for [Humidity Range].",
        "humanInputAnswer": "Our 'NordicEdge' facility utilizes a direct air economization system designed for our cold climate. The system, detailed in 'NE-CoolingDesign-v2.pdf', uses F7 + F9 filtration and mixes return air with outside air via modulating dampers to maintain supply temperature. We have real-time external PM2.5 and SO2 monitoring. If pollutant levels exceed set thresholds, or if ambient temperature/humidity is outside operational limits for free cooling, the system automatically transitions to its N+1 DX backup cooling. Last year, it operated in direct free cooling mode for 7,200 hours. IT equipment is specified for ASHRAE A3 ranges to accommodate humidity variations.",
        "llmGeneratedAnswer": "Yes, direct air free cooling has been implemented in our 'Mountain View' data centre, leveraging favorable local climatic conditions. The system design (Ref: 'MVDC_Cooling_Architecture_Rev3.pdf') allows for 100% filtered external air to be used for cooling during approximately 65% of the year. It incorporates variable-speed fans and modulating dampers for mixing external air with recirculated data hall air to maintain precise supply temperatures during colder periods. Continuous external air quality monitoring (particulates and key chemical pollutants) is in place, and if thresholds are breached, the system automatically transitions to its redundant N+1 mechanical cooling plant (chilled water system). The IT equipment in this facility is specifically selected to tolerate the wider humidity range associated with direct air economization, up to the limits of ASHRAE A3 Allowable.",
        "badExampleAnswer": "We just open the doors sometimes when it's cold outside to get some free cooling. We don't have any special systems for it."
      },
      "question_87": {
        "id": 87,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented indirect air free cooling, where recirculated air within the facility is primarily passed through an air-to-air heat exchanger (which might include adiabatic assistance like evaporative cooling on the external air side) to reject heat to the atmosphere? Does the system potentially use a thermal wheel or a quasi-indirect free cooling system? Is it understood that this design tends to have a low temperature difference between external temperature and IT supply air, that the operating IT equipment humidity range may be well controlled at negligible energy cost, and that air filtration demand is typically lower compared to direct air free cooling as data centre air is re-circulated without inducing external air directly into the data hall?",
        "simplifiedQuestion": "Do you use (or have you considered) indirect air cooling (e.g., air-to-air heat exchanger, thermal wheel) to cool your data centre without bringing outside air directly in?",
        "reference": "2025 CoC Guide 5.4.1.2",
        "priority": "High (Value: 5, Expected: Optional Practice)",
        "stepSection": "step_8",
        "recommendationTitle": "Indirect air free cooling",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "Feasibility studies for indirect air free cooling. Design documents if implemented (e.g., air-to-air heat exchangers, thermal wheels, adiabatic cooling systems). Operational data showing hours of indirect air economization and humidity control effectiveness."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Optional: If implemented or assessed: Design documentation for indirect air free cooling systems (e.g., air-to-air heat exchangers, thermal wheels), including details of any adiabatic assistance. Data on operational hours, humidity control, and internal air quality.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Analysis"
        ],
        "verificationMethods": "Review of feasibility studies or design documents for indirect air free cooling. Inspection of heat exchangers, thermal wheels, and adiabatic systems if implemented. Analysis of operational data on free cooling hours and internal environmental stability.",
        "nonconformity": {
          "major": null,
          "minor": "Indirect air free cooling is implemented but not optimally controlled, or adiabatic assistance (if used) is inefficiently managed, or its benefits for humidity control and reduced filtration are not realized (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Implementation of indirect air free cooling does not effectively isolate internal and external air streams leading to contamination or humidity control issues, or the heat exchange efficiency is significantly below design specifications. Adiabatic cooling, if used, is poorly controlled leading to excessive water use or insufficient cooling.",
        "whatThisMeans": "Your organization should explore using 'indirect air free cooling'. With this method, the air inside your data centre is cooled by passing it through a heat exchanger that uses cooler outside air on the other side to remove heat, without the outside air actually mixing with the inside air. This can also involve a 'thermal wheel' or spraying a fine mist of water on the outside of the heat exchanger (adiabatic/evaporative cooling) to improve its effectiveness. This method usually keeps humidity well-controlled inside and needs less air filtration than direct outside air systems because you're not bringing unfiltered outside air directly into the data hall.",
        "complianceRequires": "- Consider or implement indirect air free cooling systems.\n- In these systems, recirculated air within the facility is primarily passed through an air-to-air heat exchanger, using external ambient air (which may have adiabatic cooling assistance on the external side) to remove heat to the atmosphere.\n- Variations include thermal wheel (rotary heat exchanger) or quasi-indirect free cooling systems.\n- Understand that this design tends to have a low temperature difference between external air and IT supply air.\n- Recognize that the operating IT equipment humidity range can often be well controlled at negligible energy cost with this type of design.\n- Note that internal air filtration demand is typically lower compared to direct air free cooling because data centre air is recirculated, and no external air is directly induced into the data hall.",
        "expertTip": "Indirect air-to-air heat exchangers can be very effective in a wide range of climates, especially when combined with adiabatic (evaporative) cooling on the scavenger (external) air side. This combination can significantly extend the hours of free cooling. Ensure regular maintenance of the heat exchanger surfaces and any adiabatic systems to maintain efficiency.",
        "recommendedEvidence": "- Feasibility study or design documentation for an indirect air free cooling system (e.g., air-to-air plate heat exchanger, heat wheel, run-around coil).\n- Specifications for the heat exchange components and any adiabatic cooling systems used.\n- Control logic for managing the indirect free cooling system and its integration with backup mechanical cooling.\n- Operational data showing hours of indirect free cooling, achieved IT supply temperatures, and internal humidity stability.\n- Maintenance records for the heat exchangers and adiabatic components.",
        "recommendedActions": "- Conduct a climate analysis to determine the suitability of indirect air free cooling for your location(s).\n- Evaluate different indirect air heat exchange technologies (plate, wheel, coil) and the potential benefits of adiabatic assistance.\n- If viable, design the system with efficient heat exchangers, optimized airflow paths, and intelligent controls.\n- Ensure proper maintenance of heat exchange surfaces and any water systems for adiabatic cooling to prevent fouling and maintain performance.",
        "sampleResponseTemplate": "We have implemented an indirect air free cooling system at our [Facility Name] utilizing [e.g., air-to-air plate heat exchangers with adiabatic misting on the exhaust side]. This system, documented in '[HVAC Design Spec ID: HVAC-IDAC-002]', allows us to maintain good internal humidity control and reduces our reliance on bringing filtered external air directly into the data hall. In [Year], it provided [X hours] of free cooling.",
        "humanInputAnswer": "Our 'Site Beta' facility, commissioned in 2023, uses an indirect air-to-air heat exchanger system with adiabatic assistance on the external air stream. This design (Ref: 'Beta_Cooling_System_Design_v1.pdf') keeps the internal data hall air separate from outside air, simplifying humidity control and reducing the need for extensive fine particulate and gaseous filtration required by direct air systems. The system has provided over 5,000 hours of economization in its first year of operation.",
        "llmGeneratedAnswer": "Yes, our 'EcoCool Module' deployed at our 'Northern Lights Facility' utilizes indirect air free cooling. The design employs an air-to-air plate heat exchanger where recirculated data hall air transfers heat to an isolated external air stream, which is enhanced by an adiabatic (evaporative) cooling system during warmer ambient conditions. This is detailed in 'NLF_Cooling_System_Spec_Rev2.1'. This approach allows us to maintain a sealed internal environment with well-controlled humidity using minimal energy for direct humidification/dehumidification, and reduces the filtration burden compared to direct air systems. The temperature difference between external air and IT supply is typically around 3-5K with this setup.",
        "badExampleAnswer": "We only use traditional chillers and CRAC units for all our cooling."
      },
      "question_88": {
        "id": 88,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented indirect water free cooling where chilled water, supplied to Computer Room Air Handlers (CRAHs), is cooled by the external ambient air via a free cooling coil in an external dry cooler or cooling tower (which may have evaporative assistance through spray onto the dry coolers)? Is it understood that this design tends to have a higher temperature difference between the external temperature and IT supply air compared to direct air systems, but that the IT equipment humidity range may be well controlled at negligible energy cost, and that additional backup mechanical cooling (chiller or CRAC) might still be considered for extreme conditions or redundancy?",
        "simplifiedQuestion": "Do you use (or have you considered) indirect water free cooling (chilled water cooled by outside air via dry coolers/cooling towers for CRAHs), understanding its characteristics?",
        "reference": "2025 CoC Guide 5.4.1.3",
        "priority": "High (Value: 5, Expected: Optional Practice)",
        "stepSection": "step_8",
        "recommendationTitle": "Indirect water free cooling with CRAH and dry cooler or cooling tower",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "Feasibility studies for indirect water free cooling with CRAHs and dry coolers/cooling towers. Design documents if implemented, including specifications for dry coolers/towers, free cooling coils, CRAHs, control logic for economizer mode, and backup chiller integration. Operational data on free cooling hours."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Optional: If implemented or assessed: Design documentation for indirect water free cooling systems using CRAHs and external dry coolers/cooling towers (with or without evaporative assist). Details on control integration with chillers, humidity control methods, and operational performance.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Analysis"
        ],
        "verificationMethods": "Review of feasibility studies or HVAC design documents. Inspection of dry coolers/cooling towers, CRAHs, and associated pipework and controls if implemented. Analysis of BMS data on economizer mode operation, chilled water temperatures, and energy savings.",
        "nonconformity": {
          "major": null,
          "minor": "Indirect water free cooling with CRAH/dry cooler is implemented but not optimally controlled to maximize free cooling hours, or humidity control is not effectively managed, or backup integration is problematic (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: System design or operation results in a significantly higher than necessary temperature difference between external air and IT supply air, limiting free cooling hours. Or, the benefits of humidity control at low energy cost are not realized. Backup mechanical cooling integration is inefficient.",
        "whatThisMeans": "Your organization should consider using 'indirect water free cooling' with your Computer Room Air Handlers (CRAHs). In this setup, the chilled water that cools the CRAHs is itself cooled by outside air using a device called a dry cooler or a cooling tower (sometimes with water sprayed on it to help). While this method might mean the IT supply air isn't quite as close to the outside air temperature as 'direct air' systems, it often allows for good control of humidity inside the data centre with very little energy use. You might still need some backup traditional cooling for very extreme weather or for redundancy.",
        "complianceRequires": "- Consider or implement indirect water free cooling where chilled water is cooled by external ambient air via a free cooling coil, typically located in an external dry cooler or cooling tower.\n- This chilled water is then supplied to Computer Room Air Handlers (CRAHs) within the data centre.\n- Evaporative assistance (e.g., spray onto dry coolers) may be used to enhance the cooling effect of the external unit.\n- Understand that this design generally has a higher temperature difference between the external temperature and the IT supply air compared to direct air systems.\n- Recognize that the operating IT equipment humidity range may be well controlled at negligible energy cost with this design.\n- Consider the need for additional backup mechanical cooling (e.g., chillers or CRACs) for operation during extreme ambient temperature/humidity conditions or for system redundancy.",
        "expertTip": "Optimize the control system to maximize the hours of full or partial free cooling. This involves carefully selecting the changeover setpoints between free cooling and mechanical cooling modes. Using variable speed fans on dry coolers/cooling towers can significantly improve the part-load efficiency of the free cooling system.",
        "recommendedEvidence": "- Design documentation for the indirect water free cooling system, including specifications for dry coolers/cooling towers, CRAHs, free cooling coils, and control sequences.\n- Analysis showing the calculated or actual hours of free cooling operation.\n- BMS trend data showing chilled water temperatures, external ambient temperatures, and mode of operation (free cooling vs. mechanical cooling).\n- Documentation of how humidity is managed within the data centre with this system.\n- Specifications and operational procedures for any backup mechanical cooling systems.",
        "recommendedActions": "- Evaluate the feasibility of indirect water free cooling using dry coolers or cooling towers based on local climate data, water availability (for cooling towers/evaporative assist), and facility constraints.\n- If implemented, design the system with appropriately sized heat rejection units and CRAHs, and a control system that maximizes economizer operation.\n- Ensure effective water treatment for cooling towers or evaporative systems to maintain efficiency and hygiene.\n- Integrate the free cooling system controls seamlessly with any backup mechanical cooling plant.",
        "sampleResponseTemplate": "Our [Facility Name] utilizes an indirect water free cooling system with [Number] dry coolers equipped with [evaporative spray assist] serving our CRAH units. This system ([Design Doc ID: IWFC-001]) provides approximately [X hours] of free cooling annually. Humidity is managed by [Method]. Backup is provided by [Y kW] of chiller capacity. The design understands the typical temperature approach for this system type.",
        "humanInputAnswer": "Our primary data centre leverages indirect water free cooling. We have a bank of dry coolers with adiabatic spray systems that pre-cool water for our chilled water loop, which then feeds the CRAHs. This system (documented in 'Cooling Design Spec CDS-002') provides full free cooling for approximately 4,500 hours per year. Internal humidity is well-controlled via the CRAHs without requiring significant dedicated humidification/dehumidification. Our N+1 chillers provide backup during peak summer conditions or if the free cooling system is under maintenance.",
        "llmGeneratedAnswer": "Yes, our 'Northern Facility' employs an indirect water free cooling system. Chilled water, supplied to the Computer Room Air Handlers (CRAHs), is generated via an array of external dry coolers that feature an adiabatic misting system for enhanced performance during warmer ambient conditions. This system, as per 'NF_Cooling_Architecture_Rev1.pdf', is designed to maximize free cooling hours. We acknowledge the typical 5-7K approach temperature associated with this design. Humidity within the data hall is maintained by the CRAH units with minimal energy impact during free cooling mode. A parallel N+1 chiller plant provides backup mechanical cooling for extreme ambient temperatures or during maintenance of the free cooling circuit.",
        "badExampleAnswer": "We have cooling towers and chillers, but we just run the chillers all the time to make the water cold enough for the CRACs."
      },
      "question_89": {
        "id": 89,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented indirect water free cooling where Computer Room Air Conditioners (CRACs) with integrated free cooling coils are used? In this system, is chilled water (cooled by external ambient conditions via cooling towers or dry coolers, potentially with evaporative assistance) supplied to the CRAC's free cooling coil when its temperature is low enough for full or partial free cooling? Is it also supplied to the plate condenser of the CRAC's closed DX circuit when compressor operation is needed? Is it understood that this design tends to have a higher temperature difference between external temperature and IT supply air (restricting free cooling hours and increasing overhead), that partial free cooling (Mix mode) starts a few degrees below the CRAC's return air temperature, and that IT equipment humidity range may be well controlled at negligible energy cost?",
        "simplifiedQuestion": "Do you use (or have you considered) CRACs with built-in free cooling coils that use externally cooled water, understanding its characteristics and limitations?",
        "reference": "2025 CoC Guide 5.4.1.4",
        "priority": "Medium (Value: 4, Expected: Optional Practice)",
        "stepSection": "step_8",
        "recommendationTitle": "Indirect water free cooling with CRAC with integrated free cooling coil",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "Feasibility studies or design documents for CRACs with integrated free cooling coils. Specifications for the CRAC units, external heat rejection (dry coolers/towers), and control systems. Operational data on free cooling hours and mode transitions."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Optional: If implemented or assessed: Design documentation for cooling systems using CRACs with integrated free cooling coils. Details on the external water cooling loop (dry coolers/towers), control strategies for switching between free cooling, mixed-mode, and DX operation, and humidity control approach.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Analysis"
        ],
        "verificationMethods": "Review of feasibility studies or HVAC design documents. Inspection of CRAC units with integrated coils, external heat rejection equipment, and control systems if implemented. Analysis of BMS data on operational modes and free cooling hours.",
        "nonconformity": {
          "major": null,
          "minor": "CRACs with integrated free cooling coils are used, but control setpoints are not optimized to maximize free cooling/mixed-mode operation, or the system frequently resorts to less efficient DX mode unnecessarily (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Implementation of CRACs with integrated free cooling coils does not effectively maximize free cooling hours due to poor control strategy (e.g., conservative changeover setpoints) or inefficient external heat rejection. The higher approach temperatures inherent in this design are not adequately compensated for by aggressive control optimization.",
        "whatThisMeans": "Your organization should explore using special Computer Room Air Conditioners (CRACs) that have a built-in 'free cooling coil'. These CRACs can use water that has been cooled by outside air (via cooling towers or dry coolers) to cool the data centre directly when the outside air is cool enough. This saves energy by not using the CRAC's own energy-intensive compressor (its DX circuit). When it's warmer, the system can still use the externally cooled water to help the compressor, or the compressor takes over fully. This method might not give as many free cooling hours as some other designs because the temperature difference to the outside air is usually larger, but it can still offer good savings and humidity control.",
        "complianceRequires": "- Consider or implement indirect water free cooling using Computer Room Air Conditioners (CRACs) that have an integrated free cooling coil.\n- In this system, chilled water is cooled by external ambient conditions (via cooling towers or dry coolers, which may have evaporative assistance).\n- This externally cooled water is supplied to the CRAC's free cooling coil if its temperature is low enough to provide full free cooling or at least partial free cooling (mixed mode).\n- The same water loop is also supplied to the plate condenser of the CRAC's closed Direct Expansion (DX) circuit when compressor operation is needed for sufficient cooling.\n- Understand that this design typically has a higher temperature difference between the external temperature and IT supply air, which can restrict available free cooling hours and increase energy overhead compared to some other free cooling methods.\n- Recognize that partial free cooling (Mix mode) typically starts when the externally cooled water temperature is a few degrees below the return air temperature to the CRAC.\n- Note that the operating IT equipment humidity range may be well controlled at negligible energy cost with this type of design.",
        "expertTip": "Ensure the control system for CRACs with integrated free cooling coils is precisely tuned to maximize both full and partial free cooling modes. The changeover points between free cooling, mixed-mode, and full DX operation are critical. Also, ensure the external heat rejection loop (dry coolers/towers) is efficiently controlled (e.g., with VSD fans) to provide the lowest possible water temperature to the CRACs.",
        "recommendedEvidence": "- Design documentation for cooling systems employing CRACs with integrated free cooling coils.\n- Specifications for the CRAC units (highlighting the integrated coil), external dry coolers/cooling towers, and the control system managing mode transitions.\n- Control logic or sequence of operations defining when the system operates in full free cooling, partial/mixed mode, or full DX mode.\n- BMS trend data showing operational hours in each mode, water temperatures, and return air temperatures.",
        "recommendedActions": "- Evaluate the suitability of CRACs with integrated free cooling coils based on climate, facility design, and load profile.\n- If implemented, select CRAC units with high-efficiency integrated free cooling coils and efficient DX components.\n- Design and operate the external heat rejection loop (dry coolers/towers) for optimal performance to support the free cooling function.\n- Implement advanced control strategies to maximize the utilization of the free cooling coil, including effective mixed-mode operation.\n- Regularly maintain both the CRAC units (including coils) and the external heat rejection equipment.",
        "sampleResponseTemplate": "Our [Data Hall X] is equipped with [Number] [Vendor/Model] CRAC units featuring integrated water-based free cooling coils. These are served by [external dry coolers/cooling towers]. The system ([Design Doc ID: CRAC-FC-003]) is controlled by our BMS to prioritize free cooling, then mixed-mode, before engaging full DX operation. Last year, this provided [X hours] of full or partial free cooling. Humidity is managed effectively during these modes.",
        "humanInputAnswer": "In our 'West End Data Room', we utilize three 'VendorY SmartCool' CRAC units, each with an integrated free cooling coil. These coils are supplied with water from a closed-loop circuit cooled by rooftop dry coolers. The BMS prioritizes free cooling when glycol return from dry coolers is below 15\u00b0C, transitioning to mixed-mode and then full DX as needed. This system, documented in 'WEDR-Cooling-SOP.pdf', significantly reduces compressor runtime during cooler months.",
        "llmGeneratedAnswer": "Yes, our 'Site B - Legacy Wing' utilizes CRAC units with integrated free cooling coils. As per 'SiteB_LW_HVAC_Spec_Rev1.2', these units are connected to an external closed-loop dry cooler system. When ambient conditions allow the dry coolers to produce water below 18\u00b0C, the BMS diverts this water through the CRACs' integrated free cooling coils, providing either full or partial free cooling. If supplemental cooling is required, the CRACs' DX compressors are engaged, with the dry cooler loop then serving their condensers. While this design has a larger approach temperature than our newer free cooling systems, it provides substantial energy savings over full-time DX operation. The system's control logic ensures return air temperature dictates the switch to 'Mix mode' or full DX operation, and humidity is generally well-managed.",
        "badExampleAnswer": "Our CRACs are just standard DX units. We haven't looked into ones with special coils."
      },
      "question_90": {
        "id": 90,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented indirect water free cooling where Computer Room Air Handlers (CRAHs) are supplied with chilled water that is produced by a 'free cooling chiller'? In this system, does the chiller produce chilled water either through its integrated free cooling coils (using low ambient temperatures) or via normal compressor operation at higher ambient temperatures? Is it understood that this design also tends to have a higher temperature difference between external temperature and IT supply air, restricting free cooling hours and increasing overhead, and that partial free cooling (Mix mode) typically starts a few degrees below the return chilled water temperature to the chiller?",
        "simplifiedQuestion": "Do you use (or have you considered) 'free cooling chillers' that can use outside air to chill water for CRAHs when it's cold enough, understanding its characteristics?",
        "reference": "2025 CoC Guide 5.4.1.5",
        "priority": "Low (Value: 3, Expected: Optional Practice)",
        "stepSection": "step_8",
        "recommendationTitle": "Indirect water free cooling with CRAH and free cooling chiller",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "Feasibility studies or design documents for systems using free cooling chillers with CRAHs. Specifications for the free cooling chillers, CRAHs, and control systems. Operational data on hours of operation in free cooling/mixed mode vs. compressor mode."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Optional: If implemented or assessed: Design documentation for cooling systems using CRAHs supplied by 'free cooling chillers'. Details on chiller operation in free cooling, mixed, and full mechanical modes. Control strategy and performance data.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Analysis"
        ],
        "verificationMethods": "Review of HVAC design documents and chiller specifications. Inspection of free cooling chillers and associated CRAHs if implemented. Analysis of BMS data on chiller operating modes and free cooling hours.",
        "nonconformity": {
          "major": null,
          "minor": "Free cooling chillers are used, but their control setpoints or operational strategy do not maximize the utilization of the free cooling or mixed-mode capabilities, leading to excessive compressor runtime (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Free cooling chillers are installed, but their free cooling coils are underutilized due to conservative control setpoints, or the 'Mix mode' operation is not effectively implemented, leading to more compressor runtime than necessary.",
        "whatThisMeans": "Your organization should look into using 'free cooling chillers' to provide chilled water to your Computer Room Air Handlers (CRAHs). These special chillers can use their own built-in free cooling coils to chill water using cold outside air when possible. When it's too warm for that, their compressors run like regular chillers. This type of system might not give as many pure free cooling hours as some other methods because of temperature differences, and the 'mixed mode' (part free cooling, part compressor) usually starts when the water coming back to the chiller is just a bit cooler than usual. However, it's a way to integrate free cooling capabilities directly into the chiller plant.",
        "complianceRequires": "- Consider or implement indirect water free cooling where CRAHs are supplied with chilled water produced by a 'free cooling chiller'.\n- In this system, the free cooling chiller produces chilled water through one of two methods:\n    - Via its integrated free cooling coils when ambient temperatures are sufficiently low.\n    - Via normal compressor operation when ambient temperatures are higher.\n- Understand that this design typically has a higher temperature difference between the external temperature and the IT supply air, which can restrict available free cooling hours and increase energy overhead compared to some more direct free cooling methods.\n- Recognize that partial free cooling (Mix mode), where both the free cooling coil and compressors might operate, typically starts when the return chilled water temperature to the chiller is a few degrees below its design point.\n- Note that the operating IT equipment humidity range may be well controlled at negligible energy cost with this type of CRAH-based system.",
        "expertTip": "When selecting free cooling chillers, pay close attention to their efficiency in both mechanical cooling mode (IPLV/NPLV) and their effectiveness in free cooling mode. The control system should be sophisticated enough to precisely manage the transition between modes and optimize 'mixed-mode' operation to maximize energy savings across all ambient conditions.",
        "recommendedEvidence": "- Design documentation for cooling systems employing free cooling chillers and CRAHs.\n- Specifications for the free cooling chillers, detailing their free cooling coil capacity, compressor efficiency, and control capabilities.\n- Control logic or sequence of operations for the chiller plant, defining how it transitions between free cooling, mixed mode, and full mechanical cooling.\n- BMS trend data showing chiller operating modes, chilled water temperatures, ambient temperatures, and energy consumption in different modes.",
        "recommendedActions": "- Evaluate the suitability of free cooling chillers based on climate, facility load profile, and existing/planned chilled water infrastructure.\n- If implemented, select chillers with highly efficient free cooling coils and compressors, and advanced controls for mode optimization.\n- Ensure the control system accurately determines the most energy-efficient operating mode (free cooling, mixed, or mechanical) based on real-time conditions.\n- Regularly maintain the free cooling chiller, paying attention to both the refrigeration circuit and the free cooling coils/components.",
        "sampleResponseTemplate": "Our primary chilled water plant utilizes [Number] [Vendor/Model] free cooling chillers supplying our CRAH units. These chillers ([Spec Doc ID: CH-FC-001]) can operate in full free cooling mode when ambient temperatures are below [X\u00b0C], mixed mode, or full compressor mode. The BMS controls this transition to maximize economization. In [Year], this system provided [Y hours] of full or partial free cooling.",
        "humanInputAnswer": "Our 'Site Gamma' facility uses two 'VendorZ AquaChill-FC' free cooling chillers. These units have integrated free cooling coils that activate when outside air is below 8\u00b0C. Between 8\u00b0C and 14\u00b0C, they can operate in mixed mode. Above 14\u00b0C, they run as standard chillers. This approach, documented in 'Gamma_Chiller_Plant_Design.pdf', provides significant economizer hours while ensuring year-round cooling capacity for the CRAHs. Humidity control is managed at the CRAH level.",
        "llmGeneratedAnswer": "Yes, our 'Delta Wing' expansion features a chilled water plant with two 'ManufacturerCool EconoChill' units, which are free cooling chillers. As detailed in 'DW_Cooling_System_SoO_Rev1.pdf', these chillers utilize integrated free cooling coils when ambient temperatures drop below 10\u00b0C, producing chilled water for the CRAHs without compressor operation. For ambient temperatures between 10\u00b0C and 15\u00b0C, a 'mix mode' is employed, leveraging partial free cooling to reduce compressor load. Above 15\u00b0C, the chillers operate via their high-efficiency compressors. This design, while having a slightly larger approach temperature than direct water-side economizers, offers integrated free cooling benefits. The operating humidity in the data hall is maintained by the CRAHs.",
        "badExampleAnswer": "Our chillers are just regular chillers. We didn't know there were special 'free cooling' ones."
      }
    }
  },
  "step_9": {
    "description": "2025 Guide Section 5: Cooling (Practices 5.4.1.6-5.4.1.8 from Sec 5.4.1 'Free Cooling / Economised Cooling'; and 5.4.2.1-5.4.2.7 from Sec 5.4.2 'High Efficiency Cooling Plant')",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Cooling_4"
    ],
    "critical_questions": [],
    "questions": {
      "question_91": {
        "id": 91,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented an indirect water free cooling system where the cooling units' chilled water is cooled via a plate heat exchanger connected to a condenser water circuit, which in turn passes through external dry/adiabatic coolers or cooling towers? Is it understood that this design usually has the highest temperature difference (approach temperature) between the external ambient temperature and the IT supply air due to the additional heat exchange process, potentially limiting free cooling hours compared to other methods?",
        "simplifiedQuestion": "Do you use (or have you considered) indirect water free cooling where chilled water is cooled by a condenser water loop that uses external coolers/towers, understanding its higher temperature approach?",
        "reference": "2025 CoC Guide 5.4.1.6",
        "priority": "Low (Value: 3, Expected: Optional Practice)",
        "stepSection": "step_9",
        "recommendationTitle": "Indirect water free cooling with condenser water cooling chilled water",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "Feasibility studies or design documents for indirect water free cooling systems using a plate heat exchanger between chilled water and condenser water circuits. Specifications for heat exchangers, dry/adiabatic coolers or cooling towers. Control logic and operational data."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Optional: If implemented or assessed: Design documentation for indirect water free cooling systems using an intermediate plate heat exchanger with a condenser water loop. Details on control strategies and performance, noting the higher approach temperature.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Analysis"
        ],
        "verificationMethods": "Review of HVAC design documents and equipment specifications. Inspection of the plate heat exchanger, condenser water loop, and external heat rejection units if implemented. Analysis of BMS data on operating temperatures and free cooling hours.",
        "nonconformity": {
          "major": null,
          "minor": "This type of indirect water free cooling is implemented, but the system is not optimally controlled, or the high approach temperature significantly limits its effectiveness without due consideration during design (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: The indirect water free cooling system with an intermediate heat exchanger is implemented but operates inefficiently due to poor control, undersized components, or failure to account for the inherent higher temperature approach, resulting in fewer free cooling hours than potentially achievable with this design type.",
        "whatThisMeans": "Your organization could consider a type of indirect water free cooling where your main chilled water (for cooling units) is cooled using a separate water loop (condenser water). This condenser water loop then gets cooled by outside air using dry coolers or cooling towers. This method usually means the chilled water won't get as cold as with some other free cooling methods because of the extra heat exchange step, so you might get fewer free cooling hours. However, it completely separates the internal chilled water from the external loop.",
        "complianceRequires": "- Consider or implement an indirect water free cooling system where the cooling units' chilled water is cooled via a plate heat exchanger.\n- This plate heat exchanger transfers heat to a condenser water circuit.\n- The condenser water circuit then rejects heat to the atmosphere by passing through dry/adiabatic coolers or cooling towers.\n- Understand that this design usually results in the highest temperature difference (approach temperature) between the external ambient temperature and the IT supply air due to the additional heat exchange process (chilled water to condenser water, then condenser water to ambient air).\n- This higher approach temperature can limit the number of available free cooling hours compared to more direct free cooling methods.",
        "expertTip": "This 'dual-loop' free cooling approach can be beneficial where there's a need to completely isolate the primary chilled water loop from the external environment (e.g., for water quality reasons or existing infrastructure constraints). Ensure the plate heat exchanger is correctly sized for a close approach temperature to maximize effectiveness, and that both water loops have variable speed pumps optimized for the system's operation.",
        "recommendedEvidence": "- Design documentation for an indirect water free cooling system featuring a plate heat exchanger between chilled and condenser water loops.\n- Specifications for the plate heat exchanger, dry/adiabatic coolers or cooling towers, and pumps for both circuits.\n- Control logic describing how the system maximizes free cooling operation and integrates with mechanical cooling.\n- Operational data showing temperature differences across the heat exchanger and overall free cooling hours achieved.",
        "recommendedActions": "- Evaluate the suitability of this type of indirect water free cooling if complete hydraulic separation between internal chilled water and external heat rejection loops is a priority.\n- Carefully size the plate heat exchanger to achieve the closest possible approach temperature between the two water loops.\n- Optimize flow rates and control of both the chilled water and condenser water circuits to maximize heat transfer and free cooling effectiveness.\n- Implement regular maintenance for all components, including cleaning of the plate heat exchanger, dry coolers/cooling towers, and water treatment for the condenser loop.",
        "sampleResponseTemplate": "Our [Facility Name] employs an indirect water free cooling system where the primary chilled water loop is isolated from the condenser water loop via plate heat exchangers ([PHE Model]). The condenser water is cooled by [dry coolers/cooling towers]. While this design ([Design Doc ID: IWFC-PHE-001]) has a higher approach temperature, it was selected for [reason, e.g., 'glycol isolation']. We achieve approximately [X hours] of free cooling annually with this method.",
        "humanInputAnswer": "Our 'Site C' facility uses an indirect water free cooling system with a plate heat exchanger (Alfa Laval Model X) separating the internal chilled water loop from the external condenser water loop, which is served by BAC cooling towers. This design was chosen to protect the internal loop from potential contamination. We understand this results in a higher overall approach temperature, typically 5-6K above ambient wet-bulb, limiting full free cooling to periods below 10\u00b0C wet-bulb. The system is documented in 'SiteC_Cooling_System_Rev2.pdf'.",
        "llmGeneratedAnswer": "Yes, at our 'Heritage Data Centre', due to existing infrastructure constraints, we utilize an indirect water free cooling system where the primary data centre chilled water loop is cooled via a plate heat exchanger. This heat exchanger transfers heat to a secondary condenser water loop, which is then cooled by external evaporative cooling towers. As per our 'HDC_Cooling_Retrofit_Study.pdf', this configuration, while introducing an additional heat exchange step and thus a higher approach temperature (typically 4-6K higher than direct methods), was the most viable retrofit option. The system provides partial free cooling when ambient wet-bulb temperatures are between 8\u00b0C and 14\u00b0C, and full free cooling below 8\u00b0C.",
        "badExampleAnswer": "Our chillers make cold water, and that goes to the CRACs. We don't use any other heat exchangers for that."
      },
      "question_92": {
        "id": 92,
        "old_question_text": null,
        "questionText": "Has the organization evaluated alternative forms of cooling where they are available, practical to utilize, and offer genuine energy efficiency benefits? Examples include Ground Source Cooling (geothermal, from rivers, lakes, boreholes) and seawater cooling, subject to local environmental regulations and feasibility.",
        "simplifiedQuestion": "Have you evaluated using alternative cooling sources like ground/river/lake/seawater if available and practical?",
        "reference": "2025 CoC Guide 5.4.1.7",
        "priority": "Medium (Value: 4, Expected: Optional Practice)",
        "stepSection": "step_9",
        "recommendationTitle": "Alternative cooling sources",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Feasibility studies or evaluations of alternative cooling sources (geothermal, river/lake/seawater cooling). Design documents if such systems are implemented. Environmental permits or assessments related to their use."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: If assessed or implemented: Feasibility studies, design documents, and performance data for alternative cooling sources like ground source, river, lake, or seawater cooling. Documentation should include energy efficiency benefits and compliance with environmental regulations.",
        "verification_method": [
          "Document Review",
          "Interview",
          "Site Inspection (if implemented)"
        ],
        "verificationMethods": "Review of feasibility studies, design documents, and environmental permits if alternative cooling sources are used. Interviews with facilities and sustainability teams. Inspection of implemented systems.",
        "nonconformity": {
          "major": null,
          "minor": "Potentially viable alternative cooling sources (e.g., nearby water body, geothermal potential) have not been evaluated, or an evaluation was superficial and dismissed opportunities without thorough analysis (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to adequately evaluate locally available, practical, and potentially energy-efficient alternative cooling sources (e.g., ground source, river/lake/seawater cooling) or to implement them where feasibility studies indicate clear benefits and regulatory compliance.",
        "whatThisMeans": "Your organization should look into using natural or alternative sources for cooling your data centre if they are available nearby and it's practical to use them. This could include using the stable temperature of the ground (geothermal cooling), or water from rivers, lakes, or the sea. These methods can be very energy efficient but need careful evaluation for feasibility and must comply with environmental rules.",
        "complianceRequires": "- Evaluate alternative forms of cooling if they are locally available and practical to utilize.\n- Ensure that any considered alternative cooling sources offer genuine energy efficiency benefits.\n- Examples of such sources include:\n    - Ground Source Cooling (geothermal energy from boreholes, or using heat exchange with ground water).\n    - Cooling from rivers, lakes, or seawater (direct or indirect heat exchange).\n- Any implementation must be subject to local environmental regulations and permitting.",
        "expertTip": "When evaluating alternative cooling sources, engage with geological and environmental consultants early in the process to assess resource availability, sustainability, and regulatory requirements. Consider the lifecycle costs and benefits, including initial investment, operational savings, and maintenance needs. These systems often have higher upfront costs but can offer significant long-term opex savings and carbon reductions.",
        "recommendedEvidence": "- Feasibility studies or reports evaluating the potential for using alternative cooling sources (geothermal, river/lake/seawater, etc.).\n- If implemented: design documents, installation records, and performance data for the alternative cooling system.\n- Copies of environmental permits or assessments related to the use of such natural resources for cooling.\n- Energy savings analysis comparing the alternative cooling source to conventional cooling methods.",
        "recommendedActions": "- Identify potential alternative cooling sources available at or near the data centre location(s).\n- Conduct preliminary feasibility assessments considering technical viability, energy efficiency potential, environmental impact, regulatory constraints, and economic cost/benefit.\n- If promising, proceed with detailed engineering studies and pilot projects where appropriate.\n- Ensure any implemented system complies with all environmental regulations and sustainable resource management practices.",
        "sampleResponseTemplate": "We conducted a feasibility study in [Year] for our [Facility Name] regarding [e.g., geothermal cooling / river water cooling] (Ref: '[Study ID]'). The study found [Outcome, e.g., 'it was not economically viable due to X' / 'it is a promising option and is being considered for our next expansion phase']. We are aware of and would adhere to all local environmental regulations for such an implementation.",
        "humanInputAnswer": "For our 'Coastal Campus' data centre, we completed a feasibility study (Ref: 'CoastalDC_SeaWaterCooling_FS_2023.pdf') in 2023 for direct seawater cooling. The study indicated potential for significant energy savings (estimated PUE <1.1). We are currently in the environmental permitting and detailed design phase for a pilot project, with a target to implement a 1MW cooling capacity module by 2026. All local environmental regulations are being strictly followed.",
        "llmGeneratedAnswer": "Yes, our organization has evaluated alternative cooling sources. A comprehensive 'Alternative Cooling Feasibility Study (ACFS-2024)' was completed in Q1 2024 for our planned 'EcoBlue Data Hub'. This study assessed ground source (geothermal boreholes) and nearby river water cooling. While river water cooling showed high energy efficiency potential, environmental permitting complexities and seasonal water temperature variations led to its deferral. The geothermal option (closed-loop borehole system) was deemed technically feasible and is projected to provide up to 40% of annual cooling needs with significant energy savings. This option is now undergoing detailed engineering design, subject to full compliance with local environmental regulations (Ref: ACFS-2024, Section 5.2).",
        "badExampleAnswer": "We only use standard chillers; those other cooling methods seem too complicated and risky."
      },
      "question_93": {
        "id": 93,
        "old_question_text": null,
        "questionText": "Has the organization, for all new builds, retrofits, or upgrades of cooling systems, investigated and prioritized the installation of some form of free cooling or economized cooling capability, selecting the most appropriate type based on climate, facility design, IT load characteristics, and water availability (referencing practices 5.4.1.1 through 5.4.1.7 for different free cooling technology options)?",
        "simplifiedQuestion": "For all new cooling projects (builds, retrofits, upgrades), do you always investigate and prioritize installing some type of free/economized cooling?",
        "reference": "2025 CoC Guide 5.4.1.8",
        "priority": "High (Value: 5, Expected: New build or retrofit)",
        "stepSection": "step_9",
        "recommendationTitle": "Free Cooling installation",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Policy or design standard mandating investigation of free cooling for all new cooling projects. Feasibility studies for free cooling in recent projects. Design documents for new/upgraded cooling systems showing implemented free cooling technology."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds/retrofits/upgrades: Policy or design process that requires investigation and prioritization of free cooling. Documentation showing this investigation was done for recent projects, and details of any implemented free cooling solution.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of data centre design standards, cooling system upgrade project documentation, and feasibility studies for free cooling. Interviews with design engineers and facilities management.",
        "nonconformity": {
          "major": "New cooling systems are designed and installed without investigating or prioritizing free cooling/economization options, relying solely on mechanical refrigeration even where free cooling would be viable.",
          "minor": "Free cooling is considered but not always prioritized, or the selection of free cooling technology is not optimally matched to local conditions and facility needs."
        },
        "majorNonconformity": "Consistent failure to investigate or prioritize the installation of any form of free cooling or economized cooling capability during new builds, major retrofits, or cooling system upgrades, despite viable climatic or operational opportunities.",
        "minorNonconformity": "Free cooling is investigated for new cooling projects, but its implementation is not consistently prioritized where feasible, or the chosen free cooling technology is not the most appropriate or efficient for the specific site conditions and load profile.",
        "whatThisMeans": "Whenever your organization is building a new data centre, doing a major renovation, or upgrading its cooling systems, you must thoroughly investigate and give high priority to installing some type of 'free cooling' or 'economized cooling'. This means using cooler outside air (directly or indirectly) or other natural methods to cool the data centre instead of relying solely on energy-intensive chillers. The specific type of free cooling chosen should be the best fit for your local climate, building design, IT workload, and water availability (if water-based methods are considered).",
        "complianceRequires": "- For all new builds, significant retrofits, or upgrades of cooling systems, formally investigate the installation of free cooling or economized cooling capabilities.\n- Prioritize the implementation of such capabilities.\n- Select the most appropriate type of free cooling technology based on a comprehensive assessment of:\n    - Local climate conditions (temperature, humidity profiles).\n    - Facility design and constraints.\n    - IT load characteristics (density, variability).\n    - Water availability and cost (if considering evaporative or water-based systems).\n- Refer to the different free cooling technology options detailed in Practices 5.4.1.1 through 5.4.1.7.",
        "expertTip": "Develop a standardized 'Free Cooling Feasibility Assessment' to be completed for every new cooling project. This should include a climate data analysis (e.g., bin hours for temperature and humidity), evaluation of different free cooling technologies, and a lifecycle cost analysis comparing free cooling options against conventional mechanical cooling.",
        "recommendedEvidence": "- A corporate policy or data centre design standard that mandates the investigation and prioritization of free cooling for all new cooling system projects.\n- Feasibility studies or option analyses for recent new builds, retrofits, or cooling upgrades, showing the evaluation of different free cooling technologies.\n- Design documents for implemented projects detailing the selected free cooling system and the rationale for its choice.\n- Commissioning reports verifying the performance of newly installed free cooling systems.",
        "recommendedActions": "- Make the investigation and prioritization of free cooling a mandatory step in the design and approval process for all new data centre builds, major retrofits, and cooling system upgrades.\n- Ensure that the evaluation considers a range of appropriate free cooling technologies (as per 5.4.1.1 to 5.4.1.7) and selects the optimal solution based on site-specific factors.\n- Document the decision-making process, including why a particular free cooling solution was chosen or, in rare cases where it's deemed entirely unfeasible, clear justification for not implementing any free cooling.",
        "sampleResponseTemplate": "Our 'Data Centre Design Standard for New Builds & Retrofits' ([Doc ID: DC-DES-STD-003]) mandates a Free Cooling Feasibility Study for all cooling projects. For our recent [Project Name/Facility Upgrade], we evaluated [Types of Free Cooling] and selected [Implemented Type] because [Rationale]. This is projected to provide [X]% of our annual cooling needs via free cooling (Ref: '[Feasibility Study ID]').",
        "humanInputAnswer": "Our 'Cooling System Design Policy (CSDP-001)' requires a mandatory free cooling feasibility assessment for all new builds, retrofits, and major cooling plant upgrades. For the recent 'Data Hall 7' build-out, direct fresh air economization with high-efficiency filtration was investigated and selected as the primary cooling source, projected to meet 80% of annual cooling hours. This decision and the supporting climate analysis are documented in 'DH7_Cooling_Design_Report_v2.pdf'.",
        "llmGeneratedAnswer": "Yes, our organization's 'Capital Project Standard for Data Centres (CPSDC-2025)' mandates that all new builds, retrofits, or significant cooling system upgrades must include a formal investigation and prioritization of free cooling/economized cooling capabilities. Section 4.5 of this standard requires a 'Free Cooling Options Analysis' to be conducted, evaluating technologies listed in CoC Practices 5.4.1.1 through 5.4.1.7 against local climate data, facility design, projected IT loads, and water availability. For instance, the 'Omega Site Expansion Project - Cooling Upgrade Phase 1' (initiated Q1 2025) includes the installation of an indirect air-to-air heat exchanger system with adiabatic assist, selected after an analysis (Ref: 'OSE_Cooling_Upgrade_Feasibility_R1.pdf') showed it offered the best balance of efficiency and water use for our specific climate.",
        "badExampleAnswer": "We usually just replace cooling equipment with the latest version from our preferred vendor. Free cooling seems like it would only work in very cold places."
      },
      "question_94": {
        "id": 94,
        "old_question_text": null,
        "questionText": "When refrigeration is installed as part of the cooling system design (for new builds or retrofits), has the organization made the Coefficient of Performance (COP) or Energy Efficiency Ratio (EER) of chiller systems, evaluated across their likely working range of load and ambient conditions, a high priority decision factor during the procurement of new chiller plant?",
        "simplifiedQuestion": "When buying new chillers, is their energy efficiency (COP/EER) across expected working conditions a top priority in your decision?",
        "reference": "2025 CoC Guide 5.4.2.1",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_9",
        "recommendationTitle": "Chillers with high COP (EER)",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Chiller procurement policies or tender evaluation criteria emphasizing COP/EER across working range. Vendor submissions showing chiller COP/EER data (e.g., IPLV, NPLV, or custom load point efficiencies). Analysis comparing efficiency of different chiller options."
        },
        "docRequired": "Yes, if chillers installed/retrofitted",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new/retrofitted chiller plants: Procurement documents (policies, RFPs, evaluation matrices) showing that chiller Coefficient of Performance (COP) or Energy Efficiency Ratio (EER) across the *likely working range* is a high-priority selection criterion.",
        "verification_method": [
          "Document Review",
          "Procurement Record Audit"
        ],
        "verificationMethods": "Review of chiller procurement policies, tender specifications, and evaluation criteria. Audit of recent chiller purchase documentation to verify how COP/EER over the working range was assessed and prioritized.",
        "nonconformity": {
          "major": "New chillers are procured primarily based on capital cost or peak load capacity, with little or no prioritization of their COP/EER across the expected operational range.",
          "minor": "Chiller COP/EER is considered, but not as a high priority, or evaluation is based only on full-load COP/EER without considering part-load performance relevant to the facility's typical working range."
        },
        "majorNonconformity": "For new chiller plant installations or replacements, failure to make the Coefficient of Performance (COP) or Energy Efficiency Ratio (EER) across the *likely working range* a high priority decision factor in procurement.",
        "minorNonconformity": "Chiller efficiency (COP/EER) is considered in procurement, but primarily at full load conditions, without sufficient emphasis or detailed evaluation of performance across the anticipated part-load operating profile of the data centre.",
        "whatThisMeans": "If your data centre uses chillers for cooling (either for new installations or when replacing old ones), their energy efficiency should be a top factor when you decide which ones to buy. You need to look at how efficient they are (measured by COP or EER) not just at their maximum capacity, but across the range of cooling loads and outside temperatures they will typically experience in your facility. More efficient chillers use less electricity.",
        "complianceRequires": "- When refrigeration is installed (new build or retrofit), make the Coefficient of Performance (COP) or Energy Efficiency Ratio (EER) of chiller systems a high priority decision factor during procurement.\n- This evaluation of COP/EER should be conducted across their *likely working range* of operating conditions (i.e., considering various load levels and ambient temperatures they will experience), not just at a single design point or full load.",
        "expertTip": "Focus on integrated part-load value (IPLV) or non-standard part-load value (NPLV) ratings, or request custom part-load value (CPLV) calculations from vendors based on your facility's specific load profile and climate data. This gives a much better indication of real-world annual energy performance than full-load COP/EER alone.",
        "recommendedEvidence": "- Procurement policy for chillers that specifies COP/EER (including part-load metrics like IPLV/NPLV/CPLV) as a high-priority selection criterion.\n- RFP/tender documents for recent chiller purchases requiring vendors to provide detailed efficiency data across the expected operating range.\n- Evaluation matrix or report from a recent chiller procurement showing how COP/EER influenced the selection decision.\n- Manufacturer datasheets for installed chillers providing COP/EER values at various load points and operating conditions.",
        "recommendedActions": "- Update chiller procurement specifications to mandate high COP/EER across the anticipated working range (including part-load conditions) as a heavily weighted evaluation criterion.\n- Require vendors to provide certified IPLV/NPLV data or to calculate custom part-load values based on your site's specific operational profile.\n- Train procurement and engineering staff on how to evaluate and compare chiller efficiency data effectively.\n- Consider total lifecycle cost (including projected energy consumption based on efficiency) rather than just initial capital cost when selecting chillers.",
        "sampleResponseTemplate": "Our 'Chiller Procurement Standard' ([Doc ID: CHIL-PROC-002]) mandates that IPLV/NPLV (or site-specific CPLV) is a high-priority criterion, weighted at [X]% in evaluations. For our recent [Chiller Plant Y] upgrade, we selected [Vendor/Model Z] chillers which offered an IPLV of [Value] and were projected to save [Energy/Cost Amount] annually compared to less efficient alternatives, as detailed in '[Chiller Eval Report ID]'.",
        "humanInputAnswer": "Our standard for chiller procurement (Ref: 'MECH-PROC-CH-001') requires vendors to provide full-load COP and IPLV data. For our 'Data Hall 5' build, we selected 'VendorX TurboChill' units which had an IPLV of 9.5 (kW/kW), making efficiency across the working range a primary factor. This was weighted at 40% in our selection matrix, detailed in 'DH5 Chiller Selection Analysis'.",
        "llmGeneratedAnswer": "Yes, for all new chiller plant installations or retrofits, the Coefficient of Performance (COP) and/or Energy Efficiency Ratio (EER), particularly the Integrated Part Load Value (IPLV) or Non-Standard Part Load Value (NPLV) reflecting performance across the likely working range, is a high-priority decision factor. Our 'Mechanical Equipment Procurement Policy (MEPP-2025, Section 3.2)' mandates that chiller efficiency, weighted across anticipated load profiles, must account for at least 35% of the technical evaluation score. For example, the chillers selected for the 'Campus Expansion Project - Phase 1' were chosen based on superior NPLV ratings customized to our site's specific operating conditions, over units with marginally better full-load COP but poorer part-load performance.",
        "badExampleAnswer": "We buy chillers that are big enough for our needs and from brands we trust. We look at the COP if the vendor provides it."
      },
      "question_95": {
        "id": 95,
        "old_question_text": null,
        "questionText": "Has the organization evaluated opportunities to optimize its refrigeration cycle efficiency by decreasing condensing temperatures and increasing evaporating temperatures (where chillers are used)? Is it understood that reducing the temperature difference (Delta T or 'lift') between these condensing and evaporating temperatures means less work is required in the cooling cycle, hence improving efficiency, and that these temperatures are dependent on required IT equipment intake air temperatures and the quality of air flow management (as per Section 5.1 and 5.3)?",
        "simplifiedQuestion": "Do you try to optimize your chiller's refrigeration cycle by lowering condensing temperature and raising evaporating temperature to improve efficiency, considering your IT air needs and airflow management?",
        "reference": "2025 CoC Guide 5.4.2.2",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_9",
        "recommendationTitle": "Cooling system operating temperatures",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "Cooling system operational procedures or BMS control logic for managing condensing and evaporating temperatures. Analysis or reports showing reviews of these operating temperatures. Trend data of condensing/evaporating temperatures and associated chiller lift/efficiency."
        },
        "docRequired": "Yes, if chillers used",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Mandatory (if chillers are used): Documentation of efforts to optimize refrigeration cycle efficiency by managing condensing and evaporating temperatures (reducing 'lift'). This includes operational setpoints and any analysis linking these to IT air requirements and airflow management.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of chiller plant operational setpoints and control strategies for condensing/evaporating temperatures. Analysis of BMS trend data for these temperatures, chiller lift, and COP. Assessment of how these are linked to IT intake air temperature requirements and airflow management effectiveness.",
        "nonconformity": {
          "major": "Chiller condensing temperatures are kept unnecessarily high and/or evaporating temperatures unnecessarily low, without review or optimization, leading to significant chiller inefficiency.",
          "minor": "Opportunities to optimize condensing/evaporating temperatures are not systematically evaluated or implemented, or the link to IT air temperature needs and airflow management quality is not well understood or managed."
        },
        "majorNonconformity": "Consistent operation of chiller plant with unnecessarily high condensing temperatures and/or unnecessarily low evaporating temperatures, leading to excessive 'lift' and significantly reduced chiller efficiency, without evidence of evaluation or optimization.",
        "minorNonconformity": "While some effort may be made to manage chiller operating temperatures, there is no systematic evaluation of opportunities to decrease condensing temperatures (e.g., by optimizing cooling tower/dry cooler operation) or increase evaporating temperatures (e.g., by raising chilled water setpoints linked to good airflow management and appropriate IT intake temperatures).",
        "whatThisMeans": "To make your chillers run more efficiently, your organization should try to reduce the 'work' they have to do. This involves two main things: 1) making the condensing temperature (where the chiller rejects heat, often to cooling towers or dry coolers) as low as ambient conditions allow, and 2) making the evaporating temperature (which relates to how cold the chilled water needs to be) as high as possible while still meeting the IT equipment's cooling needs. The smaller the difference (or 'lift') between these two temperatures, the less energy the chiller uses. How high you can set your evaporating temperature depends on your IT equipment's allowed intake air temperature and how well you manage airflow in the data centre.",
        "complianceRequires": "- Evaluate opportunities to decrease condensing temperatures in the refrigeration cycle (e.g., by optimizing cooling tower or dry cooler performance).\n- Evaluate opportunities to increase evaporating temperatures in the refrigeration cycle (e.g., by raising chilled water setpoints, which is enabled by good airflow management (Section 5.1) and appropriate IT equipment intake air temperature settings (Section 5.3)).\n- Understand that reducing the temperature difference ('lift') between condensing and evaporating temperatures reduces the work required by the cooling cycle, thereby improving chiller efficiency (COP/EER).\n- Recognize that the achievable evaporating and condensing temperatures are dependent on the required IT equipment intake air temperatures and the effectiveness of the data centre's airflow management.",
        "expertTip": "Implement dynamic control strategies for both condenser water temperature (floating head pressure control for chillers) and chilled water temperature (reset based on load/ambient). This allows the system to continuously optimize the lift based on real-time conditions, rather than operating at fixed, worst-case setpoints.",
        "recommendedEvidence": "- BMS trend data or operational logs showing actual chiller condensing and evaporating temperatures (or corresponding water/refrigerant temperatures).\n- Control system setpoints for chilled water supply and condenser water supply/return.\n- Documentation of any strategies implemented to reduce condensing temperatures (e.g., cooling tower optimization, VSDs on tower fans) or raise evaporating temperatures (e.g., chilled water reset linked to IT load or ambient conditions).\n- Calculations or reports showing chiller 'lift' and its impact on COP/EER.",
        "recommendedActions": "- Implement monitoring of chiller condensing and evaporating temperatures (or their proxies like condenser/chilled water temperatures).\n- Analyze opportunities to optimize these temperatures: \n    - For condensing temperature: improve cooling tower/dry cooler performance, optimize fan speeds, ensure clean heat exchange surfaces.\n    - For evaporating temperature: raise chilled water setpoints as much as IT equipment and airflow management allow (see Practices 5.1 and 5.3).\n- Implement control strategies that dynamically adjust these setpoints based on ambient conditions and cooling load to minimize chiller lift and maximize efficiency.",
        "sampleResponseTemplate": "We optimize our chiller plant operating temperatures to minimize 'lift'. Our BMS ([System Name]) dynamically adjusts condenser water setpoints based on ambient wet-bulb, typically achieving [X\u00b0C approach]. Chilled water setpoints are reset based on [IT load / outside air temp], ranging from [Y\u00b0C to Z\u00b0C], as documented in '[Chiller Ops SOP ID]'. This is enabled by our effective airflow management (Section 5.1) and ASHRAE A2 compliant IT environment (Section 5.3).",
        "humanInputAnswer": "Our chilled water plant operation (Ref: 'CHWP_Ops_Manual_v2.1') includes strategies to minimize chiller lift. Condenser water temperature is reset based on ambient wet-bulb, typically maintaining a 3-4\u00b0C approach to wet-bulb via VSDs on our cooling tower fans. Chilled water supply temperature is reset based on IT return air temperature, ranging from 7\u00b0C to 12\u00b0C. This is possible due to our strict airflow management (hot aisle containment) and servers rated for ASHRAE A2. This has improved our average chiller COP by 0.8 (kW/kW).",
        "llmGeneratedAnswer": "Yes, our organization continuously evaluates and optimizes chiller operating temperatures. Our 'Chiller Plant Efficiency Protocol (CPEP-2025)' outlines procedures for this. Condensing temperatures are minimized by dynamically controlling cooling tower fan speeds (VFDs) to achieve a close approach (target <3K) to ambient wet-bulb temperature. Evaporating temperatures are maximized by operating with the highest possible chilled water setpoint (currently a dynamic range of 8\u00b0C-14\u00b0C) that still satisfies IT equipment intake requirements, as verified by our comprehensive airflow management (CoC Section 5.1) and temperature monitoring (CoC Section 5.3). This reduction in chiller 'lift' is a primary focus for energy efficiency, and its impact on COP is tracked monthly.",
        "badExampleAnswer": "The chillers are set to produce 7\u00b0C chilled water, and the cooling towers run to keep the condenser water from getting too hot. We don't really adjust these settings."
      },
      "question_96": {
        "id": 96,
        "old_question_text": null,
        "questionText": "Has the organization, for new builds or retrofits, optimized its cooling facility design for efficient operation under the partial load conditions it will experience for the majority of its operational time, rather than solely for maximum design load? Does this optimization include strategies such as exploiting available heat exchange surface area effectively, reducing fan energy consumption (e.g., through VSDs or optimized control), appropriate sequencing of chillers and other staged equipment (e.g., pumps, cooling towers), and operating cooling towers with shared load where applicable to maintain efficiency at lower loads?",
        "simplifiedQuestion": "For new/retrofitted cooling systems, is it designed to be efficient most of the time when it's not running at full capacity (i.e., good part-load efficiency)?",
        "reference": "2025 CoC Guide 5.4.2.3",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_9",
        "recommendationTitle": "Efficient part load operation",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Cooling system design philosophy emphasizing part-load efficiency. Equipment specifications showing good part-load performance (e.g., chiller IPLV/NPLV). Control strategies for equipment sequencing and load sharing. Analysis of expected load profiles and how design caters to them."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits: Design documentation demonstrating that the cooling facility is optimized for efficient part-load operation (not just peak load). This includes equipment selection criteria (e.g., IPLV/NPLV), strategies for heat exchange optimization, fan energy reduction, and intelligent sequencing/load sharing of staged equipment (chillers, pumps, towers).",
        "verification_method": [
          "Document Review",
          "System Data Analysis"
        ],
        "verificationMethods": "Review of cooling system design documents, equipment part-load efficiency data (e.g., IPLV/NPLV values for chillers), and control system logic for sequencing and load sharing. Analysis of operational data showing efficiency across varying load conditions.",
        "nonconformity": {
          "major": "New build/retrofit cooling facility design focuses solely on peak load efficiency, with no specific measures to ensure efficient operation at common part-load conditions, leading to significant energy waste during typical operation.",
          "minor": "Part-load efficiency is considered for some components but not as a holistic system design principle, or control strategies for staging and load sharing are not fully optimized."
        },
        "majorNonconformity": "For new builds or retrofits, cooling system design is primarily based on full-load efficiency with inadequate consideration or implementation of features that ensure efficient operation under partial load conditions, which are typical for most of the operational time.",
        "minorNonconformity": "While some components may have good part-load efficiency, the overall system design and control strategy (e.g., chiller sequencing, cooling tower load sharing, fan speed control) are not holistically optimized for efficient operation across the expected range of partial loads.",
        "whatThisMeans": "When designing a new data centre cooling system or upgrading an old one, it's important to make sure it runs efficiently not just at its maximum capacity, but also when it's less busy (at partial load), which is how it will operate most of the time. This means using strategies like making the most of your heat exchangers, reducing fan power, smartly turning chillers (and other equipment like pumps and cooling towers) on and off in the right sequence, and sharing the load effectively among operating cooling towers.",
        "complianceRequires": "- Optimize the cooling facility design for efficient operation under the partial load conditions it will experience for the majority of its operational time, not just for maximum design load.\n- Implement strategies such as:\n    - Effectively exploiting available heat exchange surface area (e.g., larger coils, optimized flow).\n    - Reducing fan energy consumption (e.g., through variable speed drives, low-pressure drop designs).\n    - Implementing appropriate sequencing for chillers and other staged equipment (e.g., pumps, cooling tower cells) to keep operating units in their most efficient range.\n    - Operating cooling towers with shared load where applicable to maintain efficiency at lower loads.",
        "expertTip": "Develop a detailed understanding of your facility's expected IT load profile throughout the day and across seasons. Use this profile to select cooling equipment (especially chillers) with high Integrated Part Load Values (IPLV) or Non-Standard Part Load Values (NPLV) that reflect your typical operating conditions. Implement control systems that can dynamically optimize the number of active units and their individual loading.",
        "recommendedEvidence": "- Cooling system design documents that explicitly state part-load efficiency as a key design criterion.\n- Equipment selection documentation showing preference for components with high part-load efficiency (e.g., chillers with high IPLV/NPLV, variable speed pumps/fans).\n- Control sequence of operations detailing strategies for chiller sequencing, pump staging, cooling tower fan control, and load sharing to optimize part-load efficiency.\n- Energy modeling or simulation results showing predicted system efficiency across various load conditions.",
        "recommendedActions": "- During the design phase for new builds or retrofits, model the expected IT load profile and specify cooling equipment that provides high efficiency across this profile, not just at peak load.\n- Select chillers, pumps, and fans with variable speed capabilities and good turndown ratios.\n- Implement advanced control strategies for sequencing staged equipment (chillers, cooling tower cells, pumps) to ensure operating units are loaded efficiently.\n- Optimize heat exchanger performance by ensuring adequate surface area and proper flow rates for typical part-load conditions.",
        "sampleResponseTemplate": "Our new cooling plant design for [Facility X] (Ref: '[Design Doc ID]') prioritizes part-load efficiency, as our IT load varies significantly. We selected [Chiller Model] with an IPLV of [Value] and utilize VSDs on all pumps and CRAC fans. Our BMS employs a [e.g., 'sequential load-based staging'] strategy for the [Number] chillers to ensure they operate within their optimal efficiency bands (typically [Y-Z]% load). Cooling towers also operate with shared load and VSD fans.",
        "humanInputAnswer": "Our 'Data Centre Cooling Design Standard (DCCDS-Rev4)' mandates optimization for part-load efficiency, as our facilities typically operate between 40-70% of peak IT load. For our recent 'West Wing' build, we selected chillers with high NPLV ratings specific to our climate profile, installed VFDs on all cooling tower fans and chilled water pumps, and implemented an optimized chiller sequencing strategy in our BMS that brings units online based on maintaining >50% load on active chillers. This approach is detailed in 'WW_Cooling_Efficiency_Strategy.pdf'.",
        "llmGeneratedAnswer": "Yes, the cooling system for our 'Aurora Data Centre', commissioned in 2024, was explicitly designed for efficient part-load operation. The 'Aurora Cooling System Design Basis (ACSDB-2023)' details how the selection of modular chillers with variable speed compressors, VFD-controlled pumps, and multi-cell cooling towers supports high efficiency across the anticipated 30-80% operational load range. The BMS control logic (Ref: 'Aurora_BMS_Cooling_SeqOps_v1.1') implements optimized chiller sequencing (N+1 staging based on maintaining >60% load on active units) and cooling tower cell control to maximize heat exchange surface area effectively at lower loads, significantly reducing fan and pump energy consumption during typical part-load conditions.",
        "badExampleAnswer": "Our cooling system is sized for the maximum possible IT load, so it has plenty of capacity. We turn on more chillers if it gets warmer."
      },
      "question_97": {
        "id": 97,
        "old_question_text": null,
        "questionText": "Has the organization, for new builds or retrofits, implemented variable speed control (e.g., Variable Frequency Drives - VFDs, or Electronically Commutated - EC motors) for compressors, pumps, and fans within the cooling system? Is it understood that this reduces energy consumption for these components in the part-load condition where they operate for much of the time? Is it also ensured that, in addition to installing variable speed drives, the system includes the ability to properly control their speed according to actual demand, rather than being manually set at a constant speed or having limited, ineffective control settings?",
        "simplifiedQuestion": "For new/retrofitted cooling, do you use variable speed drives (VFDs/EC motors) on compressors, pumps, and fans, and control them properly based on actual cooling demand?",
        "reference": "2025 CoC Guide 5.4.2.4",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_9",
        "recommendationTitle": "Variable speed drives for compressors, pumps and fans",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "screenshots"
          ],
          "description": "Specifications for cooling equipment (chillers, pumps, fans, CRACs/CRAHs) showing inclusion of VFDs/EC motors. Control system documentation detailing how VFD/EC motor speeds are controlled based on demand. BMS trend data showing variable speed operation."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, dwg, screenshots",
        "documentDescriptor": "Mandatory for new builds/retrofits: Specifications for cooling system components (compressors, pumps, fans) indicating use of VFDs or EC motors. Control system documentation showing how speeds are dynamically controlled based on demand, not fixed.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of equipment specifications and control system sequence of operations. Physical inspection of compressors, pumps, and fans for VFDs/EC motors. Review of BMS configuration and trend data for variable speed operation.",
        "nonconformity": {
          "major": "New build/retrofit cooling systems predominantly use fixed-speed compressors, pumps, and fans, or VFDs/EC motors are installed but are run at fixed speeds or with ineffective controls.",
          "minor": "VFDs/EC motors are used on some components but not all major variable load applications (compressors, pumps, fans), or their control strategy is not fully optimized for demand-based speed modulation."
        },
        "majorNonconformity": "For new builds or retrofits, failure to install variable speed drives (VFDs) or Electronically Commutated (EC) motors on major cooling system components (compressors, pumps, fans) that experience variable loads, or installation of such drives without effective demand-based speed control.",
        "minorNonconformity": "VFDs/EC motors are installed on some, but not all, appropriate cooling components, or the control strategy for modulating their speed based on actual demand is not fully optimized, leading to less than potential energy savings.",
        "whatThisMeans": "When building or upgrading your cooling system, major components like compressors (in chillers or AC units), water pumps, and fans should have variable speed controls (often called Variable Frequency Drives or VFDs, or Electronically Commutated - EC motors for fans). These allow the equipment to slow down when less cooling is needed, saving a lot of energy, as they often run at partial load. Just installing these drives isn't enough; they must be controlled properly by your Building Management System (BMS) to adjust speed automatically based on the actual cooling demand, not just set to run at a constant speed.",
        "complianceRequires": "- For new builds or retrofits, implement variable speed control (e.g., VFDs or EC motors) for cooling system compressors, pumps, and fans where these components experience variable loads.\n- Understand that variable speed control significantly reduces energy consumption in part-load conditions, which is typical for much of the operational time.\n- Consider new or retrofit of Electronically Commutated (EC) motors, which are significantly more energy efficient than traditional AC motors across a wide range of speeds, especially for fans.\n- Critically, ensure that in addition to installing variable speed drives, the system includes the capability to *properly control the speed according to actual demand*.\n- Avoid installing drives that are then manually set at a constant speed or have limited or ineffective control settings, as this negates the efficiency benefits.",
        "expertTip": "Ensure that VFDs are controlled based on relevant demand signals (e.g., differential pressure for pumps, temperature for CRAC fans, condensing temperature for chiller fans/compressors) rather than fixed speed setpoints. Implement PID control loops that are well-tuned to provide stable and responsive speed modulation.",
        "recommendedEvidence": "- Equipment schedules or specifications for new/retrofitted cooling systems listing VFDs or EC motors for compressors, pumps, and fans.\n- Control system documentation (sequence of operations, logic diagrams) detailing how the speed of these VFDs/EC motors is dynamically controlled based on demand signals (e.g., pressure, temperature, load).\n- BMS screenshots or trend data showing variable speed operation of these components in response to changing conditions.\n- Manufacturer datasheets for the VFDs/EC motors highlighting their efficiency characteristics.",
        "recommendedActions": "- Specify VFDs or EC motors for all new or replacement compressors, pumps, and fans in the cooling system that serve variable loads.\n- Ensure the Building Management System (BMS) or dedicated equipment controllers are programmed with effective control strategies to modulate speeds based on real-time demand.\n- Avoid manual or fixed-speed operation of VFDs/EC motors unless absolutely necessary for specific, temporary conditions.\n- Regularly review VFD/EC motor performance and control loop tuning to ensure optimal energy efficiency.",
        "sampleResponseTemplate": "All new cooling system components (chiller compressors, CHW/CW pumps, CRAH/cooling tower fans) installed in our [Facility Name] during the [Year] retrofit are equipped with [VFDs/EC motors], as per our '[M&E Design Standard ID]'. Speed is dynamically controlled by our BMS based on [e.g., 'differential pressure for pumps', 'supply air temperature for CRAH fans', 'condenser water temperature for tower fans'], as detailed in '[Control Logic Doc ID]'. We do not operate these in fixed-speed mode.",
        "humanInputAnswer": "In our 'Data Hall 6' expansion, all chilled water pumps, condenser water pumps, cooling tower fans, and CRAH supply fans are equipped with Variable Frequency Drives (VFDs). Our BMS (Ref: 'BMS Control Strategy DH6-CS-001') modulates pump speeds based on differential pressure setpoints and fan speeds based on cold aisle temperature and static pressure. For example, CRAH fan speeds typically vary between 40% and 85% depending on IT load. EC motors were specified for all new CRAH units.",
        "llmGeneratedAnswer": "Yes, variable speed control is a standard design requirement for our new builds and major retrofits. As per our 'Cooling System Design Specification (CSDS-2025, Section 4.2)', all primary pumps (chilled water, condenser water), cooling tower fans, and CRAC/CRAH fans are specified with Variable Frequency Drives (VFDs). Chiller compressors are specified with variable speed capability where appropriate for the chiller type and size. For smaller fan applications, Electronically Commutated (EC) motors are standard. The control system (documented in 'CSDS-2025, Section 7: Controls') ensures that these drives are modulated based on real-time demand signals (e.g., pressure, temperature, load) to optimize energy use, rather than operating at fixed speeds. For instance, our chilled water pump VFDs maintain a differential pressure setpoint that is dynamically reset based on cooling coil valve positions.",
        "badExampleAnswer": "Some of our new pumps have VFDs, but the operators usually set them to a speed that works well for most conditions."
      },
      "question_98": {
        "id": 98,
        "old_question_text": null,
        "questionText": "When designing new cooling systems or undertaking major retrofits, does the organization ensure that the cooling system designs chosen are those which facilitate the use of as much 'Free Cooling' or economization as is possible and practical, considering the physical site constraints and any local climatic or regulatory conditions that may apply? Is this selection based on an evaluation of whether direct or indirect air-side free cooling is feasible, or if a chilled liquid cooling system is required to allow effective use of economized cooling given site space or other constraints?",
        "simplifiedQuestion": "When designing/upgrading cooling, do you choose systems that maximize the potential for 'Free Cooling' based on your site and climate?",
        "reference": "2025 CoC Guide 5.4.2.5",
        "priority": "High (Value: 5, Expected: New build or retrofit)",
        "stepSection": "step_9",
        "recommendationTitle": "Select systems which facilitate the use of \"Free Cooling\"",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Cooling system design philosophy for new builds/retrofits that explicitly prioritizes maximizing free cooling potential. Feasibility studies comparing different free cooling options (direct air, indirect air, water-side economizers) based on site/climate. Design documents of the selected cooling system highlighting its free cooling capabilities."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits: Cooling system design documentation demonstrating that the selected system was chosen to facilitate maximum practical use of 'Free Cooling' or economization, considering site constraints, climate, and regulatory conditions. This should include evaluation of direct/indirect air-side options versus liquid-based economization.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of cooling system design basis, feasibility studies for free cooling options, and final design documentation. Interviews with design engineers and facilities management regarding the decision-making process for selecting cooling systems with free cooling capabilities.",
        "nonconformity": {
          "major": "New build/retrofit cooling system designs do not incorporate any free cooling or economization capabilities, or select technologies that actively hinder free cooling, despite viable climatic conditions and site feasibility.",
          "minor": "Free cooling is incorporated, but the chosen technology or design does not maximize the potential free cooling hours for the given site and climate, or the evaluation of different free cooling options was not comprehensive."
        },
        "majorNonconformity": "For new builds or retrofits, cooling system designs are selected that do not facilitate or actively hinder the use of 'Free Cooling' or economization, even when local climate and site conditions would make such strategies viable and highly beneficial.",
        "minorNonconformity": "While some form of free cooling may be included in new designs, the selection process did not thoroughly evaluate different types (direct/indirect air-side vs. liquid-based economization) to maximize potential based on specific site constraints, or the chosen system is not optimally configured to maximize free cooling hours.",
        "whatThisMeans": "When you're designing a new cooling system or making big changes to an old one, your organization must choose a design that allows you to use 'free cooling' (using cool outside air or water, instead of just energy-hungry chillers) as much as possible. This choice depends on your site's physical limitations, the local weather, and any rules you have to follow. You need to figure out if using outside air directly or indirectly is best, or if a chilled liquid system that can use an economizer mode is more suitable for your situation (e.g., if you don't have much space for big air systems).",
        "complianceRequires": "- Ensure cooling system designs chosen for new builds or major retrofits facilitate the use of as much 'Free Cooling' or economization as is practically possible.\n- Base this selection on a thorough evaluation of:\n    - Physical site constraints (e.g., space for air intakes/exhausts, location of external plant).\n    - Local climatic conditions (temperature and humidity profiles throughout the year).\n    - Any applicable local regulatory conditions or restrictions.\n- Evaluate whether direct or indirect air-side free cooling is feasible and most effective.\n- Alternatively, determine if a chilled liquid cooling system (which can incorporate water-side economizers) is required or more appropriate to allow effective use of economized cooling, especially if space for air-side systems is limited.",
        "expertTip": "Perform a detailed lifecycle cost analysis (LCCA) comparing different cooling system designs, explicitly factoring in the projected annual energy savings from their respective free cooling capabilities based on local climate data (e.g., TMY weather files). This helps justify potentially higher upfront costs for systems with greater free cooling potential.",
        "recommendedEvidence": "- Cooling system design philosophy or basis of design document for new builds/retrofits, stating the objective to maximize free cooling.\n- Feasibility studies or options analysis reports comparing different cooling technologies and their free cooling potential for the specific site and climate (e.g., direct air, indirect air with heat exchangers, water-side economizers integrated with chillers).\n- Design documents for the selected cooling system, highlighting the specific features that enable and maximize free cooling/economization.\n- Climate data analysis (e.g., bin hour reports) used to inform the selection of the free cooling strategy.",
        "recommendedActions": "- Make the maximization of free cooling/economization a primary design requirement for all new data centre cooling systems and major retrofits.\n- Conduct a thorough site and climate assessment to determine the most suitable and effective free cooling technologies (referencing Practices 5.4.1.1 to 5.4.1.7).\n- Evaluate both air-side and water-side economization options based on factors like space availability, water availability, air quality, and IT environmental requirements.\n- Document the rationale for the selected cooling system design and how it maximizes free cooling potential for the specific circumstances.",
        "sampleResponseTemplate": "Our 'Data Centre Cooling Design Standard' ([Doc ID: DC-CDS-004]) mandates that all new cooling systems must maximize free cooling. For our [Project/Facility Name], we conducted an options analysis ([Report ID: FC-OptAn-001]) considering [direct air/indirect air/water-side economizers]. We selected [Implemented System Type] due to [site constraints/climate suitability, e.g., 'limited plenum space favoring a chilled liquid system with waterside economizer integrated with our chillers']. This system is designed to achieve [X hours/Y%] of free cooling annually.",
        "humanInputAnswer": "Our standard design for new data centres (Ref: 'Global DC Design Guide v5') requires the primary cooling system to maximize free cooling hours. For our 'MidWest Hub' facility, due to local air quality concerns with direct air, we implemented an indirect air-to-air heat exchanger system with adiabatic assist (see Practice 5.4.1.2), as this facilitates extensive free cooling while protecting internal air quality. This was chosen after a comparative analysis (Doc: 'MWH_CoolingOptions_Study.pdf') against chilled liquid systems with economizers.",
        "llmGeneratedAnswer": "Yes, selecting systems that facilitate maximum 'Free Cooling' is a core principle of our cooling system design for new builds and retrofits, as mandated by our 'Sustainable Cooling Infrastructure Policy (SCIP-2025, Section 2.1)'. For each project, a 'Cooling Technology Options Analysis' is performed. This analysis explicitly evaluates direct air-side, indirect air-side, and various water-side economization strategies against site-specific constraints (e.g., space, air quality, water availability), local climatic data, and regulatory requirements. For instance, our 'Coastal Facility Retrofit' (completed 2024) opted for a closed-loop chilled liquid system with large waterside economizers (integrated dry coolers with adiabatic capability) because direct air-side options were limited by coastal air salinity and space for large air handlers. This choice (detailed in 'CFR_DesignReport_Cooling_v3.pdf') was made to maximize practical economized cooling hours.",
        "badExampleAnswer": "We use standard chillers and CRAC units. Free cooling is complicated and only works in very cold places."
      },
      "question_99": {
        "id": 99,
        "old_question_text": null,
        "questionText": "For new builds or retrofits in buildings principally designed to provide an appropriate environment for IT equipment with cooling systems designed to remove heat from technical spaces, has the organization ensured that data centre chilled water systems are NOT shared with human comfort cooling systems in other parts of the building? Is it understood that the lower temperatures required for comfort cooling (latent cooling) would compromise the efficiency of the data centre's sensible cooling system, and if comfort cooling remains a requirement in adjacent office areas, are alternative solutions like heat pumps considered for those office areas?",
        "simplifiedQuestion": "For new/retrofitted data centres, do you keep the data centre's chilled water system separate from any office/comfort cooling systems to maintain efficiency?",
        "reference": "2025 CoC Guide 5.4.2.6",
        "priority": "High (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_9",
        "recommendationTitle": "Do not share data centre chilled water system with comfort cooling",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "dwg",
            "docx"
          ],
          "description": "HVAC design documents for new builds/retrofits showing separation of data centre chilled water systems from comfort cooling systems. Specifications for separate comfort cooling systems (e.g., heat pumps for office areas) if applicable."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, dwg, docx",
        "documentDescriptor": "Mandatory for new builds/retrofits: HVAC design plans clearly showing separate chilled water systems for data centre technical spaces and any human comfort cooling. If comfort cooling is present, details of its independent system (e.g., heat pumps).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of HVAC system design drawings (P&IDs, schematics) to verify separation of chilled water loops. Physical inspection of plant rooms and distribution to confirm separate systems.",
        "nonconformity": {
          "major": "In new builds/retrofits, the data centre chilled water system is shared with human comfort cooling systems, forcing the data centre chillers to operate at lower, less efficient temperatures to satisfy comfort cooling demands.",
          "minor": "Chilled water systems are mostly separate, but some minor sharing exists, or alternative efficient solutions for comfort cooling (like heat pumps) were not adequately considered where separation was implemented."
        },
        "majorNonconformity": "For new builds or retrofits, the data centre's primary chilled water system is shared with human comfort cooling systems in other parts of the building, thereby forcing the data centre cooling plant to operate at lower, less efficient temperatures required for latent comfort cooling.",
        "minorNonconformity": "While largely separate, there are some shared connections or dependencies between the data centre chilled water system and comfort cooling, or the separate comfort cooling solution for office areas is not energy efficient (e.g., not considering heat pumps).",
        "whatThisMeans": "When building a new data centre or doing a major upgrade, if the building also has office spaces that need cooling, the chilled water system for the data centre's technical areas should be completely separate from the system used for human comfort cooling in those offices. Office cooling often needs much colder water (for dehumidification), which would make your data centre chillers run very inefficiently. If offices need cooling, consider separate systems for them, like heat pumps.",
        "complianceRequires": "- In buildings principally designed for IT equipment with cooling systems for technical spaces: do not share data centre chilled water systems with human comfort cooling in other parts of the building (for new builds or retrofits).\n- Understand that the required chilled water temperature to achieve latent cooling (dehumidification) for human comfort is substantially below that required for sensible cooling of the data centre.\n- Sharing systems will compromise the efficiency of the data centre cooling system by forcing it to produce unnecessarily cold water.\n- If comfort cooling is still required for adjacent office areas, consider using alternative, independent systems such as heat pumps to provide cooling (or heating) for those office areas.",
        "expertTip": "During the design phase, insist on completely separate chilled water generation and distribution loops for data centre technical cooling (optimized for higher temperatures, sensible cooling) and any office/comfort cooling (which has different temperature and humidity requirements). This ensures that the data centre cooling plant can be optimized for maximum efficiency without being dragged down by comfort cooling demands.",
        "recommendedEvidence": "- HVAC design drawings (P&IDs, schematics) for new builds/retrofits clearly showing separate and independent chilled water systems for the data centre technical spaces and any human comfort cooling zones.\n- Specifications for the data centre chilled water plant showing it's designed for sensible cooling at optimized (higher) temperatures.\n- If applicable, design documents and specifications for separate, energy-efficient comfort cooling systems (e.g., VRF, heat pumps) serving office areas within the same building.",
        "recommendedActions": "- Mandate in design standards for new builds/retrofits that data centre process cooling (chilled water) systems must be entirely separate from any human comfort cooling systems.\n- Optimize the data centre chilled water system for sensible cooling at the highest practical temperatures.\n- For any adjacent office or support spaces requiring comfort cooling, design and install dedicated, energy-efficient HVAC systems appropriate for human occupancy (e.g., consider heat pumps that can also leverage waste heat if available).",
        "sampleResponseTemplate": "Our new [Facility Name] (commissioned [Date]) has completely separate chilled water systems for the data halls and the adjoining office block, as detailed in '[HVAC Design Specification ID: HVAC-DS-001]'. The data halls use a high-temperature chilled water loop optimized for sensible cooling, while the office comfort cooling is provided by [e.g., a dedicated VRF system / air-source heat pumps].",
        "humanInputAnswer": "Our 'Data Centre Design Standard DC-STD-001' explicitly prohibits sharing of the data centre process chilled water loop with any comfort cooling systems. In our 'Tech Park Building B' which houses our new data centre and office space (completed 2024), the data centre has its own dedicated chilled water plant optimized for 12/18\u00b0C operation, while the office areas are served by a separate multi-zone VRF system for comfort cooling and heating. HVAC design 'TPB-HVAC-FINAL.dwg' shows this separation.",
        "llmGeneratedAnswer": "Yes, in our recent 'Building C - Data Centre Wing' retrofit (completed Q4 2024), the data centre chilled water system is entirely independent from the building's general comfort cooling system. As documented in 'Building_C_HVAC_Retrofit_Design_Rev2.pdf', the data centre utilizes a dedicated, high-efficiency chilled water plant optimized for sensible cooling with higher loop temperatures. The adjacent office spaces are served by a separate variable refrigerant flow (VRF) system for their comfort cooling needs. This separation ensures that the data centre's cooling efficiency is not compromised by the lower temperature and dehumidification demands typical of human comfort cooling. Heat pumps were evaluated and selected for the office VRF system to further enhance overall building energy performance.",
        "badExampleAnswer": "Our building has a central chiller plant that provides chilled water to both the offices and the data centre. It keeps everything cool."
      },
      "question_100": {
        "id": 100,
        "old_question_text": null,
        "questionText": "For new builds or retrofits, has the organization ensured that where other, non-IT equipment (e.g., specific M&E plant, specialized scientific instruments, etc.) located within or sharing the data centre environment requires a more restrictive temperature or humidity control range than the general IT equipment, this specific non-IT equipment is not permitted to dictate the setpoints of the main cooling system responsible for the bulk of the IT equipment? (This complements Practice 5.1.11 and 5.3.4 on segregating such equipment).",
        "simplifiedQuestion": "For new/retrofitted DCs, do you ensure that special non-IT gear needing tight climate control doesn't force your whole IT cooling system to be inefficient?",
        "reference": "2025 CoC Guide 5.4.2.7",
        "priority": "High (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_9",
        "recommendationTitle": "Do not allow non-IT equipment to dictate cooling system set-points",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Inventory of any non-IT equipment in/near the DC with restrictive environmental needs. Design documents showing how such equipment is environmentally managed separately (e.g., dedicated enclosures, localized conditioning) or how main cooling setpoints are protected from its influence."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits (if applicable): Documentation showing that any non-IT equipment with restrictive environmental needs does not dictate the main IT cooling system setpoints, ideally through segregation and dedicated environmental control (per 5.1.11/5.3.4).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of equipment inventories and their specified environmental requirements. Examination of data centre zoning plans and HVAC control strategies to ensure main IT cooling setpoints are not compromised by non-IT equipment with stricter needs.",
        "nonconformity": {
          "major": "In new builds/retrofits, the environmental setpoints for the entire data centre or large zones are dictated by a small amount of non-IT equipment with restrictive needs, leading to significant overcooling and energy waste for the majority IT equipment.",
          "minor": "Some non-IT equipment with restrictive needs influences main cooling setpoints, or segregation/dedicated control for such equipment is not fully effective."
        },
        "majorNonconformity": "For new builds or retrofits, the operating setpoints of the primary cooling system serving the IT equipment are determined by the more restrictive environmental requirements of co-located non-IT equipment, leading to inefficient operation for the bulk of the IT load.",
        "minorNonconformity": "While attempts are made to isolate non-IT equipment with special needs, its environmental conditions still have some influence on the main data hall setpoints, or the dedicated solutions for such equipment are not optimally efficient.",
        "whatThisMeans": "If you have special equipment in your data centre that isn't IT gear (like certain power equipment, lab instruments, etc.) and it needs much tighter temperature or humidity control than your servers, don't let that special equipment force your whole data centre cooling system to run inefficiently just to meet its needs. This type of equipment should ideally be in a separate area with its own climate control (as covered in other practices like 5.1.11 and 5.3.4).",
        "complianceRequires": "- For new builds or retrofits, ensure that if non-IT equipment requiring a more restrictive temperature or humidity control range than the general IT equipment is present, its requirements do not dictate the setpoints of the main cooling system serving the IT equipment.\n- This practice complements equipment segregation (5.1.11) and separate environmental zones (5.3.4) by focusing on the control setpoint aspect.\n- The primary goal is to prevent a small subset of equipment with niche requirements from forcing the entire data centre cooling infrastructure to operate sub-optimally.",
        "expertTip": "Identify any non-IT equipment with restrictive environmental needs early in the design phase. If it must be located within or near the data centre, provide it with its own dedicated, right-sized environmental control (e.g., a small, self-contained cooling unit or an environmentally controlled enclosure) rather than letting it influence the entire data hall's environment.",
        "recommendedEvidence": "- A list/inventory of all non-IT equipment located within or sharing the data centre environment, along with their specified operating environmental ranges.\n- Data centre zoning plans and HVAC design documents demonstrating that non-IT equipment with restrictive needs is either located outside the main IT areas or is served by independent environmental control systems.\n- BMS setpoint configurations showing that the main IT cooling system setpoints are optimized for the IT equipment, not for co-located non-IT equipment.",
        "recommendedActions": "- During the design of new facilities or retrofits, identify all equipment (IT and non-IT) that will be housed and document their environmental requirements.\n- If non-IT equipment has more restrictive needs than the general IT load, implement segregation and dedicated environmental controls for that non-IT equipment (as per 5.1.11 and 5.3.4).\n- Ensure that the control strategy for the main IT cooling system is based on the requirements of the IT equipment, not compromised by other co-located equipment.",
        "sampleResponseTemplate": "Our data centre design for [Facility Name] ensures that non-IT equipment does not dictate main cooling setpoints. For example, our [Specialized Non-IT Equipment, e.g., 'laser calibration unit'] which requires [tighter conditions] is housed in an [e.g., environmentally controlled cabinet with its own chiller / separate room with dedicated AC], as per drawing [Drawing ID]. This allows the main data hall to operate at energy-efficient ASHRAE A2 conditions.",
        "humanInputAnswer": "Our new facility design (Project Neptune, detailed in 'NEP-DESIGN-DOC-001') specifically isolates equipment with unique environmental needs. For instance, our metrology lab, adjacent to the data hall and requiring 20\u00b0C \u00b10.5\u00b0C, has its own dedicated HVAC system completely separate from the data hall's cooling, which targets ASHRAE A2 ranges. No non-IT equipment within the data hall dictates the cooling setpoints for IT equipment.",
        "llmGeneratedAnswer": "Yes, our 'Data Centre Environmental Zoning Policy (DCEZP-2025)' for new builds and retrofits mandates that any non-IT equipment with environmental requirements more restrictive than our standard IT equipment (which operates within ASHRAE A2 parameters) must not dictate the primary cooling system setpoints. Such equipment, for example, certain UPS battery chemistries requiring tighter temperature control or specialized monitoring apparatus, is either located outside the main data hall or within a dedicated, independently controlled micro-environment (e.g., a thermally isolated cabinet with its own spot cooling). This is consistent with practices 5.1.11 and 5.3.4 and ensures the main data hall cooling remains optimized for the bulk of the IT load. Our 'Site Gamma Expansion' design includes such a segregated zone for battery systems.",
        "badExampleAnswer": "We have some sensitive lab equipment in the corner of the data centre, so we have to keep the whole room a bit cooler for that."
      }
    }
  },
  "step_10": {
    "description": "2025 Guide Section 5: Cooling (Practices 5.4.2.8 from Sec 5.4.2; 5.5.1-5.5.6 from Sec 5.5; 5.6.1 from Sec 5.6; and 5.7.1-5.7.2 from Sec 5.7)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Cooling_5"
    ],
    "critical_questions": [],
    "questions": {
      "question_101": {
        "id": 101,
        "old_question_text": null,
        "questionText": "For new builds or retrofits using chilled water systems configured with dual pumps (one active, one standby), has the organization reviewed the potential for improved energy efficiency by operating both pumps in parallel at lower speeds versus a single pump at higher speed? If analysis of pump manufacturers' curves (energy use vs. pump speed) indicates a benefit, has this parallel low-speed pump operation been implemented to reduce energy usage while achieving the same target flow rate and pressure?",
        "simplifiedQuestion": "For new/retrofitted chilled water systems with dual pumps, have you checked if running both pumps at low speed is more efficient than one at high speed, and implemented it if so?",
        "reference": "2025 CoC Guide 5.4.2.8",
        "priority": "Low (Value: 1, Expected: New build or retrofit)",
        "stepSection": "step_10",
        "recommendationTitle": "Chilled water pump control strategy",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Analysis reports comparing energy use of single high-speed vs. dual low-speed pump operation for chilled water systems. Pump manufacturer curves. BMS control logic for pump operation if parallel low-speed mode is implemented."
        },
        "docRequired": "Yes, if applicable",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds/retrofits with dual active/standby chilled water pumps: Documentation of an analysis comparing the energy efficiency of running two pumps in parallel at lower speeds versus a single pump at higher speed. If beneficial, evidence of implementing parallel low-speed operation.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of any energy analysis conducted on chilled water pump operation strategies. Examination of pump curves and BMS control logic for pump staging and speed control.",
        "nonconformity": {
          "major": "In new builds/retrofits with dual chilled water pumps, no analysis has been performed to evaluate the efficiency of parallel low-speed operation versus single high-speed operation, potentially missing energy savings.",
          "minor": "Analysis was performed, but the more efficient parallel low-speed operation was not implemented despite demonstrated benefits, or the control strategy for parallel operation is not optimized."
        },
        "majorNonconformity": "For new builds or retrofits with dual chilled water pumps (active/standby), no evaluation has been performed to compare the energy efficiency of running both pumps in parallel at lower speeds against running a single pump at higher speed for the same duty.",
        "minorNonconformity": "An evaluation of parallel pump operation was conducted, and benefits were identified, but this strategy was not implemented or not optimally controlled, or the pump curves were not adequately analyzed to determine the most efficient operational mode.",
        "whatThisMeans": "If your new or upgraded chilled water system has two pumps where one is active and the other is a backup, you should check if running *both* pumps at a lower speed is more energy-efficient than running just one pump at a higher speed to achieve the same water flow and pressure. This involves looking at the pump manufacturer's performance data (pump curves). If running them in parallel at low speed is better, you should implement that control strategy.",
        "complianceRequires": "- For new builds or retrofits with chilled water systems configured with dual pumps (one active, one standby): review for improved energy efficiency during operation.\n- Using pump manufacturers' performance curves (energy use vs. pump speed), evaluate running two pumps in parallel at lower individual speeds versus a single pump running at a higher speed to achieve the same target flow rate and pressure.\n- If this analysis indicates a benefit, implement the strategy of running two (or more) pumps in parallel at lower speeds to reduce overall energy usage while meeting system demand.",
        "expertTip": "This strategy is most effective when pumps are equipped with Variable Speed Drives (VSDs). The pump affinity laws state that power is proportional to the cube of the speed, so running two pumps at 50% flow each (which might require around 63% speed if head varies with square of flow) can often use significantly less total power than one pump at 100% flow (100% speed). Ensure your control system can manage this parallel operation effectively.",
        "recommendedEvidence": "- Engineering analysis or study comparing the energy consumption of single pump operation versus parallel operation of two pumps at reduced speed for the installed chilled water system.\n- Pump manufacturer performance curves (head, flow, efficiency, power vs. speed) used in the analysis.\n- BMS control logic or sequence of operations for chilled water pump staging and speed control, showing implementation of parallel low-speed operation if deemed beneficial.\n- Operational data (pump speeds, power draw, flow rates) demonstrating the efficiency of the implemented pump control strategy.",
        "recommendedActions": "- For new chilled water systems with dual pump configurations, perform an energy efficiency analysis comparing single pump operation at higher speed vs. parallel operation of both pumps at lower speeds.\n- Utilize pump performance curves and system hydraulic characteristics for this analysis.\n- If parallel low-speed operation is found to be more energy-efficient for typical load conditions, implement this control strategy within the Building Management System (BMS).\n- Ensure pumps are equipped with VSDs to enable effective variable speed parallel operation.",
        "sampleResponseTemplate": "For our new chilled water plant ([Plant ID]), which has dual primary pumps ([Pump Model]), we conducted an analysis (Ref: '[Pump Strategy Analysis ID]') comparing single vs. parallel low-speed operation. The analysis, based on [Manufacturer X] pump curves, showed that parallel operation at [e.g., 60% speed each] is more efficient for loads between [Y% and Z%]. Our BMS control logic has been programmed to implement this strategy, resulting in estimated [Energy Saving]% pump energy reduction in this load range.",
        "humanInputAnswer": "Our 'Chilled Water Pumping Design Standard (CWPS-001)' for new builds requires an assessment of parallel versus single pump operation for N+1 primary pump sets. For our 'Site Epsilon' deployment, analysis of the 'VendorPump Model ABC' curves showed that running two pumps in parallel at approximately 65% speed each consumed 18% less energy than one pump at 90% speed for our typical system flow rate. This parallel operation strategy is now implemented in the BMS (Control Logic Ref: EPS_CHWP_CTRL_V1.2).",
        "llmGeneratedAnswer": "Yes, for our 'Data Centre Phoenix' new build, the chilled water system is configured with N+1 primary pumps (three units, two normally active). As part of the design (Ref: 'DCP_HVAC_Design_Report_Section5.4.pdf'), an analysis was performed using the manufacturer's pump curves (BrandX, Model Y). This analysis demonstrated that operating two pumps in parallel at approximately 70% speed each is more energy-efficient for our typical load profile than running a single pump at near 100% speed to achieve the same flow and head. Consequently, the BMS control strategy is configured for parallel pump operation with VFDs dynamically adjusting speed to meet demand, with the third pump on standby.",
        "badExampleAnswer": "We have two chilled water pumps; one runs and the other is a backup. We switch them over weekly."
      },
      "question_102": {
        "id": 102,
        "old_question_text": null,
        "questionText": "Has the organization, for new builds or retrofits, ensured that Computer Room Air Conditioner (CRAC) or Computer Room Air Handler (CRAH) units are equipped with variable speed fans (e.g., using VFDs or EC motors) instead of fixed speed fans? Is this implemented to address the substantial power consumption of fixed speed fans and to improve temperature management? Is it understood that variable speed fans are particularly effective where there is high cooling system redundancy, low facility utilization, or highly variable IT electrical load, and that these fans can be controlled by factors such as supply/return air temperature or chilled air plenum pressure? Is it also noted that CRAC/CRAH units with fixed speed compressors have minimum airflow requirements that may constrain the minimum operating load and therefore minimum fan airflow, which should be considered in their control strategy?",
        "simplifiedQuestion": "For new/retrofitted data centres, do your in-room AC units (CRACs/CRAHs) have variable speed fans controlled by demand, instead of fixed speed ones?",
        "reference": "2025 CoC Guide 5.5.1",
        "priority": "High (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_10",
        "recommendationTitle": "Variable Speed Fans",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "screenshots"
          ],
          "description": "Specifications for CRAC/CRAH units showing use of variable speed fans (VFDs/EC motors). Control system documentation detailing how fan speeds are modulated based on demand (temperature, pressure). BMS trend data showing variable fan speed operation."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, dwg, screenshots",
        "documentDescriptor": "Mandatory for new builds/retrofits: Specifications for CRAC/CRAH units confirming variable speed fans (VFDs or EC motors). Control logic detailing how fan speeds are modulated based on demand signals (e.g., temperature, pressure). Note on fixed speed compressor minimum flow constraints if applicable.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of CRAC/CRAH unit specifications and control system documentation. Physical inspection of fans for VFDs/EC motors. Review of BMS configuration and trend data for fan speed modulation.",
        "nonconformity": {
          "major": "New build/retrofit CRAC/CRAH units predominantly use fixed speed fans, or variable speed fans are installed but operated at fixed speeds or with ineffective controls.",
          "minor": "Variable speed fans are used, but their control strategy is not fully optimized for demand-based modulation, or minimum flow constraints for fixed speed compressors are not adequately managed."
        },
        "majorNonconformity": "For new builds or retrofits, CRAC/CRAH units are installed with fixed speed fans, or if variable speed fans are present, they are operated at a constant speed without dynamic control based on cooling demand.",
        "minorNonconformity": "Variable speed fans are installed on CRAC/CRAH units, but the control strategy is not optimally responsive to demand factors (e.g., temperature, plenum pressure), or the minimum airflow requirements for units with fixed speed compressors are not properly integrated into the control strategy, limiting turndown capability.",
        "whatThisMeans": "When building a new data centre or doing a major cooling upgrade, the air conditioning units in your computer rooms (CRACs or CRAHs) should have fans that can change their speed (using Variable Frequency Drives - VFDs, or special EC motors). Old fixed-speed fans use a lot of power all the time. Variable speed fans are much better because their speed can be adjusted based on how much cooling is actually needed (e.g., based on temperature or air pressure in the cold aisle). This is especially useful if your IT load changes a lot, if the data centre isn't full, or if you have a lot of backup cooling units. If the AC unit's compressor is fixed speed, it might need a minimum amount of airflow, which should be factored into how you control the fans.",
        "complianceRequires": "- For new builds or retrofits, equip CRAC/CRAH units with variable speed fans (e.g., using VFDs on AC motors or by using Electronically Commutated - EC motors).\n- Understand that fixed speed fans consume substantial power and can hinder effective data floor temperature management.\n- Recognize that variable speed fans are particularly effective in scenarios such as:\n    - High level of redundancy in the cooling system.\n    - Low utilization of the facility.\n    - Highly variable IT electrical load.\n- Control variable speed fans based on relevant factors such as supply or return air temperature, or chilled air plenum pressure.\n- Note and manage any minimum airflow requirements if CRAC/CRAH units have fixed speed compressors, as this can constrain the minimum operating load and thus minimum fan airflow.",
        "expertTip": "When controlling variable speed CRAC/CRAH fans, use differential pressure sensors in contained cold aisles as a primary control input. This allows fan speeds to be precisely matched to the IT equipment's actual airflow demand, minimizing bypass air and fan energy. Ensure control deadbands and response times are well-tuned to avoid hunting or instability.",
        "recommendedEvidence": "- Specifications for CRAC/CRAH units installed in new builds/retrofits, clearly indicating the use of variable speed fans (VFDs or EC motors).\n- Control system documentation (sequence of operations, logic diagrams) detailing how CRAC/CRAH fan speeds are modulated based on demand signals (e.g., temperature sensors, pressure sensors).\n- BMS trend data showing variable fan speeds correlating with changes in IT load or environmental conditions.\n- Documentation addressing how minimum airflow requirements for fixed-speed compressors (if applicable) are managed within the VSF control strategy.",
        "recommendedActions": "- Specify variable speed fans (VFDs or EC motors) as a standard requirement for all CRAC/CRAH units in new data centre builds and major cooling system retrofits.\n- Implement a control strategy that dynamically adjusts fan speeds based on real-time cooling demand indicators (e.g., cold aisle temperature, differential pressure in contained aisles, IT load).\n- If CRAC/CRAH units have fixed speed compressors, ensure the fan control strategy respects any minimum airflow requirements to protect the compressors, while still maximizing turndown.\n- Regularly review and optimize the control parameters for variable speed fans to ensure they are operating efficiently and effectively.",
        "sampleResponseTemplate": "All CRAC/CRAH units installed in our new [Facility/Data Hall Name] (commissioned [Date]) are equipped with [EC motors/VFDs] for variable speed fan operation, as per our '[Cooling Design Standard ID]'. Fan speed is controlled by our BMS based on [e.g., average cold aisle temperature and static pressure feedback from sensors in each pod]. For units with fixed speed compressors, minimum airflow is maintained at [X CFM] as per manufacturer guidelines when active.",
        "humanInputAnswer": "Our new 'Data Wing C', completed in 2024, features CRAH units exclusively equipped with EC variable speed fans. The BMS (Ref: 'BMS Control Logic DWC-HVAC-CL001') modulates fan speed based on a combination of cold aisle temperature sensors and differential pressure sensors within each contained pod. This ensures airflow is matched to the IT load, significantly reducing fan energy compared to our older fixed-speed units. For the few DX CRACs with fixed speed compressors in ancillary rooms, their VSD fans maintain a minimum airflow of 60% when the compressor is active.",
        "llmGeneratedAnswer": "Yes, all Computer Room Air Handler (CRAH) units deployed in our 'Project Aurora' new build (operational Q1 2025) are equipped with Electronically Commutated (EC) variable speed fans. As detailed in 'Aurora HVAC Design Specification AHD-002', the speed of these fans is dynamically modulated by the Building Management System (BMS) based on differential pressure sensors located within each contained cold aisle, maintaining a target static pressure. This demand-based control ensures optimal airflow delivery while minimizing fan energy consumption, particularly effective due to the facility's phased IT deployment and resulting variable initial loads. The CRAH units utilize variable speed compressors, thus avoiding minimum airflow constraints related to fixed speed compressors.",
        "badExampleAnswer": "Our CRACs have fans that run when the cooling is on. They are standard units that came with the chillers."
      },
      "question_103": {
        "id": 103,
        "old_question_text": null,
        "questionText": "Has the organization, for new builds or retrofits, implemented control of CRAC/CRAH units based on their supply air temperature rather than return air temperature? Is this approach adopted to ensure an even supply air temperature from each unit, independent of the specific cooling load on that individual CRAC/CRAH unit, which is considered more appropriate practice than historical return temperature control?",
        "simplifiedQuestion": "For new/retrofitted DCs, do you control your in-room AC units (CRACs/CRAHs) based on the temperature of the air they supply, not the air they suck back in?",
        "reference": "2025 CoC Guide 5.5.2",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_10",
        "recommendationTitle": "Control on CRAC / CRAH unit supply air temperature",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "screenshots"
          ],
          "description": "CRAC/CRAH control system documentation or BMS configuration showing control based on supply air temperature. Sequence of operations for CRAC/CRAH units. Setpoint documentation."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, screenshots",
        "documentDescriptor": "Mandatory for new builds/retrofits: Control system documentation (BMS configuration, sequence of operations) demonstrating that CRAC/CRAH units are controlled based on their supply air temperature setpoints, rather than return air temperature.",
        "verification_method": [
          "Document Review",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of CRAC/CRAH control strategy documentation and BMS configuration settings for temperature control loops. Verification that supply air temperature is the primary control variable.",
        "nonconformity": {
          "major": "New build/retrofit CRAC/CRAH units are primarily controlled based on return air temperature, leading to inconsistent supply air temperatures and potential inefficiencies.",
          "minor": "Control is primarily based on supply air temperature, but return air temperature still has significant influence, or setpoints are not well-optimized for stable supply conditions."
        },
        "majorNonconformity": "For new builds or retrofits, CRAC/CRAH units are predominantly controlled based on return air temperature, which is no longer considered appropriate practice.",
        "minorNonconformity": "While supply air temperature may be monitored, the primary control loop for CRAC/CRAH operation still relies significantly on return air temperature, or the supply air temperature setpoints are not consistently maintained or optimized.",
        "whatThisMeans": "When building a new data centre or upgrading cooling, your individual in-room air conditioning units (CRACs or CRAHs) should be controlled based on the temperature of the cold air they are *supplying* to the room, rather than the temperature of the warm air they are *sucking back in* (return air). Controlling based on supply air temperature helps ensure that each unit delivers consistently cold air, regardless of how much heat it's dealing with locally. This is now considered better practice than the older method of controlling based on return air.",
        "complianceRequires": "- For new builds or retrofits, implement control of CRAC/CRAH units based on their supply air temperature.\n- This ensures an even supply air temperature is delivered from each unit, independent of the specific cooling load on that individual unit.\n- Understand that controlling on return air temperature is a historical practice that is no longer considered appropriate or as efficient as supply air temperature control.",
        "expertTip": "Ensure that supply air temperature sensors are accurately calibrated and strategically placed within the CRAC/CRAH unit (or immediately at its discharge) to provide a true representation of the air being delivered. Combine supply air temperature control with cold aisle temperature feedback (from sensors in the cold aisle) to further optimize overall environmental conditions and efficiency.",
        "recommendedEvidence": "- Control system documentation (BMS sequence of operations, CRAC/CRAH controller settings) explicitly stating that units are controlled based on supply air temperature.\n- BMS screenshots showing setpoints and real-time readings for supply air temperature from CRAC/CRAH units.\n- Design documents for new/retrofitted cooling systems specifying supply air temperature control as the standard.",
        "recommendedActions": "- For all new CRAC/CRAH unit installations or control system upgrades, specify and implement control based on supply air temperature.\n- If existing units are controlled by return air temperature, evaluate the feasibility and benefits of converting them to supply air temperature control during a controls retrofit or unit replacement.\n- Ensure supply air temperature setpoints are coordinated with the overall cooling strategy and IT equipment requirements (e.g., target cold aisle temperatures).",
        "sampleResponseTemplate": "All CRAC/CRAH units in our new [Facility/Data Hall Name] (commissioned [Date]) are controlled based on their individual supply air temperature setpoints, as specified in our '[BMS Control Logic Doc ID: CRAC-CTRL-SUPPLY]'. The standard supply air setpoint is [X\u00b0C]. This ensures consistent cold air delivery irrespective of localized load variations.",
        "humanInputAnswer": "Our 'HVAC Control Standard HCS-004' for new data hall deployments mandates control of all CRAH units based on supply air temperature, not return. For 'Data Hall E', commissioned Q2 2024, each CRAH maintains a supply air temperature of 24\u00b0C \u00b10.5\u00b0C. This provides consistent cold air delivery to the cold aisles. The BMS configuration (Ref: 'BMS_Config_DHE_CRAH.pdf') shows the supply air sensor as the primary control input.",
        "llmGeneratedAnswer": "Yes, in our new data centre builds and retrofits since 2023, all Computer Room Air Handler (CRAH) units are controlled based on their discharge (supply) air temperature. This is a standard requirement in our 'Cooling Control Systems Design Guide (CCSDG-2025, Section 3.1)'. This method ensures a consistent supply air temperature from each unit, regardless of the individual load it is experiencing, which we find superior to the historical method of return air temperature control. For example, all CRAH units in our 'Omega Facility' maintain a supply air setpoint of 22\u00b0C, with adjustments made by the master BMS based on overall zone requirements.",
        "badExampleAnswer": "Our CRACs have a thermostat on the front that measures the room air coming back into them, and that's how they decide when to cool."
      },
      "question_104": {
        "id": 104,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented a strategy to achieve efficiency gains by running multiple CRAC/CRAH units (equipped with variable speed fans) in parallel at lower individual speeds, rather than fewer units at higher speeds, to achieve the required total air movement? Is it understood that electrical power is not linear with airflow, offering potential savings? If implementing such parallel operation via an additional control system, is care taken to understand and mitigate any new failure modes or single points of failure that may be introduced?",
        "simplifiedQuestion": "Do you run multiple variable-speed AC units (CRACs/CRAHs) together at lower speeds instead of fewer units at high speed to save energy, and have you checked for new risks from this setup?",
        "reference": "2025 CoC Guide 5.5.3",
        "priority": "Medium (Value: 4, Expected: Optional Practice)",
        "stepSection": "step_10",
        "recommendationTitle": "Run variable speed CRAC / CRAH units in parallel",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "Cooling system control philosophy or operational strategy for parallel operation of variable speed CRAC/CRAH units. Analysis or modeling showing energy efficiency benefits of this approach. Documentation of any additional control systems used for managing parallel operation and assessment of associated failure modes."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Optional: If implemented or assessed: Control strategy for running multiple variable speed CRAC/CRAH units in parallel at lower speeds. Energy analysis justifying this approach. Documentation of control system modifications and failure mode analysis if an additional control system is used.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of CRAC/CRAH control strategies for parallel operation. Analysis of BMS trend data showing fan speeds and power consumption of units running in parallel versus individual operation. Review of any FMEA for added control systems.",
        "nonconformity": {
          "major": null,
          "minor": "Variable speed CRAC/CRAH units are used, but the strategy of running more units in parallel at lower speeds for efficiency is not considered or implemented, or if implemented, potential new failure modes from added controls are not assessed (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued with variable speed CRAC/CRAHs: Failure to evaluate or implement the strategy of running more units in parallel at lower speeds, thereby missing potential fan energy savings. Or, if an additional control system is used for this, its potential impact on reliability (new failure modes) has not been assessed.",
        "whatThisMeans": "If you have multiple air conditioning units (CRACs or CRAHs) with fans that can change speed, it can be more energy-efficient to run several of them together at lower speeds rather than running fewer units at high speeds to move the same total amount of air. This is because the power used by fans doesn't increase proportionally with airflow (it usually increases much more steeply). If you use a special control system to manage this parallel operation, you need to make sure that system itself doesn't create new ways for things to fail.",
        "complianceRequires": "- Consider or implement a strategy for achieving efficiency gains by running CRAC/CRAH units equipped with variable speed fans in parallel at lower individual fan speeds.\n- This approach aims to achieve the required total air movement with less electrical power compared to running fewer units at higher fan speeds, due to the non-linear relationship between fan power and airflow (fan laws).\n- If an additional control system is implemented to manage this parallel operation, take care to understand and mitigate any new failure modes or single points of failure that may be introduced by this additional control layer.",
        "expertTip": "This strategy is most effective when the total airflow requirement is significantly below the capacity of a single unit running at full speed, or when high redundancy (e.g., N+2) is already in place. Use BMS controls to dynamically stage units and modulate their speeds in unison to maintain target environmental conditions while minimizing total fan power. Ensure the control logic prevents excessive cycling of units.",
        "recommendedEvidence": "- Control system documentation (sequence of operations) detailing the strategy for parallel operation of variable speed CRAC/CRAH units at reduced speeds.\n- Energy analysis or modeling that demonstrates the efficiency gains of this parallel operation strategy compared to other operating modes for the specific equipment and load profile.\n- BMS trend data showing multiple units operating in parallel at lower speeds and their combined power consumption.\n- If an additional control system is used: its specifications and any Failure Modes and Effects Analysis (FMEA) or risk assessment conducted for it.",
        "recommendedActions": "- Analyze the fan power curves and system airflow requirements to determine if running multiple variable speed CRAC/CRAH units in parallel at lower speeds is more energy efficient than running fewer units at higher speeds for your typical load conditions.\n- If beneficial, implement or update BMS control logic to enable and optimize this parallel operation mode.\n- Carefully design any supervisory control systems for parallel operation to ensure they are robust and do not introduce unacceptable risks or single points of failure.\n- Monitor the energy consumption and performance of units operating in parallel to verify savings and fine-tune controls.",
        "sampleResponseTemplate": "For our [Data Hall Z] which uses [Vendor/Model] variable speed CRAH units, we have implemented a parallel operation strategy controlled by our BMS (Ref: '[BMS Control Logic ID: CRAH-PARAL-001]'). For loads between [X% and Y%], the system runs [N] units at approximately [Z% speed] rather than [N-1] units at higher speeds. Analysis showed this reduces fan power by [P%]. We have assessed the control logic for new failure modes as part of our [Risk Assessment Process ID].",
        "humanInputAnswer": "Our 'Cooling Efficiency Protocol (CEP-004)' details our strategy for parallel operation of CRAH units. In data halls with variable speed fan CRAHs, when the total cooling demand is below 60% of the capacity of a single unit operating at 80% speed, the BMS automatically brings an additional unit online and reduces the speed of all active units to an optimized lower level (typically 40-55% speed each). This leverages the fan laws for energy savings. The FMEA for this control logic (Ref: 'FMEA-CRAH-CTRL-002') was reviewed and updated in 2024.",
        "llmGeneratedAnswer": "Yes, our organization employs a parallel operation strategy for our variable speed Computer Room Air Handler (CRAH) units. As documented in 'HVAC Operational Efficiency Guideline (HOEG-2025, Section 3.4)', when the data hall cooling load is below 70% of the capacity of a single unit at its optimal efficiency speed, the Building Management System (BMS) engages an additional CRAH unit. Both units then operate in parallel at significantly reduced individual fan speeds (typically 40-60%) to achieve the required total airflow. This method, based on the fan affinity laws, has demonstrated a 15-20% reduction in fan energy consumption compared to running fewer units at higher speeds for the same cooling delivery. The BMS control logic for this parallel operation has undergone a Failure Mode and Effects Analysis (FMEA Ref: BMS_CRAH_FMEA_2024) to ensure no critical single points of failure were introduced.",
        "badExampleAnswer": "We just turn on as many CRACs as we think we need to keep it cool. If one isn't enough, we turn on another one at full speed."
      },
      "question_105": {
        "id": 105,
        "old_question_text": null,
        "questionText": "In data centres where CRAC/CRAH units do not have variable speed fans (i.e., they are fixed speed), has the organization considered or implemented a strategy of 'sequencing' these units \u2013 turning entire CRAC/CRAH units on and off \u2013 to manage overall airflow volumes and cooling capacity in response to load? Is it understood that this can be effective where there is a high level of cooling redundancy, low facility utilization, or highly variable IT electrical load, but requires careful control to avoid rapid cycling or instability?",
        "simplifiedQuestion": "If your AC units (CRACs/CRAHs) have fixed-speed fans, do you turn whole units on/off in sequence to match cooling needs, especially if you have high redundancy or variable load?",
        "reference": "2025 CoC Guide 5.5.4",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_10",
        "recommendationTitle": "Sequencing of CRAC/CRAH units",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "screenshots"
          ],
          "description": "Cooling system control philosophy for fixed-speed CRAC/CRAH units detailing any sequencing strategy. BMS configuration or operational procedures for turning units on/off based on load or redundancy. Analysis of effectiveness if implemented."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, screenshots",
        "documentDescriptor": "Optional: If using fixed-speed CRAC/CRAH units, documentation of any control strategy for sequencing units (turning entire units on/off) based on load, especially in high redundancy or variable load scenarios.",
        "verification_method": [
          "Document Review",
          "BMS Configuration Review",
          "Interview"
        ],
        "verificationMethods": "Review of control strategies for fixed-speed CRAC/CRAH units. Examination of BMS settings or operational procedures for unit sequencing. Interviews with facilities staff on how fixed-speed units are managed.",
        "nonconformity": {
          "major": null,
          "minor": "Fixed-speed CRAC/CRAH units are used without any sequencing strategy, all running continuously regardless of load, or sequencing leads to excessive cycling (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued with fixed-speed CRAC/CRAHs: No strategy is implemented to sequence (turn entire units on/off) these units to manage airflow and cooling based on demand, particularly in environments with high redundancy or variable loads, leading to all units running unnecessarily.",
        "whatThisMeans": "If your data centre's in-room air conditioning units (CRACs or CRAHs) have fans that only run at one speed (fixed speed), you should consider a strategy where you turn entire units on or off as needed to match the cooling demand. This 'sequencing' can be useful if you have many spare cooling units, if your data centre isn't very full, or if your IT equipment's energy use changes a lot. It's a way to save energy when you don't need all fixed-speed units running.",
        "complianceRequires": "- In the absence of variable speed fans on CRAC/CRAH units, consider implementing a strategy to turn entire CRAC/CRAH units on and off (sequencing) to manage overall airflow volumes and cooling capacity.\n- Understand that this sequencing can be effective in scenarios with:\n    - A high level of redundancy in the cooling system.\n    - Low utilization of the facility.\n    - Highly variable IT electrical load.\n- Ensure that any sequencing strategy is carefully controlled to prevent rapid cycling of units, which can reduce equipment lifespan and cause instability.",
        "expertTip": "When implementing sequencing for fixed-speed units, use appropriate time delays and deadbands in your control logic to prevent short-cycling of compressors and fans. Rotate the lead/lag units regularly to equalize wear. This strategy is often a lower-cost interim step if upgrading to variable speed fans is not immediately feasible.",
        "recommendedEvidence": "- Control system documentation (BMS sequence of operations) detailing the logic for sequencing fixed-speed CRAC/CRAH units based on load or other parameters.\n- BMS trend data showing units cycling on and off in response to changing conditions, and the stability of environmental parameters.\n- Operational logs or procedures guiding manual sequencing if automation is not in place.\n- Justification for using sequencing in facilities with high redundancy or variable loads.",
        "recommendedActions": "- If fixed-speed CRAC/CRAH units are in use, evaluate the potential energy savings from implementing a sequencing strategy based on cooling demand.\n- Develop a control logic (automated via BMS or manual procedure) for turning units on/off, including appropriate time delays and interlocks to prevent rapid cycling.\n- Monitor the effectiveness of the sequencing strategy in maintaining environmental conditions while reducing unnecessary unit runtime.\n- Consider this as a less optimal alternative to variable speed fans (Practice 5.5.1) and plan for upgrades if feasible.",
        "sampleResponseTemplate": "For our legacy data hall ([Data Hall ID]) which has [Number] fixed-speed CRAC units, we have implemented a sequencing strategy via our BMS (Ref: '[BMS Control Logic ID: CRAC-SEQ-001]'). Based on [e.g., average return air temperature and N+1 redundancy requirements], units are brought online or taken offline with a [X minute] time delay. This is particularly effective during [low load periods like weekends/nights].",
        "humanInputAnswer": "In our older 'Site Alpha - Hall 1', which contains 8 fixed-speed CRAC units, our BMS employs a sequencing logic. Based on average return air temperature from 3 zones, units are staged on/off to maintain N+2 redundancy. A minimum run-time of 30 minutes and a minimum off-time of 15 minutes are enforced to prevent short cycling. This protocol (Ref: 'SA-H1-CRAC_Seq_SOP.pdf') reduces the number of active units during periods of lower IT load, such as overnight.",
        "llmGeneratedAnswer": "Yes, in our 'Development Lab Data Centre' which utilizes older fixed-speed CRAC units, we have implemented a sequencing control strategy. As detailed in 'DevLab_Cooling_Ops_Manual_Rev2.1', the Building Management System (BMS) sequences the CRAC units on and off based on a zone-based temperature voting logic and pre-defined N+1 redundancy requirements. This is effective due to the lab's highly variable IT load and existing N+2 redundancy. The control logic includes anti-short-cycling timers (minimum 20 minutes on/10 minutes off) to protect the compressors. This strategy allows us to reduce the number of concurrently operating fixed-speed units during low-load periods.",
        "badExampleAnswer": "All our CRACs run 24/7 at full speed to ensure there's always enough cooling available."
      },
      "question_106": {
        "id": 106,
        "old_question_text": null,
        "questionText": "For new builds or retrofits, has the organization ensured that humidity control (humidification and dehumidification) is centralized and primarily managed at the fresh 'Make Up' air handling unit (AHU) supplying air to the building, rather than at individual CRAC/CRAH units operating on re-circulating air within the equipment rooms? Is it understood that controlling humidity at the CRAC/CRAH unit is unnecessary and undesirable, and that centralized control at the make-up AHU allows for better overall control, the potential use of more efficient adiabatic humidification (with lower energy consumption), and potential additional free cooling opportunities? Is it also ensured that the chilled water loop or DX evaporator temperatures in CRACs/CRAHs are generally too high to cause significant dehumidification, and are new CRAC/CRAH units selected without humidity control capability (including reheat) to reduce capital and maintenance costs?",
        "simplifiedQuestion": "For new/retrofitted DCs, is humidity controlled centrally at the main building air intake (AHU) instead of by individual in-room AC units, and are new AC units bought without humidity controls?",
        "reference": "2025 CoC Guide 5.5.5",
        "priority": "High (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_10",
        "recommendationTitle": "Do not control humidity at CRAC/CRAH unit",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "screenshots"
          ],
          "description": "HVAC design documents showing centralized humidity control at the make-up AHU. Specifications for make-up AHUs with humidity control features (e.g., adiabatic humidifiers). CRAC/CRAH specifications showing absence of integrated humidifiers/dehumidifiers/reheat. Control strategy for humidity management."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, dwg, screenshots",
        "documentDescriptor": "Mandatory for new builds/retrofits: HVAC design showing centralized humidity control at the make-up air AHU. CRAC/CRAH specifications confirming they are purchased without integrated humidity control (including reheat). Explanation of how overall data hall humidity is managed.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of HVAC design philosophy, make-up AHU specifications, CRAC/CRAH specifications, and BMS control logic for humidity. Inspection of AHUs and CRAC/CRAH units.",
        "nonconformity": {
          "major": "In new builds/retrofits, humidity control is primarily performed by individual CRAC/CRAH units using energy-intensive methods (e.g., electric steam humidifiers, overcooling and reheat for dehumidification), instead of centralized control at the make-up AHU.",
          "minor": "Humidity control is partially centralized, but some CRAC/CRAH units still have active humidity control features that are not disabled or optimally coordinated, or new CRACs/CRAHs are still purchased with these redundant features."
        },
        "majorNonconformity": "For new builds or retrofits, humidity control (both humidification and dehumidification) is primarily managed at individual CRAC/CRAH units using re-circulating air, rather than being centralized at the fresh make-up air handling unit (AHU). New CRAC/CRAH units are procured with active humidity control and reheat capabilities.",
        "minorNonconformity": "While some make-up air humidity control exists, CRAC/CRAH units still have active (or enabled) local humidity control functions that are not optimally coordinated or are redundant. The principle of maintaining chilled water/DX evaporator temperatures too high for significant dehumidification at the CRAC/CRAH level is not consistently applied.",
        "whatThisMeans": "When building a new data centre or doing a major cooling upgrade, humidity should ideally be controlled centrally, usually where fresh 'make-up' air enters the building (at the main Air Handling Unit - AHU), not by individual CRAC/CRAH units inside the computer rooms that are mostly re-circulating air. Controlling humidity at each CRAC/CRAH is inefficient and often leads to units fighting each other. Central control is better and can allow for more energy-efficient methods like adiabatic humidification (using water evaporation, which can also provide some free cooling). Your CRAC/CRAH cooling coils should also be run at temperatures that are generally too warm to cause much dehumidification. When buying new CRAC/CRAH units, try to get models without built-in humidity controls (including any electric reheat for dehumidification) to save on cost and maintenance.",
        "complianceRequires": "- For new builds or retrofits, ensure humidity control is primarily managed at the fresh 'Make Up' air handling unit (AHU) supplying air into the building, not at individual CRAC/CRAH units operating on re-circulating air within equipment rooms.\n- Understand that humidity control at the CRAC/CRAH unit level is generally unnecessary and undesirable from an efficiency perspective.\n- Centralized humidity control at the make-up AHU provides better overall control and allows for the potential use of more energy-efficient adiabatic humidification (which can also offer some free cooling benefits).\n- Ensure the chilled water loop or DX evaporator temperatures in CRACs/CRAHs are set high enough to avoid significant dehumidification by these units.\n- When purchasing new CRAC/CRAH units, select models that are *not* equipped with integrated humidity control capability (including any reheat capability) to reduce both capital and ongoing maintenance costs.",
        "expertTip": "Design your make-up AHU with high-efficiency adiabatic humidification (e.g., wetted media, ultrasonic) and ensure it's controlled based on the overall data hall humidity requirements, considering the moisture load from IT equipment and occupants. This centralized approach is far more efficient than distributed electric steam humidifiers in CRACs.",
        "recommendedEvidence": "- HVAC design documents for new builds/retrofits showing that primary humidity control (humidification and strategy for dehumidification avoidance at CRACs) is centralized at the make-up air AHU(s).\n- Specifications for make-up AHUs detailing their humidification system (preferably adiabatic) and control sensors.\n- Specifications for CRAC/CRAH units showing they were procured *without* integrated humidifiers, dehumidification reheat coils, or local humidity controllers.\n- Operating setpoints for chilled water or DX evaporator temperatures demonstrating they are typically above the dew point of the data hall air to minimize dehumidification at CRACs/CRAHs.",
        "recommendedActions": "- In new designs, specify centralized humidity control via the make-up AHU, preferably using energy-efficient adiabatic humidification.\n- Design chilled water/DX systems and CRAC/CRAH coils to operate at temperatures that avoid unnecessary dehumidification within the data hall.\n- Procure CRAC/CRAH units without integrated humidity control features (humidifiers, dehumidification cycle, reheat) unless a specific, localized need is justified and cannot be met centrally.\n- Disable any existing redundant humidity controls at the CRAC/CRAH level if primary control is effectively managed centrally.",
        "sampleResponseTemplate": "Our new data centre's humidity control is centralized at the main make-up Air Handling Units (AHUs), which use [e.g., adiabatic spray humidifiers], as per design '[HVAC Design Doc ID: HVAC-HUMCTRL-001]'. CRAC/CRAH units (e.g., [Vendor/Model X]) in the data halls were specified and purchased without onboard humidity control or reheat. Chilled water temperatures are maintained above the typical data hall dew point to prevent CRAC-level dehumidification. This approach provides better control and efficiency.",
        "humanInputAnswer": "For our 'Project Aurora' new build, humidity control is centralized at the primary make-up AHUs using adiabatic wetted media humidifiers (Ref: 'Aurora HVAC Design Spec AUR-HVAC-002'). The CRAC units within the data halls (Model 'VendorCool FlowMax') were specifically ordered without integral humidifiers or reheat coils. Our chilled water temperature is maintained at a minimum of 12\u00b0C, which is generally above the data hall dew point, thus avoiding dehumidification at the CRAC level.",
        "llmGeneratedAnswer": "Yes, in our new facility designs (e.g., 'DataHub West', design completed 2024), humidity control is managed centrally via the make-up Air Handling Units (AHUs). These AHUs (detailed in 'DHW_AHU_Spec_Rev1.pdf') incorporate adiabatic humidification systems. The CRAC/CRAH units deployed within the data halls are specified without integral humidity control (humidifiers/dehumidifiers) or reheat capability, as per our 'Cooling Equipment Procurement Standard CEPS-2025'. Chilled water temperatures are maintained at levels that prevent significant dehumidification at the CRAH coils, typically above 10\u00b0C. This centralized approach at the AHU is more energy-efficient and provides better overall environmental stability.",
        "badExampleAnswer": "Each of our CRAC units has its own humidifier and can dehumidify if needed. They all work to keep the room at 50% humidity."
      },
      "question_107": {
        "id": 107,
        "old_question_text": null,
        "questionText": "When designing cooling systems for new builds or retrofits, does the organization carefully consider the sizing and selection of Computer Room Air Conditioner (CRAC) or Computer Room Air Handler (CRAH) units by factoring in not only the IT load (kW) but also the IT equipment Delta-T (temperature difference between intake and exhaust air), which can vary with utilization? Is it understood that overestimating IT Delta-T can result in undersized CRAC/CRAH air volumes and potential air management problems, while underestimating it can lead to oversized CRAC/CRAH air volumes, making low part-load inefficient operation and air bypass more likely?",
        "simplifiedQuestion": "When choosing in-room AC units (CRACs/CRAHs) for new/retrofitted DCs, do you properly size them considering not just IT power (kW) but also IT equipment's air temperature rise (Delta-T)?",
        "reference": "2025 CoC Guide 5.5.6",
        "priority": "High (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_10",
        "recommendationTitle": "Cooling unit sizing and selection",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "dwg"
          ],
          "description": "Cooling design calculations for CRAC/CRAH sizing, showing consideration of IT load (kW) and design IT equipment Delta-T. Specifications for selected CRAC/CRAH units matching these design calculations. Any analysis of IT equipment Delta-T variability."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, xlsx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits: CRAC/CRAH sizing calculations that explicitly consider both IT load (kW) and a realistic design IT equipment Delta-T. Justification for the chosen Delta-T value and its impact on required air volumes.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of cooling design calculations, CRAC/CRAH sizing methodology, and specifications for installed units. Interviews with design engineers regarding assumptions for IT load and Delta-T.",
        "nonconformity": {
          "major": "CRAC/CRAH units are sized based solely on IT load (kW) without proper consideration of IT equipment Delta-T, leading to significantly mismatched airflow capacity (either too much or too little) and inefficient operation.",
          "minor": "IT Delta-T is considered in CRAC/CRAH sizing, but assumptions are not well-justified, or the impact of Delta-T variability with IT utilization is not adequately factored into selection and control."
        },
        "majorNonconformity": "For new builds or retrofits, CRAC/CRAH unit sizing and selection is based predominantly on IT load (kW) without adequate consideration of the design IT equipment Delta-T (temperature difference across IT equipment), leading to incorrectly sized air volumes.",
        "minorNonconformity": "IT equipment Delta-T is considered in CRAC/CRAH sizing, but the assumed Delta-T value is not well-justified based on actual or expected IT equipment characteristics, or the impact of variable IT utilization on Delta-T and required airflow is not fully accounted for in the unit selection or control strategy.",
        "whatThisMeans": "When you choose the size and number of air conditioning units (CRACs or CRAHs) for a new or upgraded data centre, don't just look at the total power (kW) of the IT equipment. You also need to consider the 'Delta-T' \u2013 how much the air temperature rises as it passes through the IT gear. This Delta-T can change depending on how busy the IT equipment is. If you guess the Delta-T too high, your AC units might not move enough air, causing problems. If you guess too low, you'll have too much airflow capacity, which is inefficient when the IT load is low and can lead to air bypassing the equipment.",
        "complianceRequires": "- When sizing and selecting CRAC/CRAH units for new builds or retrofits, consider not only the IT load (kW) but also the design IT equipment Delta-T (temperature difference between IT equipment intake and exhaust air).\n- Understand that IT equipment Delta-T will vary with utilization.\n- Recognize the consequences of misestimating Delta-T:\n    - Overestimating IT Delta-T can result in undersized CRAC/CRAH air volumes and potential air management problems (insufficient airflow).\n    - Underestimating IT Delta-T can result in oversized CRAC/CRAH air volumes, making low part-load operation inefficient and increasing the likelihood of bypass air.",
        "expertTip": "Use a design Delta-T for IT equipment that is realistic for your planned hardware and expected utilization levels (modern servers often have higher Delta-Ts, e.g., 10-15\u00b0C or more). Ensure CRAC/CRAH units are selected with variable speed fans (Practice 5.5.1) to allow airflow to be modulated effectively as actual IT load and Delta-T vary over time.",
        "recommendedEvidence": "- Cooling design calculations for CRAC/CRAH sizing that clearly show the assumed IT load (kW) and the design IT equipment Delta-T (\u00b0C or \u00b0F) used to determine required airflow (CFM or m\u00b3/h).\n- Justification or source for the design Delta-T value used (e.g., based on planned IT equipment specifications, industry best practices for similar equipment).\n- Specifications of the selected CRAC/CRAH units, including their airflow capacity and how it matches the calculated requirement.\n- Consideration of how IT equipment Delta-T might vary with utilization, and how the cooling system design (e.g., VSFs) accommodates this.",
        "recommendedActions": "- During the cooling design phase, carefully determine a realistic design Delta-T for the planned IT equipment, considering its specifications and expected operational characteristics.\n- Use this design Delta-T, along with the design IT load (kW), to accurately calculate the required total airflow for the data centre space.\n- Select and size CRAC/CRAH units (number and capacity) to efficiently deliver this required airflow, preferably using units with variable speed fans to adapt to variations in actual load and Delta-T.\n- Avoid historical rules of thumb for airflow (e.g., CFM/kW) that do not account for modern IT equipment's higher Delta-Ts.",
        "sampleResponseTemplate": "Our CRAC/CRAH unit sizing for the new [Data Hall Alpha] was based on a design IT load of [X kW] and an assumed average IT equipment Delta-T of [Y\u00b0C], as documented in '[Cooling Design Calculations Report ID: CDC-RPT-001]'. This resulted in a required total airflow of [Z CFM], met by [Number] [Vendor/Model] CRAH units. We selected units with variable speed fans to accommodate Delta-T variations with IT utilization.",
        "humanInputAnswer": "For our 'Omega Project' data hall design, CRAC unit sizing (Ref: 'Omega_Cooling_Calc_v2.xlsx') was based on a phased IT load build-out and a target IT equipment Delta-T of 12\u00b0C, based on specifications from our preferred server vendors for equipment operating at 50-70% utilization. This design Delta-T, rather than just kW, determined the required CRAH airflow per kW. This led to selecting fewer, larger CRAH units with VFDs than if we had used an older, lower Delta-T assumption.",
        "llmGeneratedAnswer": "Yes, the sizing and selection of Computer Room Air Handler (CRAH) units for our new builds and retrofits explicitly considers both the IT load (kW) and the design IT equipment Delta-T. Our 'Cooling Design Standard (CDS-2025, Section 6.1)' specifies a standard design Delta-T of 11K (19.8\u00b0F) for typical server deployments, based on current generation hardware. This value is used in conjunction with the projected kW load to calculate the required airflow volume. For example, in the 'Sigma Wing' expansion, this methodology (documented in 'Sigma_Cooling_Sizing_Report.pdf') ensured CRAH units were selected with appropriate airflow capacity to match the IT demand, avoiding issues related to either overestimation (leading to undersized air volume) or underestimation (leading to oversized, inefficient units and bypass) of the IT Delta-T. The variability of Delta-T with IT utilization is also a factor in our VFD control strategy for the CRAHs.",
        "badExampleAnswer": "We size our CRACs based on the total kW of the IT equipment we plan to install, using a standard rule of thumb for CFM per kW."
      },
      "question_108": {
        "id": 108,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented direct liquid cooling (DLC) for part or all of some IT devices as an alternative to air cooling, particularly for high-density deployments? Is it understood that DLC can provide a more efficient thermal circuit, allow for substantially higher coolant liquid system temperatures (further driving efficiency and enabling increased/exclusive use of free cooling or heat reuse)? Does this practice specifically apply to systems delivering cooling liquid directly to heat-removing components (e.g., water-cooled heat sinks, heat pipes) and not to internal mechanical refrigeration or in-chassis air cooling? Are relevant ASHRAE guidelines (e.g., 'Liquid Cooling Guidelines for Datacom Equipment Centers') referenced or followed if DLC is implemented?",
        "simplifiedQuestion": "Have you considered or used direct liquid cooling for IT gear (especially high-density) to improve efficiency and enable heat reuse, following relevant guidelines?",
        "reference": "2025 CoC Guide 5.6.1",
        "priority": "Medium (Value: 4, Expected: Optional Practice)",
        "stepSection": "step_10",
        "recommendationTitle": "Direct liquid cooling of IT devices",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "Feasibility studies for Direct Liquid Cooling (DLC). Design documents if DLC is implemented (e.g., IT equipment specifications, CDU specifications, piping, heat rejection methods). Operational data showing performance and efficiency of DLC systems. Reference to ASHRAE liquid cooling guidelines."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Optional: If direct liquid cooling (DLC) is implemented or assessed: Design and specification documents for DLC systems (IT equipment, CDUs, etc.). Operational data on coolant temperatures, energy efficiency, and heat reuse. Adherence to ASHRAE liquid cooling guidelines.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Analysis"
        ],
        "verificationMethods": "Review of feasibility studies or design documents for DLC systems. Inspection of any implemented DLC infrastructure (CDUs, piping, IT equipment modifications). Analysis of operational data on DLC system performance and efficiency.",
        "nonconformity": {
          "major": null,
          "minor": "DLC is implemented but not optimally designed or controlled, leading to lower than expected efficiency gains, or ASHRAE guidelines are not followed, potentially impacting reliability (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Direct liquid cooling is implemented, but the system design, operational parameters (e.g., coolant temperature), or maintenance practices do not align with recognized guidelines (e.g., ASHRAE), or the potential for heat reuse is not effectively exploited.",
        "whatThisMeans": "Your organization should explore using 'direct liquid cooling' (DLC) for some or all of your IT equipment, instead of just air cooling. This can be especially useful for very power-hungry, high-density setups. DLC can be more efficient because it brings liquid coolant directly to the hot components (like CPUs via water-cooled heat sinks). This often allows the cooling liquid to be warmer than traditional chilled water, which boosts overall system efficiency, makes it easier to use 'free cooling' (using outside ambient conditions), and improves the potential to reuse the captured heat. This is different from systems that have their own little air conditioners or refrigeration units inside the IT gear. If you use DLC, you should follow established guidelines, like those from ASHRAE.",
        "complianceRequires": "- Consider or implement direct liquid cooling (DLC) for part or all of some IT devices as an alternative to air cooling.\n- Recognize that DLC can provide a more efficient thermal circuit and allow coolant liquid system temperatures to be substantially higher, which can:\n    - Further drive overall cooling system efficiency.\n    - Allow for increased or exclusive use of free cooling.\n    - Improve opportunities for heat reuse.\n- Understand that this practice applies to devices where cooling liquid is delivered *directly* to the heat removal system of the IT components (e.g., water-cooled cold plates on CPUs/GPUs, immersion cooling, heat pipes).\n- This practice does *not* apply to IT equipment with internal mechanical refrigeration plants or in-chassis air cooling systems that are themselves liquid-cooled indirectly.\n- If implementing DLC, refer to and follow relevant industry guidelines, such as ASHRAE's 'Liquid Cooling Guidelines for Datacom Equipment Centers, Second Edition'.",
        "expertTip": "When evaluating DLC, focus on applications with high heat flux components (CPUs, GPUs) or very high rack densities where air cooling becomes challenging or inefficient. Ensure that the selected DLC solution is compatible with your IT equipment and that you have a plan for managing the liquid cooling infrastructure, including coolant quality, leak detection, and maintenance. Higher return water temperatures from DLC systems significantly enhance heat reuse possibilities.",
        "recommendedEvidence": "- Feasibility studies or design documents for implemented or considered direct liquid cooling solutions.\n- Specifications for the DLC-enabled IT equipment and the supporting liquid cooling infrastructure (e.g., Coolant Distribution Units - CDUs, heat rejection systems).\n- Operational data showing coolant temperatures (supply and return), flow rates, and energy consumption of the DLC system.\n- Evidence of referencing or adhering to ASHRAE liquid cooling guidelines in the design and operation of the DLC system.\n- Plans or implemented systems for reusing heat captured by the DLC system.",
        "recommendedActions": "- For high-density IT deployments or where air cooling is reaching its limits, evaluate the technical and economic feasibility of direct liquid cooling solutions.\n- If DLC is adopted, select appropriate DLC technologies (e.g., direct-to-chip, immersion) based on IT equipment types and facility capabilities.\n- Design the DLC system in accordance with industry best practices and guidelines (e.g., ASHRAE).\n- Ensure proper integration of the DLC system with the facility's overall heat rejection and potentially heat reuse infrastructure.\n- Train staff on the operation and maintenance of DLC systems.",
        "sampleResponseTemplate": "We have implemented direct liquid cooling for our [High-Performance Computing (HPC) cluster / AI supercomputer] using [Vendor/Type of DLC system, e.g., 'VendorX direct-to-chip cold plates']. This system ([Design Doc ID: DLC-HPC-001]) operates with a supply coolant temperature of [e.g., 30\u00b0C] and allows us to [e.g., 'capture high-grade waste heat for building heating / operate with 100% free cooling via dry coolers year-round']. We follow ASHRAE's liquid cooling guidelines in its operation and maintenance.",
        "humanInputAnswer": "Our new 'DeepLearn AI' cluster utilizes direct-to-chip liquid cooling with warm water (35\u00b0C supply) for the GPUs and CPUs. This system, designed according to ASHRAE's Liquid Cooling Guidelines (2nd Ed.), allows us to achieve a chip-level PUE contribution close to 1.05 for the cooled components. The heat captured is being used to pre-heat water for our adjacent office building's heating system. All design and operational parameters are documented in 'DLC_System_DeepLearn_v1.pdf'.",
        "llmGeneratedAnswer": "Yes, our organization has deployed direct liquid cooling (DLC) for our dedicated High-Performance Computing (HPC) environment. As detailed in 'HPC_Cluster_Cooling_Design_Rev2.pdf', this involves a direct-to-chip, warm-water cooling solution serving the primary heat-generating components (CPUs and GPUs). This system allows for significantly higher coolant temperatures (supply at 30\u00b0C, return at 45\u00b0C) compared to traditional air or chilled water cooling, which facilitates highly efficient heat rejection via dry coolers and offers viable opportunities for low-grade heat reuse (currently under investigation for adjacent office heating). The design and operation adhere to recommendations from ASHRAE's 'Liquid Cooling Guidelines for Datacom Equipment Centers, Second Edition'. This system is distinct from our general IT areas which remain air-cooled within ASHRAE A2 parameters.",
        "badExampleAnswer": "We heard liquid cooling is complicated and risky with leaks, so we only use air cooling for all our servers."
      },
      "question_109": {
        "id": 109,
        "old_question_text": null,
        "questionText": "Has the organization evaluated the possibility of providing low-grade heating directly from heat rejected from the data centre to other targets, such as adjacent office space fresh air pre-heating or nearby industrial spaces? Is it understood that while this does not reduce the energy consumed by the data centre itself, it can offset total energy overhead by potentially reducing energy use elsewhere, and that opportunities increase with higher IT equipment utilization and exhaust temperatures (including from liquid cooling)?",
        "simplifiedQuestion": "Have you evaluated reusing waste heat from your data centre for heating nearby offices or industrial spaces?",
        "reference": "2025 CoC Guide 5.7.1",
        "priority": "Medium (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_10",
        "recommendationTitle": "Waste heat re-use",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "dwg"
          ],
          "description": "Feasibility studies for waste heat reuse. Design documents if implemented (e.g., heat recovery systems, integration with building HVAC). Agreements with any third-party heat recipients. Performance data on heat recovery and offset energy."
        },
        "docRequired": "Yes, if new build/retrofit (evaluation is expected)",
        "acceptedFiles": "pdf, docx, xlsx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits: Documentation of evaluation of waste heat reuse possibilities. If implemented, design and performance data of the heat recovery system and its application (e.g., heating adjacent office space, industrial processes).",
        "verification_method": [
          "Document Review",
          "Site Inspection (if implemented)",
          "Interview"
        ],
        "verificationMethods": "Review of feasibility studies for waste heat reuse, design documents for any implemented systems, and agreements with heat consumers. Inspection of heat recovery installations. Interviews with facilities and energy managers.",
        "nonconformity": {
          "major": "For new builds/retrofits, no evaluation of waste heat reuse potential has been conducted, despite opportunities existing (e.g., co-located offices, nearby heat demand).",
          "minor": "Waste heat reuse was evaluated but dismissed without thorough justification, or if implemented, the system is inefficient or underutilized."
        },
        "majorNonconformity": "For new builds or retrofits, failure to evaluate the possibility of reusing waste heat from the data centre for applications like heating adjacent office spaces or industrial processes, where clear opportunities exist.",
        "minorNonconformity": "An evaluation of waste heat reuse was conducted, but viable opportunities were not pursued without adequate justification, or an implemented system is significantly underperforming or not optimized.",
        "whatThisMeans": "Your data centre produces a lot of waste heat. Your organization should look into whether this heat can be captured and used for other purposes, like heating fresh air for nearby office spaces or providing low-temperature heat to local industrial facilities. While this doesn't make your data centre use less energy directly, it reduces the overall energy needed by your company or the community by offsetting heating fuel that would otherwise be burned elsewhere. The chances of this being effective increase if your IT gear is well-used (creating more consistent heat) and if you have higher temperature exhaust air (e.g., from liquid cooling).",
        "complianceRequires": "- Evaluate the possibility of providing low-grade heating from data centre rejected heat to other targets.\n- Examples of targets include adjacent office space fresh air pre-heating or nearby industrial spaces.\n- Understand that this practice does not reduce the energy consumed by the data centre itself but offsets total energy overhead by reducing energy use elsewhere.\n- Recognize that opportunities for waste heat reuse increase with higher IT equipment utilization and higher exhaust temperatures (e.g., from liquid-cooled IT equipment or well-contained hot aisles).",
        "expertTip": "When planning new data centres, consider co-location with potential heat consumers (e.g., district heating networks, greenhouses, swimming pools, industrial processes requiring low-grade heat). Early engagement with potential off-takers is crucial for designing effective and economically viable heat reuse schemes.",
        "recommendedEvidence": "- Feasibility studies or reports evaluating waste heat reuse opportunities, identifying potential heat sources within the DC and potential consumers.\n- Design documents for any implemented heat recovery systems, including heat exchangers, pipework, and integration with recipient systems.\n- Agreements or contracts with any third parties receiving the waste heat.\n- Monitoring data showing the amount of heat recovered and reused, and an estimation of the offset energy savings for the recipient.",
        "recommendedActions": "- For new builds and major retrofits, conduct a feasibility study for waste heat reuse, considering local demand for low-grade heat.\n- Identify the quantity and quality (temperature) of waste heat available from the data centre's cooling systems.\n- Engage with potential local heat consumers to explore opportunities for heat export.\n- If viable, design and implement appropriate heat recovery and distribution systems.\n- Monitor and report on the amount of heat reused and the resulting energy/carbon savings.",
        "sampleResponseTemplate": "We evaluated waste heat reuse potential for our [Facility Name] in [Year] (Ref: '[Heat Reuse Study ID]'). We currently reuse [X kW of waste heat / Y% of rejected heat] from our [e.g., air-cooled chillers / liquid cooling system] to [e.g., pre-heat fresh air for our adjacent office building / supply a local district heating loop]. This offsets an estimated [Z MWh] of [natural gas/electricity] annually for the recipient. The system is documented in '[Heat Recovery System Design Doc]'.",
        "humanInputAnswer": "Our 'Project Phoenix' data centre (new build 2024) incorporates a waste heat recovery system. As documented in 'PHX-HeatReclaim-Design.pdf', low-grade heat (approx. 35\u00b0C water) from our chiller condensers is used to pre-heat the domestic hot water for the adjacent administrative building and to provide underfloor heating in our loading dock area. This is estimated to offset 200 MWh of natural gas consumption annually for the office building.",
        "llmGeneratedAnswer": "Yes, our organization has evaluated and implemented waste heat reuse. For our 'EcoPark Data Centre' (new build completed Q1 2025), a feasibility study (Ref: 'EPDC_HeatReuse_FS_2024.pdf') identified an opportunity to reuse rejected heat from our liquid-cooled IT deployments. The system, detailed in 'EPDC_HeatRecovery_System_Design_v1.1.dwg', captures ~40\u00b0C water from the cooling loop's secondary side and directs it to an adjacent facility's (a partner greenhouse project) heating system via insulated pipework. This directly offsets their conventional heating fuel usage. While this does not lower the DC's PUE, it contributes to overall site energy efficiency and carbon reduction for the campus.",
        "badExampleAnswer": "All the heat from our data centre just goes out through the cooling towers. We haven't looked at using it for anything else."
      },
      "question_110": {
        "id": 110,
        "old_question_text": null,
        "questionText": "For new builds or retrofits, where direct reuse of data centre waste heat is not possible due to its temperature being too low for the intended application, has the organization evaluated whether it is economic to use additional heat pumps to raise the temperature of this waste heat to a more useful point? Is it understood that this upgraded heat can potentially supply office heating, district heating networks, or other heating needs, thereby still contributing to overall energy savings even if it involves additional energy input for the heat pump itself?",
        "simplifiedQuestion": "If your DC waste heat isn't hot enough for direct reuse, have you evaluated using heat pumps to boost its temperature for heating offices or district heating?",
        "reference": "2025 CoC Guide 5.7.2",
        "priority": "Medium (Value: 4, Expected: New build or retrofit)",
        "stepSection": "step_10",
        "recommendationTitle": "Heat pump assisted waste heat re-use",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Feasibility studies for heat pump assisted waste heat reuse. Design documents if implemented, including heat pump specifications, integration with waste heat source and heat recipient. COP of heat pumps. Energy balance calculations showing net benefit."
        },
        "docRequired": "Yes, if new build/retrofit (evaluation is expected)",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Documentation of evaluation of heat pump assisted waste heat reuse. If implemented, design and performance data of the system, including heat pump COP and overall energy balance.",
        "verification_method": [
          "Document Review",
          "Site Inspection (if implemented)",
          "Interview"
        ],
        "verificationMethods": "Review of feasibility studies or design documents for heat pump assisted heat reuse. Inspection of implemented systems. Interviews with facilities and energy managers.",
        "nonconformity": {
          "major": "For new builds/retrofits, no evaluation of using heat pumps to upgrade low-temperature waste heat has been conducted, despite viable heat sources and potential local heat demands.",
          "minor": "Heat pump assisted reuse was evaluated but dismissed without thorough economic/technical justification, or if implemented, the system is inefficient or not well-integrated."
        },
        "majorNonconformity": "For new builds or retrofits, failure to evaluate the economic and technical feasibility of using heat pumps to upgrade low-temperature data centre waste heat for useful applications, where direct reuse is not viable.",
        "minorNonconformity": "An evaluation of heat pump assisted waste heat reuse was conducted, but viable opportunities were not pursued without adequate justification, or an implemented system has a low Coefficient of Performance (COP) or is poorly integrated, diminishing its net energy benefit.",
        "whatThisMeans": "Sometimes the waste heat from your data centre isn't hot enough to be used directly for things like office heating. In such cases, your organization should look into using heat pumps. Heat pumps can take this low-temperature waste heat and 'boost' its temperature to make it useful for heating offices, contributing to district heating systems, or other heating needs. Even though the heat pump uses some energy itself, this approach can still lead to overall energy savings by reducing the need for other heating sources.",
        "complianceRequires": "- For new builds or retrofits: Where direct reuse of data centre waste heat (Practice 5.7.1) is not possible due to its temperature being too low for the intended application, evaluate the economic feasibility of using additional heat pumps.\n- These heat pumps would raise the temperature of the waste heat to a more useful point.\n- Understand that this upgraded heat can potentially supply heating needs for:\n    - Office spaces.\n    - District heating networks.\n    - Other heating demands.\n- This approach can contribute to overall energy savings even though it involves energy input for the heat pump operation.",
        "expertTip": "When evaluating heat pumps for waste heat upgrade, select high-efficiency heat pumps with a high Coefficient of Performance (COP), especially at the expected source (waste heat) and sink (application) temperatures. Ensure the heat pump capacity is well-matched to the available waste heat stream and the heating demand to maximize utilization and efficiency.",
        "recommendedEvidence": "- Feasibility studies or economic analyses for using heat pumps to upgrade data centre waste heat.\n- If implemented: design documents for the heat pump system, including heat pump specifications (COP, capacity), integration with the waste heat source, and connection to the heating application.\n- Performance data showing the amount of heat delivered, the energy consumed by the heat pump, and the net energy savings achieved.\n- Agreements with any recipients of the upgraded heat (e.g., district heating provider, office building management).",
        "recommendedActions": "- For new builds and major retrofits, if direct waste heat reuse is not viable due to low temperature, conduct a feasibility study for heat pump assisted reuse.\n- Identify available low-temperature waste heat sources from the data centre (e.g., air-cooled condenser exhaust, liquid cooling loops).\n- Identify potential local demands for medium-temperature heat (e.g., space heating, domestic hot water, district heating input).\n- Evaluate the economics and energy balance of using heat pumps to bridge the temperature gap.\n- If viable, design and implement an optimized heat pump system, selecting high-COP units appropriate for the temperature lift required.",
        "sampleResponseTemplate": "We evaluated heat pump assisted waste heat reuse for our [Facility Name] (Ref: '[HP Feasibility Study ID]'). While direct reuse wasn't viable for [Reason], the study showed that using a [X kW capacity, Y COP] heat pump to upgrade waste heat from our [e.g., chiller condensers] to [Z\u00b0C] could supply [e.g., our office block's primary heating loop / pre-heat for district heating]. This system [is under construction / was commissioned on Date] and is projected to save [Energy Amount] annually.",
        "humanInputAnswer": "Our 'North Campus Data Centre' design (NCDC-Design-2024.pdf) includes a heat pump system to upgrade low-grade waste heat (28\u00b0C water from CRAH return) to 55\u00b0C for use in the adjacent campus administrative building's radiant heating system. The feasibility study (Ref: 'NCDC_HP_HeatReuse_Study.pdf') projected a net COP of 3.5 for the heat pump system, offsetting 70% of the admin building's heating gas consumption. The system is currently in the commissioning phase.",
        "llmGeneratedAnswer": "Yes, for our 'City Hub' data centre, where direct waste heat reuse was limited by the low temperature of the exhaust air from our air-cooled chillers, we conducted an evaluation (Ref: 'CHDC_HeatPump_Feasibility_R1.pdf') for heat pump assisted reuse. This study confirmed the economic viability of using a 250kW heat pump system (target COP of 4.0) to upgrade this waste heat to supply the base heating load for our adjoining three-story office building. This system was implemented in Q2 2025 as part of the facility retrofit and is projected to meet 60% of the office's annual heating demand, significantly reducing natural gas consumption.",
        "badExampleAnswer": "Our waste heat is pretty low temperature, so we didn't think it could be used for anything. Heat pumps use electricity anyway."
      }
    }
  },
  "step_11": {
    "description": "2025 Guide Section 5: Cooling (Practices 5.7.3-5.7.5 from Sec 5.7 'Reuse of Data Centre Waste Heat')",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Cooling_6"
    ],
    "critical_questions": [],
    "questions": {
      "question_111": {
        "id": 111,
        "old_question_text": null,
        "questionText": "Has the organization evaluated using warm exhaust air from the data floor (e.g., from hot aisles or direct equipment exhaust) to reduce or eliminate the electrical preheat loads for standby generators (engine block heaters, fuel storage heaters) or to provide some warming for office areas, generator rooms, or fuel storage areas, especially in colder climates or seasons?",
        "simplifiedQuestion": "Have you considered using warm exhaust air from your data centre to help heat generator/fuel storage areas or offices, reducing their separate heating needs?",
        "reference": "2025 CoC Guide 5.7.3",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_11",
        "recommendationTitle": "Use data floor waste heat to warm office, generator and fuel storage areas",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Feasibility studies or design considerations for using data floor exhaust air for preheating generators, fuel storage, or warming adjacent spaces. HVAC diagrams showing any ducting or airflow paths for this purpose. Estimated or measured energy savings from reduced preheat loads."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Optional: If implemented or assessed: Design documentation or operational procedures showing how warm exhaust air from the data floor is ducted or utilized to reduce heating/preheating loads for generators, fuel storage, or adjacent office/plant rooms.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of any feasibility studies or design documents for using data floor exhaust air for heating purposes. Physical inspection of ductwork or airflow paths if implemented.",
        "nonconformity": {
          "major": null,
          "minor": "Opportunities to use data floor exhaust air for preheating generators, fuel stores, or warming adjacent areas are not evaluated or implemented where it could be simple and beneficial (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to evaluate or implement the use of data floor warm exhaust air to reduce preheating loads for generators or fuel storage, or to warm adjacent office/plant areas, where such applications are feasible and could offer energy savings.",
        "whatThisMeans": "The warm air coming out of your data centre's IT equipment can sometimes be used for simple heating tasks. Your organization should see if it's possible to use this warm exhaust air to help keep standby generators ready (by warming their engines or fuel), or to provide some warmth to nearby office spaces, generator rooms, or fuel storage areas. This can reduce the need for separate electric heaters for those purposes, saving energy, especially in colder weather.",
        "complianceRequires": "- Evaluate the potential for reducing or eliminating electrical preheat loads for standby generators (engine block heaters, fuel storage heaters) by using warm exhaust air from the data floor.\n- Evaluate using warm exhaust air from the data floor to help maintain temperature in areas housing generators, fuel storage tanks, or even adjacent office areas.\n- This is particularly relevant in colder climates or seasons where such preheating or warming loads are significant.",
        "expertTip": "Simple ducting can often redirect warm exhaust air from a contained hot aisle or directly from CRAH/CRAC return paths to adjacent generator rooms or fuel storage areas. Ensure any such system includes dampers to control airflow and prevent overheating of the target spaces, and consider fire safety implications of any new duct penetrations.",
        "recommendedEvidence": [
          "- Feasibility study or design notes considering the use of data floor exhaust air for preheating or warming purposes.\n- If implemented: HVAC diagrams showing ductwork routing warm exhaust air to generator rooms, fuel storage, or office areas.\n- Records of reduced runtime for generator block heaters or other preheaters attributed to this practice.\n- Temperature logs from areas being warmed by data floor exhaust air."
        ],
        "recommendedActions": [
          "- Identify any significant electrical preheating loads for generators or fuel storage, or heating loads for adjacent plant/office spaces.\n- Assess the temperature and volume of readily available warm exhaust air from the data floor (e.g., from hot aisle return plenums or direct CRAC/CRAH exhaust before it hits chillers/economizers).\n- Evaluate the feasibility of ducting this warm air to the areas requiring heating/preheating.\n- If implemented, monitor the effectiveness and ensure proper controls to prevent overheating."
        ],
        "sampleResponseTemplate": "We have evaluated using data floor exhaust air for preheating. Currently, [describe implementation or outcome of evaluation, e.g., 'we duct a portion of our hot aisle exhaust (approx. X CFM at Y\u00b0C) to the adjacent generator room during winter months (October-March) to reduce the runtime of the 5kW electric block heaters. This is controlled by a thermostat and motorized damper.' / 'the evaluation showed limited benefit due to the short distance and small preheating load.']. This is documented in '[Study/Design Doc ID]'.",
        "humanInputAnswer": "For our 'Northern Tier' data centre, which experiences cold winters, we have implemented a system (Ref: 'NTDC_Exhaust_Heat_Reuse_Design.dwg') that ducts a controlled portion of the hot aisle return air (typically 28-30\u00b0C) into the adjacent generator enclosure and the fuel day-tank room. This has allowed us to reduce the operational hours of the 10kW generator block heater by an estimated 60% during winter months and maintain optimal fuel temperature without dedicated heating for the day tank.",
        "llmGeneratedAnswer": "Yes, the organization has evaluated this practice. At our 'Highlands Facility', which operates in a colder climate, we utilize a portion of the warm exhaust air from the primary data hall's hot aisle return plenum. This air (typically around 30-32\u00b0C) is ducted, via a thermostatically controlled damper system (detailed in 'HF_Generator_Preheat_System_SOP.pdf'), to the adjacent standby generator room. This maintains a minimum ambient temperature of 10\u00b0C in the generator room during winter, significantly reducing the runtime of the electric engine block heaters and fuel preheaters. This system is estimated to save approximately 5,000 kWh annually in heater energy consumption.",
        "badExampleAnswer": "Our generators have their own heaters to keep them ready. The data centre heat just goes outside."
      },
      "question_112": {
        "id": 112,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented the use of standardized energy reuse metrics and reporting, such as the Energy Reuse Factor (ERF) defined in ISO/IEC 30134-6, to quantify and report on the effectiveness of its data centre waste heat reuse initiatives? Is it understood that such metrics are required for EU Commission EED (Energy Efficiency Directive) reporting?",
        "simplifiedQuestion": "Do you use standard metrics like ERF (ISO/IEC 30134-6) to measure and report your data centre's waste heat reuse, especially for EED reporting?",
        "reference": "2025 CoC Guide 5.7.4",
        "priority": "Low (Value: 1, Expected: Entire Data Centre, NOTE: Required for EU Commission EED reporting)",
        "stepSection": "step_11",
        "recommendationTitle": "Energy reuse metrics and reporting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Documentation of methodology for calculating Energy Reuse Factor (ERF) as per ISO/IEC 30134-6. Reports showing calculated ERF values. Evidence of how this data is used for internal analysis or external reporting (e.g., EED compliance)."
        },
        "docRequired": "Yes, if EED reporting applicable or ERF claimed",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory if EED reporting is applicable or if ERF is being used: Documented methodology for calculating and reporting energy reuse metrics, specifically Energy Reuse Factor (ERF) according to ISO/IEC 30134-6. Reports including ERF values.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of energy reuse reporting methodology, ERF calculation worksheets, and any reports submitted for EED compliance or published in sustainability reports.",
        "nonconformity": {
          "major": "Waste heat is reused, but no standardized metrics like ERF are calculated or reported, especially if required for EED compliance.",
          "minor": "ERF or similar metrics are calculated but not strictly according to ISO/IEC 30134-6, or reporting is inconsistent or not used for improvement."
        },
        "majorNonconformity": "If waste heat reuse is implemented (Practices 5.7.1-5.7.3) and EED reporting is applicable, failure to calculate and report energy reuse using standardized metrics like Energy Reuse Factor (ERF) as per ISO/IEC 30134-6.",
        "minorNonconformity": "Energy reuse metrics are reported, but the calculation methodology deviates from ISO/IEC 30134-6, or the reporting is not consistently used for internal performance tracking or external communication where appropriate.",
        "whatThisMeans": "If your data centre reuses its waste heat (as covered in other practices), your organization should use standard ways to measure and report how effective this is. One such standard is the Energy Reuse Factor (ERF), defined in ISO/IEC 30134-6. Using such metrics is also important because it's required for reporting under the EU Energy Efficiency Directive (EED).",
        "complianceRequires": "- Consider and, where waste heat reuse is implemented, implement the use of standardized energy reuse metrics and reporting.\n- Specifically, reference and utilize metrics like the Energy Reuse Factor (ERF) as defined in ISO/IEC 30134-6 ('Information technology \u2014 Data centres \u2014 Key performance indicators \u2014 Part 6: Energy Reuse Factor').\n- Understand that reporting on energy reuse (potentially using ERF) is noted as being required for EU Commission EED (Energy Efficiency Directive) reporting purposes.",
        "expertTip": "Familiarize yourself thoroughly with the calculation methodology for ERF as defined in ISO/IEC 30134-6. Ensure you have accurate metering for both the energy consumed by the data centre and the amount of reused energy that is demonstrably offsetting a primary energy source elsewhere. Maintain clear documentation of all data inputs and calculations for auditability.",
        "recommendedEvidence": [
          "- Documented procedure for calculating and reporting the Energy Reuse Factor (ERF) aligned with ISO/IEC 30134-6.\n- Data logs or reports showing the inputs used for ERF calculation (e.g., total data centre energy, reused energy amount, type of primary energy offset).\n- Examples of internal or external reports that include the calculated ERF value.\n- If applicable, evidence of ERF reporting for EU Energy Efficiency Directive compliance."
        ],
        "recommendedActions": [
          "- If waste heat reuse is implemented, establish a process for calculating and reporting the Energy Reuse Factor (ERF) according to ISO/IEC 30134-6.\n- Ensure accurate metering is in place to measure both the data centre's energy consumption and the amount of energy reused.\n- Integrate ERF reporting into regular energy performance reviews and sustainability reporting.\n- Use ERF metrics to track the effectiveness of heat reuse initiatives and identify areas for improvement."
        ],
        "sampleResponseTemplate": "We calculate our Energy Reuse Factor (ERF) according to ISO/IEC 30134-6 for our [Facility Name] where we reuse waste heat for [Application]. Our ERF for [Reporting Period] was [Value]. This is reported as part of our [EED compliance process / annual sustainability report]. The methodology is documented in '[ERF Calculation Procedure ID]'.",
        "humanInputAnswer": "For our 'EcoHub Data Centre' where we supply waste heat to a local district heating network, we calculate ERF monthly as per ISO/IEC 30134-6. For calendar year 2024, our average ERF was 0.12 (meaning 12% of our total energy input was effectively reused). This calculation (methodology doc: 'ERF_Calc_Method_EHDC.pdf') is included in our annual EED reporting submission to the national authority.",
        "llmGeneratedAnswer": "Yes, the organization utilizes standardized energy reuse metrics. For facilities where waste heat reuse is implemented (as per practices 5.7.1, 5.7.2, or 5.7.3), we calculate the Energy Reuse Factor (ERF) in accordance with ISO/IEC 30134-6. This calculation is performed quarterly and reported annually in our 'Corporate Sustainability Report' and is also used for our mandatory EU Energy Efficiency Directive (EED) reporting. Our 'Energy Metrics Reporting Standard (EMRS-2025, Section 4.2)' details the methodology for ERF calculation, including data collection points and conversion factors.",
        "badExampleAnswer": "We send some hot water to the office next door from our chillers; they say it helps them a bit with their heating."
      },
      "question_113": {
        "id": 113,
        "old_question_text": null,
        "questionText": "During new build and retrofit projects, has the organization considered installing 'Capture Ready' infrastructure to facilitate the future advantage and distribution of available data centre waste heat, even if immediate reuse opportunities are not present or fully developed? This might include provisioning space for heat exchangers, pipework pathways, or structural considerations for future heat recovery equipment.",
        "simplifiedQuestion": "For new/retrofit projects, do you install 'Capture Ready' infrastructure (like space/paths for pipes/equipment) to make it easier to reuse waste heat in the future?",
        "reference": "2025 CoC Guide 5.7.5",
        "priority": "Low (Value: 1, Expected: Optional Practice)",
        "stepSection": "step_11",
        "recommendationTitle": "Capture Ready Infrastructure",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg"
          ],
          "description": "Design documents for new builds/retrofits showing provisions for 'Capture Ready' infrastructure (e.g., designated space for future heat exchangers, capped pipe connections, planned routes for pipework). Feasibility studies or strategy documents discussing future heat reuse and capture-ready design."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg",
        "documentDescriptor": "Optional: For new builds/retrofits, design documentation or plans showing 'Capture Ready' provisions (e.g., allocated space, capped-off pipe stubs, planned pathways) to facilitate future implementation of waste heat reuse.",
        "verification_method": [
          "Document Review",
          "Site Inspection (if implemented)"
        ],
        "verificationMethods": "Review of design documents for new builds or retrofits to identify any 'Capture Ready' provisions. Physical inspection of any installed capture-ready infrastructure.",
        "nonconformity": {
          "major": null,
          "minor": "New builds or retrofits do not include any 'Capture Ready' provisions for future waste heat reuse, even where long-term plans or potential opportunities for reuse might exist (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Despite acknowledging potential future heat reuse, new build or retrofit designs fail to incorporate basic 'Capture Ready' infrastructure provisions, making future implementation more costly and disruptive.",
        "whatThisMeans": "When you are building a new data centre or doing a major renovation, even if you don't have an immediate way to reuse the waste heat, your organization should think about making it easier to do so in the future. This means designing the facility to be 'Capture Ready'. For example, you might leave space for future heat recovery equipment, plan routes for pipes, or make sure the building structure can support add-ons later. This makes it cheaper and less disruptive to implement heat reuse when the opportunity arises.",
        "complianceRequires": "- During new build and retrofit projects, consider installing 'Capture Ready' infrastructure.\n- The purpose of this infrastructure is to take advantage of, and distribute, available data centre waste heat at a future date, even if immediate reuse is not planned.\n- This might involve provisioning elements such as:\n    - Designated space for future heat exchangers or other heat recovery equipment.\n    - Pathways or knock-outs for future pipework or ducting.\n    - Structural considerations for supporting future heat recovery installations.",
        "expertTip": "Incorporating 'Capture Ready' provisions is often a low-cost measure during initial construction compared to the significant expense and disruption of retrofitting later. Identify main waste heat streams (e.g., chiller condenser water return, liquid cooling loops) and provide capped-off connection points and clear pathways for future pipework routing to potential heat consumers or the building perimeter.",
        "recommendedEvidence": [
          "- Design documents (architectural, mechanical, structural) for new builds or retrofits that highlight 'Capture Ready' provisions for future waste heat reuse.\n- Examples include: designated plant space for future heat recovery equipment, capped-off pipe connections on main cooling loops, planned routes for future heat distribution pipework, structural reinforcement for future rooftop equipment.\n- Feasibility studies or strategic plans that discuss future heat reuse and the rationale for including 'Capture Ready' infrastructure."
        ],
        "recommendedActions": [
          "- During the design phase of new builds and major retrofits, explicitly evaluate and identify potential 'Capture Ready' provisions for future waste heat reuse.\n- Incorporate these provisions into the design where the future benefit outweighs the modest initial cost and complexity (e.g., including extra pipe stubs, allocating plant space).\n- Document the location and specifications of any 'Capture Ready' infrastructure for future reference.\n- Periodically review potential heat reuse opportunities that could leverage this pre-installed infrastructure."
        ],
        "sampleResponseTemplate": "Our design standard for new data centres ([Doc ID: DC-NewBuild-Std-004]) requires an assessment for 'Capture Ready' infrastructure. For our [Facility Name] project, we have included [e.g., 'dedicated space adjacent to the chiller plant for future heat exchangers and capped pipe connections on the condenser water loop'] to facilitate potential future connection to a [planned local district heating network / adjacent building's heating system]. This is shown on drawing [Drawing Number].",
        "humanInputAnswer": "For our 'Greenfield Alpha' data centre build, the design (Ref: 'GF-Alpha_Design_Pack_Rev3', Section M.7) includes 'Capture Ready' provisions. Specifically, we have allocated a 50m\u00b2 plant space adjacent to the main cooling plant and installed capped-off 6-inch pipe flanges on our primary cooling tower return lines to facilitate potential future connection to a proposed local community heating scheme. This was a low-cost provision to enable future heat export.",
        "llmGeneratedAnswer": "Yes, our 'New Data Centre Construction Standard (NDCCS-2025, Section 8.3)' incorporates the principle of 'Capture Ready Infrastructure'. For all new builds initiated after 2024, the design phase must include an assessment for incorporating provisions to facilitate future waste heat reuse. For example, our 'Sigma Facility' (currently under construction) includes designated physical space and capped-off pipe stubs on the condenser water loop for the potential future installation of heat exchangers to connect to an anticipated local district heating network. This ensures that future heat reuse projects can be implemented with minimal disruption and lower cost.",
        "badExampleAnswer": "We only build what we need for today. If we want to reuse heat later, we'll figure it out then."
      }
    }
  },
  "step_12": {
    "description": "2025 Guide Section 6: Data Centre Power Equipment (Practices 6.1.1-6.1.5 from Sec 6.1; and 6.2.1-6.2.2 from Sec 6.2)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Power_1"
    ],
    "critical_questions": [],
    "questions": {
      "question_114": {
        "id": 114,
        "old_question_text": null,
        "questionText": "For new builds or retrofits, has the organization implemented modular (scalable) Uninterruptible Power Supply (UPS) system deployments across a broad range of power delivery capacities? Is this approach used such that while physical installation, transformers, and cabling are prepared for the design electrical load of the facility, the sources of inefficiency (e.g., switching units, batteries) are installed in modular units only as required? Is this practice aimed at substantially reducing both the capital cost and the fixed overhead losses of these UPS systems, whether they are frame-based plug-in modules for low power or entire UPS units for larger installations?",
        "simplifiedQuestion": "For new/retrofitted DCs, do you use modular UPS systems that scale with demand to reduce costs and energy losses?",
        "reference": "2025 CoC Guide 6.1.1",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_12",
        "recommendationTitle": "Modular UPS deployment",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "UPS system design documents showing modular architecture. Specifications for modular UPS units and frames. Capacity planning documents detailing phased deployment of UPS modules. Purchase records showing staged acquisition of UPS modules."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Design documentation and specifications for UPS systems demonstrating a modular and scalable approach, where capacity (power modules, batteries) is deployed in phases aligned with load growth.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of UPS system design documents, specifications for modular components, and capacity planning for phased deployment. Physical inspection of UPS installations to verify modularity.",
        "nonconformity": {
          "major": "New build/retrofit UPS systems are monolithically sized for full future facility load from day one, leading to significant capital overspend and high fixed energy losses due to low initial utilization.",
          "minor": "UPS systems have some modular capability, but modules are still oversized for initial loads, or the phased deployment strategy does not closely match actual load growth."
        },
        "majorNonconformity": "For new builds or retrofits, UPS systems are installed with full ultimate capacity from day one, without leveraging modularity to phase in power modules and batteries as IT load grows, leading to high fixed losses and inefficient operation at low loads.",
        "minorNonconformity": "Modular UPS systems are used, but the initial deployment of modules or batteries significantly exceeds near-term demand, or the strategy for adding modules is not well-aligned with load projections, reducing the benefits of modularity.",
        "whatThisMeans": "When building a new data centre or upgrading your power systems, you should use Uninterruptible Power Supplies (UPSs) that are modular, meaning you can add capacity (like power modules or batteries) in stages as your IT load grows. While the main electrical infrastructure might be set up for the final load, the actual UPS components that cause inefficiency if underloaded are only installed when needed. This saves money upfront and reduces energy waste from oversized UPS systems running inefficiently.",
        "complianceRequires": "- For new builds or retrofits, implement modular (scalable) UPS system deployments.\n- This applies across a broad range of power delivery capacities.\n- Ensure that while core infrastructure (physical installation, transformers, cabling) may be prepared for the design electrical load, the main sources of inefficiency within the UPS (e.g., power switching units, batteries) are installed in modular units only as actual demand requires.\n- This approach aims to substantially reduce both initial capital costs and the fixed overhead energy losses associated with underutilized UPS systems.\n- This applies to various scales, from frames with plug-in modules in low power installations to entire UPS units being paralleled in larger installations.",
        "expertTip": "Select modular UPS systems that offer high efficiency at low individual module loads, and ensure the control system can intelligently manage the number of active modules to keep them operating in their most efficient range. 'Right-sizing' by adding modules just-in-time with load growth maximizes these benefits.",
        "recommendedEvidence": "- Design documents for the electrical system showing a modular UPS architecture.\n- Specifications for the modular UPS systems, detailing power module capacity, battery module capacity, and scalability features.\n- Capacity planning reports or forecasts that show phased deployment of UPS modules aligned with IT load growth.\n- Purchase orders or installation records demonstrating that UPS modules and batteries were acquired and installed in stages.",
        "recommendedActions": "- Specify and procure modular, scalable UPS systems for all new data centre builds and major power system retrofits.\n- Develop an IT load growth forecast and a corresponding UPS capacity deployment plan that phases in modules as needed.\n- Ensure the UPS system allows for online addition of power and battery modules without disrupting the critical load.\n- Regularly review IT load growth against the UPS capacity plan and adjust module deployment schedules accordingly.",
        "sampleResponseTemplate": "Our new data centre ([Facility Name]) utilizes a modular UPS system ([Vendor/Model]), as detailed in our '[Electrical Design Spec ID: ELEC-UPS-MOD-001]'. We initially deployed [X kW] of UPS power module capacity to support the day-1 IT load of [Y kW], with [Z] empty slots for future modules. Additional modules are planned based on our 18-month rolling IT load forecast. This approach reduced initial CapEx by [Amount] and avoids significant underloading of UPS power modules.",
        "humanInputAnswer": "Our standard for all new UPS deployments (Ref: 'Power Systems Standard PSS-003') is to use modular, scalable UPS systems. For our 'Data Hub East' facility, we installed Schneider Symmetra PX frames capable of 2MW, but initially populated with only 500kW of power modules and associated battery strings to match the Year 1 projected load. Additional 250kW power modules will be added as per our 12-month rolling capacity forecast, preventing inefficient operation of oversized units.",
        "llmGeneratedAnswer": "Yes, our organization has standardized on modular UPS deployments for all new builds and major retrofits since 2023. As outlined in our 'Critical Power Design Guide (CPDG-2025, Section 4.1)', this approach involves installing UPS frames and core distribution to support the facility's ultimate design load, but power modules and battery strings are deployed in phases, typically aligned with 12-18 month IT load growth forecasts. For example, our 'Site Gamma' UPS system ([VendorX ModularUPS Model S]) has a 1MW frame capacity but was initially commissioned with 400kW of power modules. This strategy significantly reduces upfront capital expenditure and, more importantly, minimizes fixed energy losses by ensuring active power modules operate at higher, more efficient utilization levels.",
        "badExampleAnswer": "We install a big UPS system when we build a new data centre so it will last us for many years without needing changes."
      },
      "question_115": {
        "id": 115,
        "old_question_text": null,
        "questionText": "If static AC Uninterruptible Power Systems (UPS) are to be installed in new builds or retrofits, has the organization ensured that these systems are highly energy efficient, specifically compliant with the 'elite requirements' of the 2021 (or most current) version of the EU Code of Conduct on Energy Efficiency of AC Uninterruptible Power Systems? Is it ensured that such a UPS is also compliant with the IEC 62040 series for UPS systems, including its environmental operating conditions (e.g., Temperature 0\u00b0C to +40\u00b0C, Relative Humidity 20% to 80%)? Is it understood that while Rotary and Direct Current UPS systems are not currently covered by this specific EU CoC for AC UPS, this does not preclude their use if they offer high efficiency?",
        "simplifiedQuestion": "For new/retrofitted static AC UPS systems, do you ensure they meet top EU Code of Conduct efficiency standards (elite requirements) and IEC 62040, and operate in their specified environmental range?",
        "reference": "2025 CoC Guide 6.1.2",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_12",
        "recommendationTitle": "High efficiency UPS",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Procurement specifications for static AC UPS systems requiring compliance with EU CoC 'elite requirements' and IEC 62040. Vendor declarations of conformity or test reports for selected UPS models. Documentation of UPS operating environment."
        },
        "docRequired": "Yes, if static AC UPS installed/retrofitted",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new/retrofitted static AC UPS: Procurement specifications and vendor conformity documents demonstrating compliance with the 'elite requirements' of the EU CoC for AC UPS (2021 or current) and the IEC 62040 series (including environmental operating conditions).",
        "verification_method": [
          "Document Review",
          "Equipment Specification Audit"
        ],
        "verificationMethods": "Review of procurement specifications for static AC UPS systems. Verification of vendor documentation (certificates, datasheets) confirming compliance with EU CoC elite requirements and IEC 62040. Check actual UPS operating environment against specified ranges.",
        "nonconformity": {
          "major": "Newly installed static AC UPS systems do not meet the 'elite requirements' of the EU CoC for AC UPS or are not compliant with IEC 62040, leading to lower than achievable operational efficiency.",
          "minor": "Compliance with EU CoC elite requirements or IEC 62040 is sought but not consistently verified for all new static AC UPS installations, or operating environmental conditions are not consistently maintained within specified limits."
        },
        "majorNonconformity": "For new builds or retrofits, static AC UPS systems are installed that do not comply with the 'elite requirements' of the current EU Code of Conduct on Energy Efficiency of AC Uninterruptible Power Systems, or do not meet IEC 62040 standards.",
        "minorNonconformity": "New static AC UPS systems are compliant with baseline IEC 62040 but not specifically verified against the EU CoC 'elite requirements', or their specified environmental operating conditions (0-40\u00b0C, 20-80% RH) are not consistently ensured by the plant room design.",
        "whatThisMeans": "When you install new static AC UPS systems (the common type that use batteries and inverters), they must be very energy efficient. Specifically, they should meet the 'elite requirements' of the EU Code of Conduct for AC UPS systems (use the 2021 version or the latest one). They also need to comply with the IEC 62040 standards, which include rules about the temperatures and humidity they can operate in (e.g., 0-40\u00b0C, 20-80% RH). Using other types of UPS, like rotary or DC UPS, is fine if they are efficient, but they aren't covered by this specific AC UPS Code of Conduct.",
        "complianceRequires": "- If installing static AC UPS systems (new build or retrofit), select energy-efficient models that are compliant with the 'elite requirements' of the 2021 (or most current) version of the EU Code of Conduct on Energy Efficiency of AC Uninterruptible Power Systems.\n- Ensure that any such selected UPS is also compliant with the IEC 62040 series for UPS systems, which includes specifications for environmental operating conditions (e.g., Temperature 0\u00b0C to +40\u00b0C, Relative Humidity 20% to 80%).\n- Maintain the operating environment for the UPS within these specified ranges.\n- Understand that while Rotary and Direct Current (DC) UPS systems are not included in the scope of the EU CoC for *AC* UPS, their use is not discouraged; selection should still be based on high efficiency for their respective technology type.",
        "expertTip": "Look for UPS systems that offer high efficiency not just at full load but also at the typical part-load conditions (e.g., 25%, 50%, 75% load) expected in your facility. The EU CoC for AC UPS provides specific efficiency targets for these load points under its 'elite requirements' category.",
        "recommendedEvidence": "- Procurement specifications for static AC UPS systems explicitly requiring compliance with the 'elite requirements' of the relevant EU Code of Conduct for AC UPS and IEC 62040 series.\n- Vendor declarations of conformity, test certificates, or datasheets for the installed UPS models, demonstrating this compliance.\n- Documentation or monitoring data showing that the ambient environmental conditions for the UPS are maintained within the IEC 62040 specified ranges (0-40\u00b0C, 20-80% RH).",
        "recommendedActions": "- Update procurement standards for static AC UPS systems to mandate compliance with the 'elite requirements' of the current EU Code of Conduct for AC UPS and the IEC 62040 series.\n- Verify vendor claims of compliance through certified test reports or listings on the EU Code of Conduct website if available.\n- Ensure the UPS room or area provides an environment that meets the specified operating conditions for temperature and humidity.\n- If considering Rotary or DC UPS systems, evaluate them based on their specific technology's best-in-class efficiency and suitability for the application.",
        "sampleResponseTemplate": "All static AC UPS systems installed in our new [Facility Name] (commissioned [Date]) are compliant with the 'elite requirements' of the EU Code of Conduct for AC UPS (2021 version) and IEC 62040. We selected [Vendor/Model X] UPS, and vendor documentation ([Doc ID]) confirms compliance. The UPS room is maintained at [e.g., 20-25\u00b0C and 40-60% RH], well within the 0-40\u00b0C, 20-80% RH operational limits.",
        "humanInputAnswer": "Our 'UPS Procurement Specification UPS-SPEC-005' requires all new static AC UPS units to meet the 'elite requirements' of the 2021 EU CoC for AC UPS and be fully IEC 62040 compliant. Our recently installed 'VendorX PowerWave' 500kVA units meet these criteria, as verified by vendor conformity certificate (Ref: VXPW-COC-CERT-2024). The UPS room environment is maintained at 23\u00b0C and 45% RH, well within the IEC 62040 limits.",
        "llmGeneratedAnswer": "Yes, for all new static AC UPS installations, such as the system deployed in our 'Omega Data Centre - Phase 2' (Retrofit completed Q1 2025), we mandate compliance with the 'elite requirements' of the 2021 EU Code of Conduct on Energy Efficiency of AC Uninterruptible Power Systems. Vendor proposals must include documented proof of this compliance. The selected UPS systems (e.g., 'MakeModel UltraEff UPS') are also certified compliant with the IEC 62040 series, including adherence to the specified environmental operating conditions (Temperature 0\u00b0C to +40\u00b0C, Relative Humidity 20% to 80%), which our UPS plant rooms are designed to maintain. While we currently deploy static AC UPS, we remain open to evaluating high-efficiency Rotary or DC UPS technologies for future projects if they align with our operational and efficiency goals.",
        "badExampleAnswer": "We buy UPS systems from reputable vendors that are sized for our load. We assume they are efficient."
      },
      "question_116": {
        "id": 116,
        "old_question_text": null,
        "questionText": "For new builds or retrofits, are Uninterruptible Power Supply (UPS) units deployed and operated in their most energy-efficient operating modes where appropriate for the application and critical load requirements? Is it understood that while alternative UPS technologies like rotary or direct current systems may be considered, any comparison and evaluation must be based on the latest, non-biased information about available products? Are UPS system features allowing energy optimization at partial load levels taken into account, especially for UPS systems feeding mechanical loads like CRAC/CRAH fans which often have variable power draw?",
        "simplifiedQuestion": "For new/retrofitted UPS systems, do you use their most energy-efficient modes (like eco-mode if safe) and consider their part-load efficiency, especially if they power mechanical loads?",
        "reference": "2025 CoC Guide 6.1.3",
        "priority": "Medium (Value: 2, Expected: New build or retrofit)",
        "stepSection": "step_12",
        "recommendationTitle": "Use efficient UPS operating modes",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "UPS operational policy or procedures detailing use of efficient operating modes (e.g., eco-mode). Risk assessment for using such modes. UPS specifications highlighting efficient modes and part-load optimization features. BMS/monitoring data showing UPS operating mode and efficiency."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Mandatory for new builds/retrofits: Policy or documentation demonstrating that UPS units are operated in their most efficient modes (e.g., eco-mode where appropriate, considering risk). Evidence of evaluating UPS part-load efficiency features, especially for mechanical loads.",
        "verification_method": [
          "Document Review",
          "UPS Configuration Audit",
          "Interview"
        ],
        "verificationMethods": "Review of UPS operating procedures and risk assessments for using high-efficiency modes. Audit of UPS configuration settings. Interviews with facilities staff regarding UPS operation and mode selection.",
        "nonconformity": {
          "major": "UPS units are consistently operated in their least efficient (e.g., full double-conversion) mode without justification or consideration of available higher-efficiency modes appropriate for the load's criticality.",
          "minor": "High-efficiency UPS operating modes are available but not consistently utilized, or part-load optimization features are not actively managed, or risk assessments for eco-mode are outdated/incomplete."
        },
        "majorNonconformity": "For new builds or retrofits, UPS units are deployed without consideration for, or are not operated in, their most energy-efficient modes (e.g., eco-mode where load criticality and power quality permit), leading to continuous unnecessary energy losses.",
        "minorNonconformity": "Efficient UPS operating modes are available but are not consistently utilized due to lack of clear policy or risk assessment. Features for energy optimization at partial loads, especially for mechanical loads, are not fully leveraged or understood.",
        "whatThisMeans": "When you install new UPS systems or upgrade old ones, they should be set up to run in their most energy-saving mode whenever it's safe and appropriate for the IT equipment they protect. Some UPSs have an 'eco-mode' that's more efficient. You should also consider how efficiently the UPS runs when it's not fully loaded, especially if it powers mechanical gear like cooling unit fans that have varying power needs. If you're looking at different types of UPS (like rotary or DC systems), make sure you compare them using up-to-date, unbiased information.",
        "complianceRequires": "- Deploy and operate UPS units in their most energy-efficient operating modes where appropriate for the application and the criticality of the load they protect.\n- If considering alternative UPS technologies (e.g., rotary, direct current), base any comparison and evaluation on the latest, non-biased information about available products in the market.\n- Take into account any UPS system features that allow for energy optimization at partial load levels.\n- This is particularly relevant for UPS systems that may be feeding mechanical loads, such as CRAC/CRAH fans, which often have variable power draw.",
        "expertTip": "Conduct a thorough risk assessment before enabling high-efficiency modes like 'eco-mode' on UPS systems protecting critical loads. These modes may offer slightly less protection or slower transfer times. Implement them where the input power quality is high and the protected load can tolerate the mode's specific characteristics. For UPSs feeding mechanical loads, ensure the UPS can handle the inrush currents and variable nature of these loads efficiently.",
        "recommendedEvidence": "- UPS operational policy or standard operating procedure (SOP) that defines criteria for using high-efficiency operating modes (e.g., eco-mode).\n- Risk assessment documentation related to the use of specific UPS operating modes for different critical loads.\n- UPS configuration settings or BMS logs showing the active operating mode(s) of installed UPS units.\n- Manufacturer documentation highlighting the efficiency of different operating modes and any part-load optimization features of the UPS.\n- If applicable, documentation of any evaluation of alternative UPS technologies (rotary, DC) and their efficiency.",
        "recommendedActions": "- For all new and existing UPS installations, evaluate the feasibility and appropriateness of using their most energy-efficient operating modes (e.g., eco-mode, high-efficiency double conversion).\n- Conduct a risk assessment to determine which loads can safely be supported by higher-efficiency modes.\n- Configure UPS units accordingly and monitor their performance and efficiency in these modes.\n- When selecting UPSs for mechanical loads, ensure they are designed for such applications and can operate efficiently under variable load conditions.\n- Stay informed about the latest advancements and efficiency data for different UPS technologies.",
        "sampleResponseTemplate": "Our standard configuration for new UPS units ([Vendor/Model X]) includes enabling [e.g., 'intelligent eco-mode'] for loads classified as [e.g., Tier 2 or lower criticality], as per our '[UPS Operations Policy UPS-OP-002]'. This decision is based on a risk assessment ([Risk Assessment ID]). For UPS units feeding our CRAH VFDs, we selected models with high efficiency at variable partial loads (20-80%). We continuously monitor UPS operating efficiency via [Monitoring System].",
        "humanInputAnswer": "Our 'Critical Power Management Policy (CPMP-001)' allows for the use of high-efficiency 'eco-mode' on UPS systems serving non-critical administrative loads and development environments, following a site-specific power quality assessment. For our primary data hall UPS (VendorX ModularPower), we operate in high-efficiency double-conversion mode (97% efficient). UPS units powering our CRAH VFDs were specifically selected for their wide efficient operating range under variable mechanical loads. All evaluations of UPS tech are based on latest independent test reports.",
        "llmGeneratedAnswer": "Yes, our newly deployed UPS systems (e.g., 'MakeModel EnergySaver Series' in 'Site Delta') are operated in their most efficient mode appropriate for the supported load's criticality. As per 'UPS Operational Guideline UPSOG-2025', non-critical IT loads and some mechanical loads are supported by UPS units running in high-efficiency 'eco-mode' (achieving >98% efficiency), subject to continuous input power quality monitoring. For critical IT loads, we use high-efficiency double-conversion modes (typically 96-97% efficient). All UPS selections involve an evaluation of part-load efficiency curves, especially for units supplying variable mechanical loads like CRAC/CRAH fans, ensuring optimal performance across their typical operating range. We maintain a technology watch brief for alternative UPS technologies (Rotary, DC) based on unbiased industry data and reports.",
        "badExampleAnswer": "Our UPS systems are always in online double-conversion mode for maximum protection. We don't use eco-mode as it might be risky."
      },
      "question_117": {
        "id": 117,
        "old_question_text": null,
        "questionText": "For new builds or retrofits in Europe, has the organization eliminated the use of isolation transformers in power distribution to IT equipment (e.g., for stepping down to 120V) where they are not strictly required, understanding that they introduce additional, unnecessary transformer losses? Is this practice followed to improve overall electrical distribution efficiency?",
        "simplifiedQuestion": "For new/retrofitted DCs in Europe, do you avoid using unnecessary isolation transformers in the power path to IT gear to prevent energy losses?",
        "reference": "2025 CoC Guide 6.1.4",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_12",
        "recommendationTitle": "Elimination of Isolation Transformers",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "dwg",
            "docx"
          ],
          "description": "Electrical design documents (single-line diagrams, equipment schedules) for new builds/retrofits showing the power distribution architecture. Justification if any isolation transformers are used in the power path to IT equipment in European facilities."
        },
        "docRequired": "Yes, if new build/retrofit in Europe",
        "acceptedFiles": "pdf, dwg, docx",
        "documentDescriptor": "Mandatory for new builds/retrofits in Europe: Electrical design documentation demonstrating the elimination of unnecessary isolation transformers in the power distribution to IT equipment. If used, strong justification for their necessity.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of electrical single-line diagrams and design specifications for new builds/retrofits in Europe. Physical inspection of power distribution units and paths to IT equipment.",
        "nonconformity": {
          "major": "In new European builds/retrofits, isolation transformers are routinely included in the power distribution path to IT equipment without clear technical justification, introducing unnecessary energy losses.",
          "minor": "Some unnecessary isolation transformers are present in new designs, or their use is not critically evaluated for alternatives."
        },
        "majorNonconformity": "For new builds or retrofits in Europe, unnecessary isolation transformers are included in the power distribution design to IT equipment (e.g., for 120V conversion where not essential), leading to avoidable energy losses.",
        "minorNonconformity": "The use of isolation transformers in the IT power path is not critically reviewed in new designs, or they are included for reasons that could be addressed by more efficient alternatives (e.g., dual-corded equipment with direct 230V input).",
        "whatThisMeans": "In new data centres or major electrical upgrades in Europe, your organization should generally avoid using isolation transformers in the power lines going to your IT equipment, especially if they are just to step down voltage (like to 120V for some US-designed gear). These transformers themselves use energy (losses). Since most modern IT equipment can handle European voltages directly, these extra transformers are often not needed and just waste power.",
        "complianceRequires": "- For new builds or retrofits in Europe, eliminate isolation transformers in the power distribution path to IT equipment where they are not strictly required (e.g., for specific legacy equipment that cannot accept European voltages directly).\n- This is particularly relevant for avoiding transformers used to step down voltage to 120V for IT equipment, as most modern IT equipment is designed for direct 200-240V input.\n- Understand that isolation transformers introduce additional energy losses (typically 2-5% or more) unnecessarily if the IT equipment can accept the primary distribution voltage.",
        "expertTip": "Standardize on IT equipment that supports direct 230V (or local European nominal voltage) input. For any legacy equipment requiring lower voltages, assess if it can be replaced or if a small, dedicated point-of-use converter is more efficient than a larger upstream isolation transformer serving multiple devices.",
        "recommendedEvidence": "- Electrical single-line diagrams for new builds/retrofits in Europe showing the power distribution path to IT equipment and the absence of unnecessary isolation transformers.\n- IT equipment procurement standards specifying universal voltage input capabilities (e.g., 200-240V AC).\n- If any isolation transformers are used in the IT power path, provide clear technical justification for their necessity (e.g., specific medical or industrial equipment requirements).",
        "recommendedActions": "- Review and update electrical design standards for new data centres and retrofits in Europe to explicitly avoid the use of unnecessary isolation transformers in the power path to IT equipment.\n- Prioritize the procurement of IT equipment with universal power supplies capable of directly accepting local European grid voltages.\n- If isolation transformers are deemed essential for specific, limited applications, ensure they are highly efficient and correctly sized for their load.",
        "sampleResponseTemplate": "Our electrical design standard for European data centres ([Doc ID: EU-ELEC-STD-001]) prohibits the use of isolation transformers in the power path to IT equipment, as all our standard IT hardware accepts direct 230V input. This is reflected in the single-line diagrams for our new [Facility Name] facility (see [Drawing Ref: FN-SLD-002]). No isolation transformers are used for general IT power distribution.",
        "humanInputAnswer": "Our standard European data centre electrical design, as per 'EU_DC_Electrical_Spec_v4.pdf', feeds IT racks directly with 230V AC (Phase-Neutral) from PDUs. We do not employ isolation transformers for stepping down voltage to IT equipment, as all our procured servers, storage, and network devices since 2018 have universal power supplies (100-240V AC input). This eliminates the typical 2-4% losses associated with such transformers.",
        "llmGeneratedAnswer": "Yes, the organization's 'Electrical Design Standard for European Facilities (EDS-EU-2025, Section 5.3)' explicitly mandates the elimination of isolation transformers in the power distribution path to IT equipment, unless a specific, documented technical or regulatory requirement necessitates their use. All IT equipment procured for our European data centres is specified with universal power supplies capable of direct 230V AC operation. Consequently, our new 'Frankfurt Hub' (design completed Q1 2025) features a transformer-less power distribution architecture from the UPS output to the IT racks, thereby avoiding these additional points of energy loss.",
        "badExampleAnswer": "We sometimes use 120V PDUs for our US-made equipment, so we have a step-down transformer upstream to feed those. It's just easier for compatibility."
      },
      "question_118": {
        "id": 118,
        "old_question_text": null,
        "questionText": "For new builds or retrofits, has the organization ensured that the electrical infrastructure (including UPS, transformers, switchgear, PDUs) is designed to remain energy efficient under partial fill conditions and variable IT electrical loads, as also described in Practice 3.3.4 (Design infrastructure to maximise part load efficiency - a general principle)?",
        "simplifiedQuestion": "For new/retrofitted DCs, is your electrical infrastructure (UPS, transformers, PDUs) designed to be efficient even when not fully loaded or when IT loads vary?",
        "reference": "2025 CoC Guide 6.1.5",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_12",
        "recommendationTitle": "Efficient part load operation",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "dwg"
          ],
          "description": "Electrical system design philosophy emphasizing part-load efficiency. Specifications for UPS, transformers, PDUs showing good part-load efficiency curves or features. Control strategies or design choices (e.g., modularity, N+1 vs 2N) that support efficient part-load operation."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, xlsx, dwg",
        "documentDescriptor": "Mandatory for new builds/retrofits: Design documentation for electrical infrastructure (UPS, transformers, PDUs, etc.) demonstrating that components were selected and systems designed to maintain high energy efficiency under partial load conditions and variable IT loads, referencing Practice 3.3.4.",
        "verification_method": [
          "Document Review",
          "Equipment Specification Audit"
        ],
        "verificationMethods": "Review of electrical system design documents, equipment specifications (efficiency curves for UPS, transformers), and capacity management plans. Verification that part-load efficiency was a key criterion in component selection and system design.",
        "nonconformity": {
          "major": "New build/retrofit electrical infrastructure is designed primarily for peak load efficiency with little consideration for part-load performance, leading to significant inefficiencies during typical, often lower, operational loads.",
          "minor": "Part-load efficiency is considered for some electrical components (e.g., UPS) but not as a holistic system design principle, or the benefits of modularity for maintaining high part-load efficiency are not fully exploited."
        },
        "majorNonconformity": "For new builds or retrofits, the electrical infrastructure (UPS, transformers, switchgear, PDUs) is predominantly sized and selected based on full-load efficiency, without adequate measures to ensure efficient operation under expected partial fill and variable IT load conditions.",
        "minorNonconformity": "While some electrical components may have good part-load characteristics, the overall system design (e.g., number and sizing of parallel units, distribution architecture) is not optimized to maintain high efficiency across the typical range of IT loads.",
        "whatThisMeans": "When designing a new data centre's electrical systems or upgrading them, it's vital that components like UPS systems, transformers, and power distribution units (PDUs) are chosen and configured to be energy efficient not just at their full rated capacity, but also when the data centre isn't fully populated with IT gear or when the IT workload varies. This ties back to the general principle of designing all infrastructure for good part-load efficiency (as mentioned in Practice 3.3.4).",
        "complianceRequires": "- For new builds or retrofits, ensure the electrical infrastructure (including UPS, transformers, switchgear, PDUs) is designed to remain energy efficient under partial fill (data centre not at full IT capacity) conditions.\n- Ensure efficiency is maintained under variable IT electrical loads.\n- This practice directly relates to and reinforces the general principle outlined in Practice 3.3.4 ('Design infrastructure to maximise part load efficiency').\n- This includes selecting components with good efficiency curves across a wide load range and designing systems (e.g., modular UPS, right-sized transformers) that avoid significant efficiency drops at lower utilization levels.",
        "expertTip": "Specify electrical equipment (UPS, transformers) with flat efficiency curves that maintain high efficiency even at loads as low as 20-25%. For systems with parallel redundant components (like N+1 UPS modules), implement control strategies that optimize the number of active modules to keep them operating in their most efficient load range.",
        "recommendedEvidence": "- Electrical system design philosophy or Basis of Design document explicitly stating part-load efficiency as a key requirement for components like UPS, transformers, and PDUs.\n- Manufacturer datasheets or test reports for selected electrical equipment showing their efficiency curves across various load percentages.\n- Design choices that support efficient part-load operation, such as the use of modular UPS systems (Practice 6.1.1), right-sized transformers, and efficient distribution configurations.\n- Reference to how Practice 3.3.4 (general part-load efficiency design) was applied to the electrical infrastructure.",
        "recommendedActions": "- During the design of new electrical infrastructure or major retrofits, specify and select components (UPS, transformers, PDUs) that demonstrate high energy efficiency across the expected operational load range (e.g., 20% to 80% load), not just at full load.\n- Utilize modular designs for systems like UPS to allow capacity to be matched more closely to actual load, improving part-load efficiency.\n- Ensure transformers are 'right-sized' for their expected load profile to operate near their peak efficiency point.\n- Regularly review IT load growth and adjust modular electrical systems (e.g., add UPS modules) to maintain efficient operation.",
        "sampleResponseTemplate": "Our electrical infrastructure design for new builds, such as [Facility Gamma], prioritizes part-load efficiency as per Practice 3.3.4 and 6.1.5. We select [e.g., modular UPS systems with >96% efficiency from 25-75% load, and transformers compliant with EU Ecodesign Tier 2 for low no-load and part-load losses]. This ensures efficiency even during initial partial fill stages. Design details are in '[Electrical Design Standard EDS-003]'.",
        "humanInputAnswer": "Our 'Electrical Infrastructure Design Standard EIDS-2024' for new builds explicitly requires that all major components (UPS, transformers, PDUs) are selected based on their efficiency profile across 20-100% loading, not just peak efficiency. For example, our new modular UPS systems (VendorX ModUPS) maintain >96% efficiency from 25% load upwards per module, and we phase module deployment to keep active modules loaded above 40%. This aligns with our overarching goal in Practice 3.3.4.",
        "llmGeneratedAnswer": "Yes, our 'Data Centre Electrical System Design Guide (DCESDG-2025, Section 3.2)' mandates that all electrical infrastructure components for new builds or retrofits, including UPS systems, transformers, and PDUs, must be selected and configured to ensure high energy efficiency under partial load conditions and variable IT electrical demands. This is a direct application of Practice 3.3.4. We specify equipment with high efficiency curves across a wide operating range (typically 25%-75% load) and utilize modular designs (e.g., for UPS as per Practice 6.1.1) to ensure that active components are appropriately loaded. For instance, transformers selected for our 'Site Beta' expansion are EU Ecodesign Tier 2 compliant, ensuring minimized no-load and part-load losses.",
        "badExampleAnswer": "Our electrical systems are built to handle the full design load of the data centre. Efficiency is best when they are fully loaded."
      },
      "question_119": {
        "id": 119,
        "old_question_text": null,
        "questionText": "Has the organization considered reducing the engine heater temperature setpoint for standby generators, especially when these heaters are used to keep generators ready for rapid starts? Is it ensured that block heaters for standby generators are controlled to operate only when ambient temperature conditions warrant their use, and have manufacturer guidelines been consulted to understand any risk or reliability implications of such temperature setpoint reductions?",
        "simplifiedQuestion": "Have you considered lowering the temperature settings for your standby generator heaters (engine block/fuel) to save energy, ensuring it's safe and they only run when needed?",
        "reference": "2025 CoC Guide 6.2.1",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_12",
        "recommendationTitle": "Reduce engine-generator heater temperature set-point",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Standby generator operational procedures. Settings for engine block heaters and fuel heaters. Analysis or documentation of considerations for reducing heater setpoints, including manufacturer consultation and risk assessment."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: Documentation showing consideration or implementation of reduced temperature setpoints for standby generator engine/fuel heaters, including any risk/reliability assessment and manufacturer consultation. Controls for operating heaters only when warranted by ambient conditions.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "Interview"
        ],
        "verificationMethods": "Review of generator O&M procedures, heater control settings, and any analysis regarding setpoint reduction. Inspection of generator heater controls. Interviews with facilities staff managing generators.",
        "nonconformity": {
          "major": null,
          "minor": "Generator engine heaters operate continuously at high setpoints without consideration for ambient conditions or potential for safe setpoint reduction, leading to unnecessary energy waste (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Standby generator block heaters are not controlled based on ambient temperature conditions (i.e., they run constantly or at a fixed high setpoint irrespective of need), or no consideration has been given to safely reducing their temperature setpoint after consulting manufacturer guidelines.",
        "whatThisMeans": "Standby generators often have heaters to keep their engines warm for quick starts. Your organization should see if it's possible to lower the temperature setting on these heaters to save energy. Also, these heaters should ideally only turn on when it's actually cold enough to need them, not run all the time. It's important to check with the generator manufacturer to make sure any changes won't affect reliability or risk.",
        "complianceRequires": "- Consider reducing the engine heater temperature setpoint for standby generators, particularly those used for rapid starts.\n- Ensure that block heaters for standby generators are controlled to operate only when ambient temperature conditions genuinely warrant their use (e.g., below a certain outside air temperature).\n- Consult generator manufacturer guidelines to understand any potential risk or reliability implications associated with reducing heater temperature setpoints before making changes.",
        "expertTip": "Implement ambient temperature-based controls for generator block heaters, so they only energize when the outside temperature drops below a certain point (e.g., 5-10\u00b0C, depending on manufacturer recommendations and local climate). This can save significant energy compared to continuously energized heaters, especially in milder climates or seasons.",
        "recommendedEvidence": "- Standard operating procedures for standby generators, including settings for engine block heaters and fuel heaters.\n- Documentation of any analysis or risk assessment conducted regarding the reduction of heater setpoints.\n- Evidence of consultation with the generator manufacturer on acceptable heater setpoint ranges.\n- Configuration or specifications for controls that manage heater operation based on ambient temperature.",
        "recommendedActions": "- Review current engine heater setpoints and operating schedules for all standby generators.\n- Consult generator manufacturers for their recommendations on minimum safe heater temperatures for reliable starting in your climate.\n- Evaluate the feasibility of installing ambient temperature-based controls for generator heaters if not already present.\n- If deemed safe and viable, adjust heater setpoints and control logic to minimize energy consumption while ensuring generator readiness.",
        "sampleResponseTemplate": "We have reviewed the engine heater setpoints for our standby generators ([Generator Models]). Based on [manufacturer consultation/internal analysis - Ref: Doc ID], the block heaters for [Generator X] are now controlled by an ambient thermostat set at [Y\u00b0C], only operating when conditions warrant, instead of continuous operation. We estimate this saves [Z kWh/year].",
        "humanInputAnswer": "Our standby generator (Model Cat-XYZ) engine block heaters are thermostatically controlled. As per our 'Generator Maintenance Protocol (GMP-002)', the thermostat is set to activate the 7kW heater only when the ambient temperature in the generator enclosure drops below 7\u00b0C. This setting was confirmed with the manufacturer (Caterpillar Technical Bulletin TB-GEN789) to ensure rapid start capability while minimizing unnecessary heating energy. Previously, they were set to a fixed 20\u00b0C continuous.",
        "llmGeneratedAnswer": "Yes, the organization has considered and implemented measures to reduce standby generator heater energy consumption. Our 'Standby Power System SOP (SPS-SOP-2025, Section 6.1)' details that engine block heaters for our generators are controlled by ambient temperature sensors, activating only when the enclosure temperature falls below 5\u00b0C. This operational parameter was established after consultation with the generator manufacturer ([Manufacturer X]) to confirm no adverse impact on start-up reliability or engine longevity (Ref: 'Manufacturer_Consultation_Heaters_2024.pdf'). This practice has reduced heater energy consumption by an estimated 60% compared to continuous operation.",
        "badExampleAnswer": "The generator heaters are always on to make sure the generators start when we need them. We don't want to risk them not starting."
      },
      "question_120": {
        "id": 120,
        "old_question_text": null,
        "questionText": "Does the organization monitor, understand, and actively manage the consequences of Power Factor for both its Mechanical and Electrical (M&E) infrastructure and its installed IT equipment within the data centre? Is there an awareness that poor Power Factor management can lead to higher cable losses and introduce significant risk to the continuity of power supply (e.g., due to overloaded neutrals or utility penalties)? Is the use of Power Factor Correction (PFC) equipment considered and implemented where appropriate to mitigate these issues and improve electrical efficiency?",
        "simplifiedQuestion": "Do you monitor and manage your data centre's Power Factor to improve efficiency and reliability, using correction equipment if needed?",
        "reference": "2025 CoC Guide 6.2.2",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_12",
        "recommendationTitle": "Power Factor Correction",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "xlsx",
            "csv",
            "screenshots"
          ],
          "description": "Power quality monitoring data showing Power Factor. Analysis of Power Factor and its impact. Documentation of any Power Factor Correction (PFC) equipment installed (specifications, performance). Utility bills showing any Power Factor penalties (if applicable)."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, xlsx, csv, screenshots",
        "documentDescriptor": "Optional: Power quality monitoring reports showing Power Factor values. Documentation of any Power Factor Correction (PFC) strategy or equipment implemented. Analysis of benefits (e.g., reduced losses, avoidance of utility penalties).",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "Interview"
        ],
        "verificationMethods": "Review of power quality monitoring practices and data related to Power Factor. Examination of any PFC equipment specifications and performance. Interviews with electrical engineers or facilities managers.",
        "nonconformity": {
          "major": null,
          "minor": "Power Factor is not monitored or managed, potentially leading to higher electrical losses, equipment stress, or utility penalties, where improvement opportunities exist (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to monitor Power Factor systematically, or if poor Power Factor is identified, lack of consideration or implementation of appropriate Power Factor Correction (PFC) measures, leading to avoidable electrical losses and potential utility penalties.",
        "whatThisMeans": "Your organization should keep an eye on the 'Power Factor' of your data centre's electrical system (for both the building's M&E gear and the IT equipment). A poor Power Factor means your electrical system is working harder than it needs to, leading to more energy loss in cables and potentially risking your power supply (like overloaded wires or fines from your electricity company). If your Power Factor is poor, you should consider installing Power Factor Correction (PFC) equipment to improve it.",
        "complianceRequires": "- Monitor, understand, and actively manage the consequences of the Power Factor (PF) for both M&E infrastructure and installed IT equipment within the data centre.\n- Be aware that poor Power Factor management can lead to:\n    - Higher I\u00b2R losses in cabling (increased energy waste).\n    - Increased risk to power supply continuity (e.g., overloaded neutrals, transformer stress).\n    - Potential penalties and extra charges from the utility provider.\n- Understand that low-cost power supplies (often in older or non-enterprise IT equipment) can have very poor Power Factors with little or no correction, and their cumulative effect can be significant.\n- Consider and implement Power Factor Correction (PFC) equipment where appropriate to mitigate these issues and improve overall electrical efficiency.",
        "expertTip": "Implement continuous Power Factor monitoring at your main utility incomer and major distribution boards. Aim for a Power Factor of 0.95 or higher. Modern UPS systems and IT power supplies often have good built-in PFC, but older equipment or specific types of M&E loads (like certain motors) can degrade overall PF. Automatic PFC capacitor banks can be effective for centralized correction.",
        "recommendedEvidence": "- Reports from power quality monitoring systems showing Power Factor values at various points in the electrical distribution.\n- Documentation of any analysis conducted on Power Factor and its impact on energy losses or operational risks.\n- If PFC equipment is installed: specifications, location, and performance data (e.g., demonstrating improvement in PF).\n- Utility bills, if they show penalties or charges related to poor Power Factor (and subsequent elimination of these if PFC is implemented).",
        "recommendedActions": "- Implement power quality monitoring that includes continuous measurement of Power Factor at key points in the electrical system.\n- Regularly analyze Power Factor data to identify any issues or trends.\n- If Power Factor is consistently low (e.g., below 0.92-0.95), conduct an assessment to identify the primary causes (e.g., specific loads, harmonic distortion).\n- Evaluate the technical and economic feasibility of installing Power Factor Correction (PFC) equipment (e.g., capacitor banks, active PFC units).\n- If PFC is implemented, monitor its effectiveness in improving Power Factor and reducing losses/penalties.",
        "sampleResponseTemplate": "We monitor Power Factor at [our main switchboards / PDU level] using [Monitoring System Name]. Our current average Power Factor is [Value, e.g., 0.96]. We [have installed X kVAR of automatic PFC / determined PFC is not currently required due to Y]. This is reviewed [Frequency] as part of our power quality management program (Ref: '[PQM Program ID]').",
        "humanInputAnswer": "We continuously monitor Power Factor at our main utility incomers and major PDUs using Schneider PowerLogic meters. Our target is to maintain a PF of >0.95. Currently, Site A averages 0.97 and Site B averages 0.96. We have not needed dedicated PFC equipment as modern IT and UPS PSUs have high internal PFC. Utility bills confirm no PF penalties. This is reviewed monthly (Ref: 'PQ_Monthly_Report_Template').",
        "llmGeneratedAnswer": "Yes, our organization actively monitors and manages Power Factor. As per our 'Electrical System Management Protocol (ESMP-2025, Section 4.1)', power factor is continuously monitored at all main distribution boards and UPS outputs using our Power Monitoring Expert system. We aim to maintain a power factor of 0.95 or higher. Analysis in Q4 2024 indicated a dip to 0.92 in one legacy M&E plant room due to older motor loads. A 50kVAr automatic Power Factor Correction unit was subsequently installed in Q1 2025 (Ref: 'PFC_Install_Report_MER01.pdf'), restoring the PF in that section to 0.97 and reducing calculated cable losses by approximately 1.5%. This proactive management helps avoid utility penalties and minimizes I\u00b2R losses.",
        "badExampleAnswer": "We don't specifically track Power Factor. We assume our equipment is designed to be efficient."
      }
    }
  },
  "step_13": {
    "description": "2025 Guide Section 7: Other Data Centre Equipment (Practices 7.1.1 through 7.1.4 - General Practices)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Other_Equip_1"
    ],
    "critical_questions": [],
    "questions": {
      "question_121": {
        "id": 121,
        "old_question_text": null,
        "questionText": "Has the organization implemented a policy and procedures to ensure that lights are turned off in data centre technical spaces, plant rooms, and other ancillary areas when these spaces are unoccupied? Is this actively managed through manual switches with clear staff responsibility, or through automated systems like occupancy sensors or timers?",
        "simplifiedQuestion": "Do you have a policy and actively ensure lights are off in unoccupied data centre technical areas and plant rooms?",
        "reference": "2025 CoC Guide 7.1.1",
        "priority": "Low (Value: 1, Expected: Entire Data Centre)",
        "stepSection": "step_13",
        "recommendationTitle": "Turn off Lights",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "screenshots"
          ],
          "description": "Policy document regarding lighting control in unoccupied areas. Procedures for staff to turn off lights or documentation of automated lighting control systems (occupancy sensors, timers). Photographic evidence of 'lights off' policy in practice or sensor installations."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, jpg, screenshots",
        "documentDescriptor": "Mandatory: Policy or procedure for turning off lights in unoccupied DC technical spaces and plant rooms. Evidence of implementation (e.g., signage, automated controls like occupancy sensors, staff reminders).",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "Interview"
        ],
        "verificationMethods": "Review of lighting control policies and procedures. Physical inspection of data centre technical spaces and plant rooms during periods of inactivity to observe lighting status. Interviews with staff regarding lighting practices.",
        "nonconformity": {
          "major": "Lights are routinely left on in unoccupied data centre technical spaces, plant rooms, or ancillary areas without any policy or active measures (manual or automated) to turn them off.",
          "minor": "A 'lights off' policy exists but is not consistently enforced, or automated systems are not functioning correctly or are overridden without justification."
        },
        "majorNonconformity": "Systematic failure to turn off lights in unoccupied data centre technical spaces, plant rooms, and ancillary areas, leading to continuous unnecessary energy consumption from lighting.",
        "minorNonconformity": "A policy to turn off lights in unoccupied areas exists but is inconsistently followed by staff, or automated lighting controls (if present) are not maintained or are frequently overridden without need.",
        "whatThisMeans": "Your organization should have a clear rule and make sure that lights are switched off in data centre technical areas (like server rooms), plant rooms (where cooling/power equipment is), and other support areas whenever nobody is working in them. This can be done by staff manually switching them off, or by using automatic systems like sensors that detect if someone is in the room, or timers.",
        "complianceRequires": "- Implement a policy to turn off lights in data centre technical spaces, plant rooms, and other ancillary areas when these spaces are unoccupied.\n- Actively manage this through either:\n    - Manual switches with clearly assigned staff responsibility and awareness programs.\n    - Automated lighting control systems, such as occupancy sensors or timers.",
        "expertTip": "Install occupancy sensors with appropriate time delays in all infrequently accessed technical spaces (data halls, M&E rooms, storage areas). For areas with more frequent but intermittent access, consider manual switches with clear 'Turn Lights Off' signage and staff training. LED lighting often pairs well with occupancy sensors due to its instant-on capability.",
        "recommendedEvidence": "- Written policy or procedure on turning off lights in unoccupied data centre areas.\n- Signage in relevant areas reminding staff to turn off lights.\n- If automated controls are used: specifications or locations of occupancy sensors/timers, and their operational settings.\n- Photographic evidence during a site visit showing lights off in unoccupied areas or functioning occupancy sensors.",
        "recommendedActions": "- Develop and communicate a clear policy regarding turning off lights in unoccupied data centre technical spaces and plant rooms.\n- Install occupancy sensors or timers in appropriate areas to automate lighting control.\n- For manually controlled areas, ensure switches are clearly labeled and staff are trained on the policy.\n- Include checks on lighting status in regular data centre walkthroughs or audits.",
        "sampleResponseTemplate": "Our 'Data Centre Energy Conservation Policy' ([Doc ID: DC-ECP-003], Section 5) mandates that lights in all technical spaces and plant rooms are turned off when unoccupied. In [Data Hall X], this is managed by [PIR occupancy sensors with a 15-minute timer / manual switches with staff responsibility outlined in SOP Y]. Ancillary rooms like [Storage Z] also have [occupancy sensors/manual policy].",
        "humanInputAnswer": "Our 'Facility Operations Standard FOS-002' requires lights in data halls and M&E plant rooms to be off unless personnel are present. Data halls are equipped with motion-activated LED lighting zones that dim to 20% after 10 minutes of inactivity and turn off completely after 30 minutes. Plant rooms have manual switches with 'Lights Off When Unoccupied' signage, and this is part of the daily security walkthrough checklist.",
        "llmGeneratedAnswer": "Yes, the organization enforces a 'Lights Out Policy' for unoccupied data centre spaces, as detailed in our 'Energy Efficiency Operations Manual (EEOM-2025, Section 3.1)'. All data centre technical spaces, including server rooms and M&E plant areas, are equipped with occupancy sensors controlling LED lighting fixtures. These sensors are configured to turn lights off after 15 minutes of no detected occupancy. For areas with less frequent access, manual switches are supplemented by clear signage and staff protocols mandating lights be turned off upon exiting.",
        "badExampleAnswer": "The lights are usually on in the data centre so people can see if they need to go in. It's just easier to leave them on."
      },
      "question_122": {
        "id": 122,
        "old_question_text": null,
        "questionText": "For new builds, retrofits, or when replacing lighting fixtures, has the organization installed low energy lighting, such as LED (Light Emitting Diode) or high-frequency fluorescent lighting, throughout the data centre including technical spaces, plant rooms, and office areas?",
        "simplifiedQuestion": "When installing or replacing lights in your data centre (all areas), do you use energy-efficient types like LED or high-frequency fluorescent?",
        "reference": "2025 CoC Guide 7.1.2",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_13",
        "recommendationTitle": "Low energy lighting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Lighting design specifications for new builds/retrofits showing use of LED or high-frequency fluorescent fixtures. Purchase orders or inventory of installed lighting fixtures. Manufacturer datasheets for lighting fixtures confirming technology and efficiency."
        },
        "docRequired": "Yes, if new build/retrofit/replacement",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds/retrofits/lighting replacements: Specifications and evidence (e.g., purchase orders, datasheets) demonstrating installation of low energy lighting (e.g., LED, high-frequency fluorescent) throughout all data centre areas.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of lighting specifications, procurement records for lighting fixtures, and design documents for new/retrofitted areas. Physical inspection of installed lighting types in data halls, plant rooms, and office areas.",
        "nonconformity": {
          "major": "New lighting installations or full replacements predominantly use inefficient technologies (e.g., incandescent, old fluorescent) instead of LED or high-frequency fluorescent.",
          "minor": "Low energy lighting is used in some areas but not consistently throughout all data centre spaces during new builds, retrofits, or replacements, or older inefficient fixtures are retained where replacement would be beneficial."
        },
        "majorNonconformity": "For new builds, retrofits, or lighting replacements, failure to install low energy lighting (such as LED or high-frequency fluorescent) as the primary lighting technology throughout data centre technical spaces, plant rooms, and office areas.",
        "minorNonconformity": "While LED or high-frequency fluorescent lighting is used for some new installations or replacements, older, less efficient lighting technologies are still being installed in other areas, or there's no proactive plan to replace existing inefficient fixtures.",
        "whatThisMeans": "When you build a new data centre, do a major renovation, or simply replace old light fittings, your organization should install energy-efficient lighting. This means using technologies like LED (Light Emitting Diode) lights or modern high-frequency fluorescent lights. This should be done in all parts of the data centre building, including the server rooms, plant rooms where M&E gear is located, and any office spaces.",
        "complianceRequires": "- For new builds, retrofits, or when replacing existing lighting fixtures, install low energy lighting.\n- Preferred low energy lighting technologies include LED (Light Emitting Diode) or high-frequency fluorescent lighting.\n- This practice should be applied throughout the entire data centre facility, including technical spaces (data halls), plant rooms, and office areas.",
        "expertTip": "Prioritize LED lighting for all new installations and replacements due to its high efficacy (lumens per Watt), long lifespan, good color rendering, and compatibility with controls like dimming and occupancy sensors. When replacing fluorescent lighting, ensure proper disposal of old lamps containing mercury.",
        "recommendedEvidence": "- Lighting design plans or specifications for new builds/retrofits explicitly requiring LED or high-frequency fluorescent lighting.\n- Purchase records or invoices for new lighting fixtures, detailing the type and quantity of low energy lights installed.\n- Manufacturer datasheets for the installed lighting fixtures, confirming their technology (LED, high-frequency fluorescent) and energy efficiency ratings.\n- Photographic evidence of the installed low energy lighting in various data centre areas.",
        "recommendedActions": "- Establish a policy that all new lighting installations and replacements in data centre facilities must use LED or high-frequency fluorescent technology.\n- Conduct an audit of existing lighting to identify and prioritize the replacement of inefficient fixtures (e.g., incandescent, T12 fluorescent) with low energy alternatives.\n- When selecting low energy lighting, consider factors like appropriate light levels for different tasks and areas, color temperature, and compatibility with lighting controls.\n- Ensure proper recycling or disposal of old, inefficient lighting fixtures, especially those containing hazardous materials.",
        "sampleResponseTemplate": "Our standard for all new builds, retrofits, and lighting replacements ([Lighting Standard Doc ID: LIGHT-STD-001]) mandates the use of [LED lighting / high-frequency T5 fluorescent lighting]. Our new [Facility Name / Retrofitted Area X] exclusively uses [LED fixtures], with specifications detailed in [Lighting Design Plan Y]. We replaced [Number] old fluorescent fixtures with LEDs in [Year], saving an estimated [Z kWh/year].",
        "humanInputAnswer": "All lighting installed in our 'Data Centre Phoenix' (new build 2024) and any lighting retrofits in older facilities since 2023 utilize LED technology. This is mandated by our 'Building Services Standard BSS-004'. For example, the data halls in Phoenix use high-efficiency LED panel lights, and plant rooms use industrial LED battens. The office areas are also 100% LED. We estimate a 60% lighting energy reduction compared to traditional fluorescent lighting.",
        "llmGeneratedAnswer": "Yes, our organization exclusively installs low energy lighting for all new builds, retrofits, and when replacing existing fixtures. As per our 'Facility Design & Maintenance Standard (FDMS-2025, Section 7.2: Lighting Systems)', Light Emitting Diode (LED) technology is the mandated standard for all data centre areas, including technical spaces, plant rooms, and associated office areas. For instance, our 'Site Gamma Expansion' project (completed Q1 2025) features 100% LED lighting throughout, with typical fixture efficacy exceeding 120 lumens/Watt. This replaces legacy T8 fluorescent lighting in retrofitted sections, achieving significant energy savings.",
        "badExampleAnswer": "We replace light bulbs when they burn out with whatever standard bulbs are easily available."
      },
      "question_123": {
        "id": 123,
        "old_question_text": null,
        "questionText": "Has the organization ensured that for new builds or retrofits, fixtures and fittings within the data centre technical spaces and plant rooms (including walls, floors, ceilings, and IT equipment cabinets/racks where appropriate) are pale or light in colour? Is this done to maximize the reflectance of available lighting, potentially reducing the required lighting levels and therefore energy consumption for lighting?",
        "simplifiedQuestion": "For new/retrofitted DCs, do you use pale/light colours for walls, floors, ceilings, and racks to make lighting more effective and save energy?",
        "reference": "2025 CoC Guide 7.1.3",
        "priority": "Low (Value: 1, Expected: New build or retrofit)",
        "stepSection": "step_13",
        "recommendationTitle": "Pale coloured fixtures and fittings",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg"
          ],
          "description": "Design specifications for new builds/retrofits indicating colour choices for walls, floors, ceilings, and cabinets. Photographic evidence of light-coloured finishes in data centre spaces."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, jpg",
        "documentDescriptor": "Optional: Design specifications or photographic evidence showing use of pale/light colours for interior surfaces and fixtures (walls, floors, ceilings, cabinets) in new builds/retrofits to enhance lighting reflectance.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of design specifications for interior finishes. Physical inspection of data centre technical spaces and plant rooms to observe colours of surfaces and fixtures.",
        "nonconformity": {
          "major": null,
          "minor": "In new builds/retrofits, dark coloured fixtures and fittings are predominantly used, reducing lighting reflectance and potentially increasing lighting energy needs (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: For new builds or retrofits, interior surfaces and fixtures are predominantly dark, thereby not maximizing light reflectance and potentially requiring higher lighting levels than would be necessary with paler colours.",
        "whatThisMeans": "When designing or renovating data centre spaces (technical rooms, plant rooms), using pale or light colours for walls, floors, ceilings, and even IT racks can help make the lighting more effective. Light colours reflect more light, meaning you might not need as many lights or as bright lights to achieve the necessary visibility, which can save a small amount of energy.",
        "complianceRequires": "- For new builds or retrofits, ensure fixtures and fittings within data centre technical spaces and plant rooms are pale or light in colour.\n- This includes walls, floors, ceilings, and where appropriate, IT equipment cabinets/racks.\n- The aim is to maximize the reflectance of available lighting, which can potentially reduce the required lighting levels and thus the energy consumed by the lighting system.",
        "expertTip": "Opt for light grey, off-white, or beige for major surfaces like floors and walls. For cabinets, light grey or white is common and aids visibility within the cabinet. Ensure flooring materials are also anti-static and meet any load-bearing requirements.",
        "recommendedEvidence": "- Design specifications or mood boards for new builds/retrofits indicating the selection of light-coloured finishes for walls, floors, ceilings, and cabinets.\n- Manufacturer datasheets for selected paints, flooring materials, or cabinets showing their colour.\n- Photographic evidence of the completed spaces showcasing the light-coloured fixtures and fittings.",
        "recommendedActions": "- When planning new data centre builds or major refurbishments, include light colour selection for interior surfaces and fixtures in the design specifications.\n- Choose durable, light-coloured materials that are easy to clean and maintain.\n- Consider the impact of colour choice on overall lighting design and potential for energy savings.",
        "sampleResponseTemplate": "Our design standard for new data centre fit-outs ([Doc ID: DC-InteriorFitOut-Std-001]) specifies the use of light colours (e.g., light grey for floors, off-white for walls and ceilings, light grey for cabinets) to maximize light reflectance. This was implemented in our recent [Facility Name/Room X] refurbishment, as shown in the attached completion photos and [Design Specification Y].",
        "humanInputAnswer": "Our 'Data Centre Build Specification (DCBS-Rev2.1)' mandates light-reflective finishes. Walls and ceilings in our data halls are painted off-white (RAL 9010), floors are light grey anti-static vinyl, and our standard server cabinets are light grey. This was implemented in our 'Site Delta' build completed in 2024 to enhance visibility and optimize lighting efficiency.",
        "llmGeneratedAnswer": "Yes, for all new builds and major retrofits, our 'Data Centre Design and Fit-out Standard (DCDFS-2025, Section 6.2)' specifies the use of pale or light-coloured fixtures and fittings. This includes light grey (RAL 7035) for IT equipment cabinets and server racks, off-white for walls and ceilings, and light-coloured, sealed concrete or epoxy flooring in technical spaces and plant rooms. This practice is adopted to maximize the reflectance of our energy-efficient LED lighting, thereby improving visibility and potentially allowing for reduced lighting density or lower power settings while maintaining adequate illumination levels.",
        "badExampleAnswer": "We don't really have a standard for colours in the data centre; it depends on what the builders use or what colour racks we can get."
      },
      "question_124": {
        "id": 124,
        "old_question_text": null,
        "questionText": "When selecting new building management system (BMS) components, Power Distribution Units (PDUs), or other similar data centre equipment (excluding primary IT computing, storage, and network devices covered elsewhere), has the organization ensured that these devices contain capabilities to report their energy use and/or local temperature if this data is useful for energy management? Is it preferred that these devices report using common industry standard protocols (e.g., SNMP, Modbus, BACnet) without requiring additional vendor licenses for data access?",
        "simplifiedQuestion": "When buying new BMS parts, PDUs, or similar non-IT gear, do you ensure they can report their energy use and/or temperature via standard, license-free protocols if useful for energy management?",
        "reference": "2025 CoC Guide 7.1.4",
        "priority": "Medium (Value: 3, Expected: Optional Practice)",
        "stepSection": "step_13",
        "recommendationTitle": "Energy & temperature reporting hardware",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "Procurement specifications for BMS components, PDUs, and other non-IT equipment requiring energy/temperature reporting capabilities via standard protocols. Vendor documentation confirming these features. Screenshots or configurations from monitoring systems showing integration of this data."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Optional: Procurement specifications for non-IT data centre equipment (BMS components, PDUs, etc.) requiring built-in energy/temperature reporting via standard, license-free protocols. Evidence of this data being integrated into monitoring systems.",
        "verification_method": [
          "Document Review",
          "System Configuration Review"
        ],
        "verificationMethods": "Review of procurement specifications for BMS, PDUs, etc. Examination of equipment datasheets and monitoring system configurations to verify data reporting capabilities and integration.",
        "nonconformity": {
          "major": null,
          "minor": "New BMS components, PDUs, or other relevant non-IT equipment are selected without consideration for their energy/temperature reporting capabilities, or if present, these features are not utilized or require proprietary/licensed access (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Procurement of new BMS components, PDUs, or other relevant non-IT equipment does not prioritize or include evaluation of their capability to report energy use and/or local temperature via standard, license-free protocols, thereby missing opportunities for enhanced energy management visibility.",
        "whatThisMeans": "When your organization buys new equipment for managing your data centre \u2013 like parts for your Building Management System (BMS), Power Distribution Units (PDUs) that deliver power to racks, or similar gear (but not the main IT servers, storage, and network devices which are covered elsewhere) \u2013 you should choose models that can report how much energy they are using and/or their local temperature, if that information would be useful for managing energy. It's best if they can report this information using common industry standards (like SNMP, Modbus, or BACnet) without needing extra software licenses from the vendor to get the data.",
        "complianceRequires": "- When selecting new Building Management System (BMS) components, Power Distribution Units (PDUs), or other similar data centre equipment (excluding primary IT computing, storage, and network devices which are addressed in Practice 4.1.11), ensure these devices contain capabilities to report their own energy use and/or local temperature, provided this data is useful for energy management purposes.\n- Prefer devices that can report this data using common industry standard protocols (e.g., SNMP, Modbus, BACnet).\n- Ensure that access to this data via standard protocols does not require additional vendor licenses.",
        "expertTip": "Standardize on PDUs that offer branch circuit monitoring (energy and power for each outlet/group) and environmental sensor support (temperature/humidity at rack level), reporting via SNMP or Modbus. For BMS components, ensure they can expose all relevant operational data via BACnet or other open protocols to your central energy management platform.",
        "recommendedEvidence": "- Procurement specifications for new BMS components, PDUs, and other relevant non-IT data centre equipment, requiring energy and/or temperature reporting capabilities via standard, license-free protocols.\n- Vendor datasheets or technical documentation for selected equipment confirming these reporting features and supported protocols.\n- If implemented, screenshots or configuration details from a monitoring system showing the integration and collection of energy/temperature data from these non-IT devices.",
        "recommendedActions": "- Update procurement standards for relevant non-IT data centre equipment (BMS components, PDUs, etc.) to include requirements for embedded energy use and/or temperature reporting capabilities.\n- Prioritize equipment that uses open, standard protocols (SNMP, Modbus, BACnet) for data reporting and does not require additional licenses for data access.\n- When such equipment is installed, integrate its data feeds into your central monitoring or energy management system.\n- Utilize this data to gain better insights into the energy consumption patterns of your facility's supporting infrastructure.",
        "sampleResponseTemplate": "Our procurement specification for new [e.g., rack PDUs / BMS controllers] ([Doc ID: PDU-SPEC-002]) requires them to report [energy consumption (kWh, kW) / temperature] via [SNMP/Modbus] without additional licensing. For example, our recently installed [Vendor/Model X PDUs] provide per-outlet energy monitoring, which is integrated into our [DCIM/BMS System Name].",
        "humanInputAnswer": "Our standard for new intelligent Rack PDUs (rPDUs), as per 'PDU Acquisition Spec PDU-AS-003', mandates per-outlet energy monitoring (kWh, W, A, V) and support for rack-level temperature/humidity sensors, all accessible via SNMPv3 without additional licensing. The 'VendorX SmartPDU' units deployed in our new data hall meet this requirement, and their data is polled by our BMS.",
        "llmGeneratedAnswer": "Yes, when selecting new non-IT data centre equipment such as Building Management System (BMS) components and Power Distribution Units (PDUs), our 'Facility Infrastructure Procurement Policy (FIPP-2025, Section 4.1)' requires evaluation of their capability to report energy usage and/or local temperature, where such data is deemed beneficial for energy management. Preference is given to devices that provide this telemetry via industry-standard protocols (e.g., SNMP, Modbus, BACnet) without incurring additional vendor licensing costs for data access. For instance, all new rack PDUs specified since 2024 must offer per-outlet energy monitoring accessible via SNMP.",
        "badExampleAnswer": "Our PDUs just distribute power. We monitor power at the main UPS level. We don't check temperature on those smaller devices."
      }
    }
  },
  "step_14": {
    "description": "2025 Guide Section 8: Data Centre Building (Practices 8.1.1-8.1.5 from Sec 8.1 'Building Physical Layout'; and 8.2.1-8.2.3 from Sec 8.2 'Building Geographic Location')",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Building_1"
    ],
    "critical_questions": [],
    "questions": {
      "question_125": {
        "id": 125,
        "old_question_text": null,
        "questionText": "For new data centre builds or major retrofits, has the organization located Mechanical and Electrical (M&E) plant (including generators, chillers, transformers, switchgear, UPS) outside of the primary cooled area (e.g., data halls)? If not, is there a strong, documented justification for placing any such M&E plant within the cooled area, and are measures taken to minimize its heat contribution to that space?",
        "simplifiedQuestion": "For new/retrofitted DCs, is your M&E plant (generators, chillers, UPS, etc.) located outside the main cooled data hall areas?",
        "reference": "2025 CoC Guide 8.1.1",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_14",
        "recommendationTitle": "Locate M&E plant outside the cooled area",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "dwg",
            "docx"
          ],
          "description": "Data centre design documents (architectural and M&E layouts) showing placement of M&E plant. Justification if any M&E plant is located within primary cooled areas, including measures to mitigate its heat contribution."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, dwg, docx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Design plans showing M&E plant located outside primary cooled data hall areas. If any M&E plant is within cooled areas, provide justification and details of heat mitigation strategies.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of data centre architectural and M&E layout drawings. Physical inspection of the facility to verify the location of major M&E plant relative to cooled IT spaces.",
        "nonconformity": {
          "major": "In new builds/retrofits, significant M&E plant is located within the primary cooled data hall areas without strong justification or effective heat mitigation, increasing the cooling load.",
          "minor": "Some non-essential M&E plant is located within cooled areas without clear justification, or heat mitigation measures are not fully effective."
        },
        "majorNonconformity": "For new builds or major retrofits, significant heat-generating Mechanical and Electrical (M&E) plant (e.g., chillers, UPS, transformers) is located within the primary cooled data hall areas without a compelling, documented justification.",
        "minorNonconformity": "Some M&E plant is located within cooled IT spaces, and while justification may exist, measures to minimize its heat contribution to the cooled space are not fully optimized or documented.",
        "whatThisMeans": "When building a new data centre or doing a major renovation, large equipment like generators, chillers, transformers, switchgear, and UPS systems should be placed outside the main server rooms (data halls) that are actively cooled for IT equipment. If any of this M&E plant *must* be in a cooled area, there needs to be a very good reason for it, and steps must be taken to ensure it doesn't add unnecessary heat to that space.",
        "complianceRequires": "- For new data centre builds or major retrofits, locate Mechanical and Electrical (M&E) plant outside of the primary cooled area (e.g., data halls).\n- This includes equipment such as generators, chillers, transformers, switchgear, and Uninterruptible Power Supplies (UPS).\n- If M&E plant must be located within a cooled area, provide a strong, documented justification.\n- In such cases, implement measures to minimize the heat contribution of this M&E plant to the cooled space (e.g., direct exhaust, thermal barriers).",
        "expertTip": "Design dedicated plant rooms or external compounds for M&E equipment, separate from the IT data halls. This not only improves thermal efficiency by reducing heat loads in cooled spaces but can also enhance safety, maintenance access, and space utilization within the data halls.",
        "recommendedEvidence": "- Architectural and M&E layout drawings for new builds/retrofits clearly showing the location of major M&E plant (generators, chillers, UPS, transformers, switchgear) relative to the data halls.\n- If any M&E plant is located within cooled IT areas, provide the specific justification documentation for this design choice and details of any heat mitigation measures implemented.\n- Photographic evidence of M&E plant locations.",
        "recommendedActions": "- During the design phase of new data centres or major retrofits, prioritize locating all significant M&E plant outside of environmentally controlled IT spaces.\n- If specific M&E components must be within or adjacent to cooled areas, ensure their heat output is minimized or directly managed (e.g., ducted exhaust for UPS heat) to prevent impacting the primary IT cooling load.\n- Develop clear design guidelines that specify preferred locations for M&E plant with respect to energy efficiency.",
        "sampleResponseTemplate": "Our new [Facility Name] (design completed [Date]) locates all major M&E plant (chillers, generators, main switchgear, UPS systems) in dedicated plant rooms outside the data hall cooled envelope, as shown in architectural drawing [Drawing ID: ARCH-MEP-LAYOUT-001]. This minimizes extraneous heat loads on the data hall cooling systems. No M&E plant is located within the primary cooled areas.",
        "humanInputAnswer": "For our 'Site Omega' new build, all M&E plant, including Schneider Galaxy V-Series UPS, Caterpillar generators, and Trane chillers, are housed in dedicated plant rooms with direct ambient ventilation, entirely separate from the IT data halls. This is a standard in our 'DC Design Blueprint DB-2024', specifically section 4.1 'M&E Plant Segregation'. The only exception is in-row PDUs, which are within the data hall but are considered part of the IT power distribution.",
        "llmGeneratedAnswer": "Yes, in our 'Project NorthStar' new data centre build, all primary Mechanical and Electrical (M&E) plant \u2013 including generators, chillers, main transformers, switchgear, and UPS systems \u2013 are located in dedicated external plant compounds or specifically designed M&E rooms that are not part of the primary cooled IT environment. This is explicitly detailed in our 'NorthStar Facility Design Specification (NSFDS-Rev2, Section 3.2: Plant Location Strategy)'. This segregation minimizes the thermal load on the data hall cooling systems and allows for optimized environmental conditions for both the M&E plant and the IT equipment independently.",
        "badExampleAnswer": "Our UPS and some power distribution panels are in the server room because it's convenient for cabling and they are also kept cool that way."
      },
      "question_126": {
        "id": 126,
        "old_question_text": null,
        "questionText": "For new data centre builds or major retrofits, has the organization selected or created a building that provides sufficient 'slab to slab' separation or overall ceiling height within technical areas? Is this done to facilitate airflow management (e.g., effective hot air return paths via ceiling plenums, adequate height for hot/cold aisle containment or cabinet chimneys) and to allow for flexible installation of overhead services like cable trays, power distribution, and lighting without impeding critical airflow paths?",
        "simplifiedQuestion": "For new/retrofitted DCs, does your building have enough ceiling height in technical areas for good airflow management and overhead services?",
        "reference": "2025 CoC Guide 8.1.2",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_14",
        "recommendationTitle": "Select or create a building with sufficient 'slab to slab' separation / ceiling height",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "dwg",
            "docx"
          ],
          "description": "Architectural design documents for new builds/retrofits specifying slab-to-slab heights or clear ceiling heights in data halls and technical spaces. Justification for chosen heights related to airflow management and overhead service installation."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, dwg, docx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Architectural plans and design specifications showing adequate slab-to-slab or clear ceiling heights in technical areas to support effective airflow management (e.g., return plenums, containment) and overhead service installation.",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of architectural drawings and data centre design specifications. Physical inspection of new/retrofitted technical spaces to assess ceiling heights and space for overhead services and airflow management.",
        "nonconformity": {
          "major": "New build/retrofit data centre spaces have insufficient ceiling height, severely compromising effective airflow management (e.g., inability to implement proper containment or return plenums) or leading to congested overhead services that obstruct airflow.",
          "minor": "Ceiling height is adequate but not optimally utilized for airflow management, or minor congestion of overhead services impacts some airflow paths."
        },
        "majorNonconformity": "For new builds or major retrofits, the selected building or design provides insufficient slab-to-slab separation or clear ceiling height in technical areas, significantly hindering effective airflow management strategies (e.g., making hot aisle containment or adequate return plenums impractical).",
        "minorNonconformity": "While ceiling height is generally adequate, the design does not fully optimize it for airflow management (e.g., shallow return plenums) or allows overhead services to create notable obstructions to critical airflow paths.",
        "whatThisMeans": "When building a new data centre or doing a major renovation, it's important that the technical areas (like server rooms) have enough vertical space \u2013 either between the floor slab and the ceiling slab above it, or just a high overall ceiling. This height is needed for good airflow management, such as creating effective paths for hot air to return to the cooling units (e.g., using the space above a false ceiling as a plenum), properly installing hot or cold aisle containment systems, or using cabinet chimneys. It also ensures there's enough room to install overhead services like cable trays and lights without them blocking airflow.",
        "complianceRequires": "- For new data centre builds or major retrofits, select or create a building that provides sufficient 'slab to slab' separation or overall clear ceiling height within technical areas.\n- Ensure this height is adequate to facilitate effective airflow management strategies, such as:\n    - Creation of effective hot air return paths (e.g., via ceiling plenums).\n    - Installation of full-height hot or cold aisle containment systems.\n    - Use of cabinet chimneys or ducted exhaust.\n- Ensure sufficient height allows for flexible installation of overhead services (e.g., cable trays, power distribution busways, lighting fixtures) without these services impeding critical airflow paths.",
        "expertTip": "Aim for a clear height of at least 3 to 4 meters (10 to 13+ feet) above the finished floor in data halls, especially if using ceiling plenums for hot air return or implementing full aisle containment. Coordinate the layout of overhead services carefully to maintain clear pathways for airflow and access.",
        "recommendedEvidence": "- Architectural drawings for new builds/retrofits specifying slab-to-slab heights and clear finished ceiling heights in data halls and other technical spaces.\n- HVAC design documents showing how the available ceiling height is utilized for airflow management (e.g., return air plenum design, containment system dimensions).\n- Coordinated services drawings showing the layout of overhead cable trays, power distribution, lighting, and fire suppression systems, demonstrating clear airflow paths.",
        "recommendedActions": "- During the site selection or building design phase for new data centres, specify minimum slab-to-slab or clear ceiling height requirements for technical areas based on the planned airflow management strategy.\n- Optimize the layout of overhead services to minimize obstruction to airflow and ensure adequate access for maintenance.\n- If using ceiling plenums, ensure they are properly sealed and of sufficient depth to handle the required return air volume without excessive velocity or pressure drop.",
        "sampleResponseTemplate": "Our new [Facility Name] design specifies a slab-to-slab height of [e.g., 5.0 meters] in data halls, providing a clear raised floor to ceiling plenum height of [e.g., 3.5 meters]. This (Ref: '[Architectural Plan ID: ARCH-DH-Height-001]') facilitates our [e.g., hot aisle containment with ducted return via ceiling plenum] strategy and allows for non-obstructive overhead installation of cable trays and power busways.",
        "humanInputAnswer": "Our 'Data Centre Construction Standard DCS-001' mandates a minimum clear ceiling height of 4 meters above the raised floor in all new data halls. This was implemented in our 'Site Bravo' build (completed 2023) to allow for effective hot aisle containment (2.4m high racks + containment roof) and a substantial overhead return air plenum. Overhead services are routed in designated corridors above aisles to avoid airflow impedance.",
        "llmGeneratedAnswer": "Yes, for our 'Omega Facility' new build, the architectural design specified a slab-to-slab height of 5.5 meters in all technical data hall spaces. As detailed in 'Omega_Architectural_Design_Rev3.pdf (Section 4.2: Data Hall Vertical Space)', this provides a clear height of 3.8 meters above the 900mm raised floor. This generous height was intentionally designed to facilitate our chosen hot aisle containment strategy (full height containment with a ducted ceiling return plenum) and allows for layered overhead installation of power busways, fiber optic trays, and lighting systems without impeding critical return airflow paths or access.",
        "badExampleAnswer": "The building we use has standard office ceiling heights. We fit the racks in, but it's a bit tight overhead with all the cables and lights."
      },
      "question_127": {
        "id": 127,
        "old_question_text": null,
        "questionText": "When designing new data centres or undertaking major retrofits, has the organization ensured that the building design and physical layout specifically facilitates the use of 'Free Cooling' or economization techniques (as detailed in Practice 5.4.1.8 and its sub-practices 5.4.1.1 to 5.4.1.7)? Does this include considerations such as the orientation and location of air intakes/exhausts, provision of adequate space for larger air handling units or heat exchangers, and pathways for external ducting or pipework associated with free cooling systems?",
        "simplifiedQuestion": "For new/retrofitted DCs, is your building designed and laid out to make it easy to use 'Free Cooling' (e.g., good air intake/exhaust placement, space for equipment)?",
        "reference": "2025 CoC Guide 8.1.3",
        "priority": "High (Value: 5, Expected: New build or retrofit)",
        "stepSection": "step_14",
        "recommendationTitle": "Facilitate the use of \"Free Cooling\"",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "dwg",
            "docx"
          ],
          "description": "Architectural and M&E design documents showing how building layout and features facilitate free cooling. Site plans indicating air intake/exhaust locations. Space allocation for free cooling equipment (AHUs, heat exchangers, ductwork). Justification for design choices related to optimizing free cooling potential."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, dwg, docx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Design documentation demonstrating how the building's physical layout, orientation, and allocated spaces specifically facilitate the implementation and operation of chosen 'Free Cooling' or economization techniques (per 5.4.1.8 and sub-practices).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of architectural and M&E design plans, paying attention to features supporting free cooling (e.g., air intake/exhaust louvers, plant space, duct/pipe routes). Physical inspection of the building exterior and interior for these features.",
        "nonconformity": {
          "major": "New build/retrofit building design actively hinders or fails to make reasonable provisions for the implementation of viable free cooling techniques, despite local climate offering significant potential.",
          "minor": "Building design includes some consideration for free cooling, but layout or space allocation is not optimal, partially limiting the effectiveness or type of free cooling system that can be deployed."
        },
        "majorNonconformity": "For new builds or major retrofits, the building design and physical layout do not adequately consider or actively hinder the effective implementation of 'Free Cooling' or economization techniques, despite climatic feasibility (as per Practice 5.4.1.8).",
        "minorNonconformity": "While some free cooling is possible, the building layout or specific design choices (e.g., poor air intake/exhaust locations, insufficient plant space for economizers) limit the full potential or choice of the most effective free cooling strategies for the location.",
        "whatThisMeans": "When you design a new data centre or do a big renovation, the building itself should be laid out to make it as easy and effective as possible to use 'free cooling' techniques (like using outside air or water to cool, instead of just chillers \u2013 as covered in Practice 5.4.1.8 and its sub-parts). This means thinking about where fresh air intakes and warm air exhausts are located, making sure there's enough space for any large air handlers or heat exchangers needed for free cooling, and planning routes for any outside pipes or ducts.",
        "complianceRequires": "- Ensure the building design and physical layout for new data centres or major retrofits specifically facilitates the use of 'Free Cooling' or economization techniques.\n- This facilitation should be based on the prioritized free cooling strategy identified in Practice 5.4.1.8 and its sub-practices (5.4.1.1 to 5.4.1.7).\n- Key considerations include:\n    - Optimal orientation and location of air intakes and exhausts for air-side economizers.\n    - Provision of adequate plant space for larger air handling units, heat exchangers (air-to-air or water-to-water), or other free cooling equipment.\n    - Clear pathways and penetrations for external ducting or pipework associated with free cooling systems.",
        "expertTip": "Integrate free cooling considerations into the very earliest stages of site selection and building architectural design. For example, orienting the building to take advantage of prevailing winds or to shield air intakes from direct sun or pollution sources can enhance free cooling effectiveness. Ensure ample, easily accessible space for large economizer coils or air handlers.",
        "recommendedEvidence": "- Architectural and site layout plans showing the location of air intakes, exhausts, and external plant areas optimized for the chosen free cooling strategy.\n- M&E design drawings illustrating how the building layout accommodates free cooling equipment (e.g., large AHUs, ductwork, pipework, heat exchangers).\n- A design narrative or report explaining how the building design choices specifically support and maximize the potential of the selected free cooling system.\n- Confirmation that space and access for maintenance of free cooling components have been considered in the layout.",
        "recommendedActions": "- During the conceptual design of new data centres or major retrofits, explicitly consider how the building layout and orientation can maximize free cooling potential.\n- Allocate sufficient space in appropriate locations (e.g., rooftop, external compounds, dedicated plant rooms) for free cooling equipment and associated ductwork/pipework.\n- Design air intake and exhaust louvers to minimize pressure drop and prevent re-entrainment of exhaust air.\n- Ensure the building structure can support any large free cooling components.",
        "sampleResponseTemplate": "The design of our new [Cityville Data Centre] (Ref: '[Master Design Document ID: CVD-MDD-001]') was developed to maximize free cooling. The building is oriented [e.g., with air intakes facing north to minimize solar gain], and dedicated external yards are allocated for [e.g., large air-to-air heat exchangers / dry coolers with adiabatic assist]. Internal layouts include [e.g., large vertical shafts for ducting from rooftop AHUs]. This supports our chosen [indirect air economization] strategy (see 5.4.1.x).",
        "humanInputAnswer": "Our 'Greenfield Site Design Template (GFDT-Rev3)' requires that building orientation and layout prioritize free cooling. For our 'Site Sierra' build, the data halls are aligned to prevailing winds with large intake louvers on the windward side for our direct air economization system. Dedicated external plant areas are sized 50% larger than initial need to accommodate future expansion of free cooling components or alternative technologies. This is detailed in 'Sierra_Architectural_Concept_Final.pdf'.",
        "llmGeneratedAnswer": "Yes, our 'New Data Centre Design Standard (NDCDS-2025, Section 2.1: Site and Building Layout)' mandates that the physical building design must inherently facilitate the selected 'Free Cooling' or economization strategy (as determined per Practice 5.4.1.8). For instance, our 'Project Alpine' facility, designed for indirect air economization using air-to-air heat exchangers, features:\n- Building orientation to minimize solar gain on air intakes.\n- Dedicated, oversized external plant yards to house the heat exchanger modules and provide adequate airflow clearance.\n- Short, direct ducting routes between external units and internal air handlers, with appropriately sized wall/roof penetrations planned from inception.\nThis integrated design approach ensures optimal performance and scalability of the free cooling system.",
        "badExampleAnswer": "We find a building shell that fits our space needs, then we figure out how to fit the cooling system inside. Sometimes it's tricky to get big ducts in."
      },
      "question_128": {
        "id": 128,
        "old_question_text": null,
        "questionText": "For new data centre builds or major retrofits, has the organization carefully considered the location and orientation of external plant equipment (such as chillers, dry coolers, cooling towers, generators) to minimize environmental impacts (e.g., noise pollution on surrounding areas, visual impact) and maximize their operational efficiency (e.g., ensuring unimpeded airflow for air-cooled plant, avoiding direct prolonged solar gain on equipment, preventing recirculation of exhaust air into intakes)?",
        "simplifiedQuestion": "For new/retrofitted DCs, do you carefully place and orient external plant (chillers, towers, gensets) to minimize environmental impact and maximize their efficiency?",
        "reference": "2025 CoC Guide 8.1.4",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_14",
        "recommendationTitle": "Location and orientation of plant equipment",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "dwg",
            "docx"
          ],
          "description": "Site layout plans showing location and orientation of external plant equipment. Design documentation or studies demonstrating consideration of environmental impacts (noise, visual) and operational efficiency (airflow, solar gain, recirculation avoidance) for external plant placement."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, dwg, docx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Site plans and design documents showing considered placement and orientation of external M&E plant to minimize environmental impact and maximize operational efficiency (e.g., airflow, solar shading, avoiding exhaust recirculation).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of site layout plans, environmental impact assessments (if any), and M&E design documents related to external plant placement. Physical inspection of external plant areas.",
        "nonconformity": {
          "major": "In new builds/retrofits, external plant equipment is located or oriented without due consideration for environmental impacts or operational efficiency, leading to significant issues (e.g., noise complaints, poor plant performance due to airflow restrictions or recirculation).",
          "minor": "Some consideration given to external plant placement, but optimization for environmental impact and operational efficiency is not thorough or fully documented."
        },
        "majorNonconformity": "For new builds or major retrofits, external plant equipment (chillers, cooling towers, generators) is sited or oriented without proper consideration for minimizing environmental impacts (e.g., noise, visual) or maximizing operational efficiency (e.g., airflow access, prevention of exhaust recirculation, solar gain avoidance).",
        "minorNonconformity": "The placement of external plant shows some consideration for efficiency and environmental impact, but detailed analysis or optimization is lacking, or documentation of these considerations is incomplete.",
        "whatThisMeans": "When building a new data centre or doing a major upgrade, where you put your big outdoor equipment (like chillers, cooling towers, and generators) and how it's positioned matters. Your organization needs to think carefully about this to reduce negative effects on the surroundings (like noise or being an eyesore) and to make sure the equipment itself runs as efficiently as possible. This means ensuring good airflow for air-cooled gear, protecting equipment from excessive direct sun, and preventing hot exhaust air from being sucked back into air intakes.",
        "complianceRequires": "- For new data centre builds or major retrofits, carefully consider the location and orientation of external plant equipment (e.g., chillers, dry coolers, cooling towers, generators).\n- Minimize environmental impacts such as noise pollution on surrounding areas and visual impact.\n- Maximize operational efficiency by:\n    - Ensuring unimpeded airflow for air-cooled plant.\n    - Avoiding direct prolonged solar gain on equipment enclosures or heat exchange surfaces.\n    - Preventing recirculation of exhaust air (e.g., from cooling tower plumes or generator exhausts) back into equipment air intakes or building ventilation intakes.",
        "expertTip": "Use CFD modeling or wind engineering studies during the design phase to optimize the placement and orientation of external plant, especially for large installations or sites with complex surroundings. Consider prevailing wind directions, adjacent structures, and potential for future expansion. Acoustic barriers and visual screening can also be integrated into the site design.",
        "recommendedEvidence": "- Site layout plans showing the considered placement and orientation of all major external plant equipment.\n- Design narratives or reports explaining how environmental impacts (noise, visual) and operational efficiency factors (airflow, solar gain, recirculation) were considered in determining plant location and orientation.\n- If applicable, results from noise impact studies, CFD modeling for airflow, or solar shading analysis.\n- Photographic evidence of the installed external plant and any mitigating features (e.g., acoustic screens, strategic landscaping).",
        "recommendedActions": "- During the site planning and design phase for new data centres or major retrofits, conduct a thorough assessment for the optimal location and orientation of external plant equipment.\n- Consider environmental impact assessments and local planning regulations related to noise and visual amenity.\n- Design for adequate spacing between units and from building structures to ensure sufficient airflow and prevent recirculation.\n- Implement strategies to mitigate solar gain on sensitive equipment where appropriate (e.g., shading, reflective surfaces).",
        "sampleResponseTemplate": "The placement and orientation of external plant for our [Facility Name] (Design Ref: '[Site Plan ID SP-001]') was determined following an [e.g., environmental impact assessment and airflow study]. Chillers are located [Location, e.g., 'on the north side, shielded by an acoustic barrier'] to minimize noise and ensure clear airflow. Cooling tower plumes are oriented [Direction] to avoid recirculation into air intakes. We considered [solar gain/prevailing winds] in this placement.",
        "humanInputAnswer": "For our 'Project Terra' new build, the external plant layout (Drawing #TERRA-SITE-MEP-002) positions cooling towers downwind of prevailing summer breezes and ensures a minimum 10m separation from building air intakes to prevent plume recirculation. Chillers are located in a screened compound with acoustic louvers, minimizing visual and noise impact on adjacent properties. This was based on a site-specific environmental and microclimate study (Ref: 'TERRA_Env_Study_Final.pdf').",
        "llmGeneratedAnswer": "Yes, the location and orientation of external plant equipment for our new 'Alpha Data Hub' were key design considerations, detailed in 'ADH_Site_Layout_and_Plant_Strategy_Rev3.pdf'. For instance, air-cooled chillers are positioned on the north side of the building to minimize direct solar gain and are spaced according to manufacturer guidelines to ensure unimpeded airflow. Cooling tower discharge plumes are oriented away from prevailing winds and building air intakes to prevent recirculation. A noise impact assessment was also conducted to ensure compliance with local ordinances, resulting in the specification of low-noise fans and acoustic screening for the generator compound. This holistic approach maximizes operational efficiency while minimizing environmental impact.",
        "badExampleAnswer": "We put the chillers and generators where there was space available outside the building."
      },
      "question_129": {
        "id": 129,
        "old_question_text": null,
        "questionText": "For new data centre builds or major retrofits, has the organization designed the building to minimize direct solar heat gain on external walls, the roof, and through windows (if any are present in technical or plant areas)? Does this include considerations such as building orientation, appropriate insulation levels, use of reflective or 'cool roof' materials, solar shading (e.g., overhangs, louvers, external blinds), and minimizing glazed areas in spaces housing heat-generating equipment or requiring stable temperatures?",
        "simplifiedQuestion": "For new/retrofitted DCs, is your building designed to minimize heat gain from the sun (e.g., orientation, insulation, reflective roof, shading, minimal windows in tech areas)?",
        "reference": "2025 CoC Guide 8.1.5",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_14",
        "recommendationTitle": "Minimise direct solar heating",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "dwg",
            "docx"
          ],
          "description": "Architectural design documents for new builds/retrofits detailing strategies to minimize solar heat gain (building orientation, insulation specifications, roof material selection, window specifications and shading designs). Any thermal modeling or solar gain analysis performed."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, dwg, docx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Architectural and building design documentation illustrating measures taken to minimize direct solar heat gain (e.g., orientation, insulation levels, reflective roofing, solar shading, minimized glazing in technical areas).",
        "verification_method": [
          "Document Review",
          "Site Inspection"
        ],
        "verificationMethods": "Review of architectural design plans, building material specifications (insulation, roofing, glazing), and any solar gain studies. Physical inspection of the building envelope for features designed to minimize solar heat gain.",
        "nonconformity": {
          "major": "New build/retrofit building design does not incorporate any significant measures to minimize direct solar heat gain, leading to increased heat load on the cooling system.",
          "minor": "Some measures to reduce solar gain are included, but they are not comprehensive or optimized (e.g., standard insulation but no reflective roof or solar shading where beneficial)."
        },
        "majorNonconformity": "For new builds or major retrofits, the building design fails to incorporate reasonable measures to minimize direct solar heat gain on external walls, roofs, and through windows, leading to a significant avoidable heat load on the cooling system.",
        "minorNonconformity": "Some basic measures to reduce solar gain (like insulation) are included, but more advanced or impactful strategies (e.g., building orientation, reflective roofing, solar shading for windows in relevant areas) are not implemented where they would be beneficial.",
        "whatThisMeans": "When designing a new data centre building or doing a major renovation, it's important to minimize how much heat it absorbs from the sun. This can be done by orienting the building smartly, using good insulation, installing a reflective 'cool roof', providing shading for walls and windows (like overhangs or blinds), and having very few or no windows in areas with heat-producing equipment or those needing stable temperatures. Reducing solar heat gain means your cooling system doesn't have to work as hard.",
        "complianceRequires": "- Design the data centre building (new builds or major retrofits) to minimize direct solar heat gain on external walls and the roof.\n- Minimize solar gain through windows, especially in technical or plant areas where windows are generally not required or desired for operational reasons.\n- Considerations should include:\n    - Building orientation (e.g., minimizing east/west facing glazing and large wall surfaces).\n    - Appropriate levels of insulation for walls and roof.\n    - Use of reflective or 'cool roof' materials (high Solar Reflectance Index - SRI).\n    - Implementation of solar shading strategies (e.g., overhangs, louvers, external blinds, brise-soleil).\n    - Minimizing the area of glazing (windows) in spaces housing heat-generating equipment or requiring stable temperature control.",
        "expertTip": "Utilize building energy modeling software during the design phase to simulate the impact of different solar gain mitigation strategies (orientation, glazing types, shading, roof reflectivity) on the building's thermal load and cooling energy consumption. This can help optimize the design for both energy efficiency and cost-effectiveness.",
        "recommendedEvidence": "- Architectural design documents showing building orientation, window placement and specifications (e.g., U-value, Solar Heat Gain Coefficient - SHGC), and details of solar shading elements.\n- Specifications for building insulation (walls and roof) indicating R-values or U-values.\n- Specifications for roofing materials, including their Solar Reflectance Index (SRI) value if a 'cool roof' is implemented.\n- If thermal modeling was performed, a summary of the solar gain analysis and its influence on design decisions.",
        "recommendedActions": "- During the architectural design of new data centres or major retrofits, prioritize strategies to minimize solar heat gain.\n- Optimize building orientation to reduce exposure of large surfaces (especially glazing) to direct summer sun.\n- Specify high levels of insulation for the building envelope (roof and walls).\n- Select roofing materials with high solar reflectance (cool roofs).\n- Design effective solar shading for any windows, particularly on east and west facades.\n- Minimize or eliminate windows in data halls and critical plant rooms.",
        "sampleResponseTemplate": "Our new [Data Centre Building Name] design incorporates several measures to minimize solar heat gain, as detailed in '[Architectural Spec ID: ARCH-SOLAR-001]'. These include [e.g., 'a north-south building orientation', 'high-SRI white roofing material', 'minimal glazing on east/west facades with external solar shading', 'R-X insulation in walls and R-Y in roof']. A solar gain analysis indicated these measures reduce peak cooling load by an estimated [Z%].",
        "humanInputAnswer": "The design of our 'Phoenix Data Campus' (PDC-DesignManual-2024.pdf) focuses on minimizing solar gain. Buildings are oriented with their longest facades facing north/south. We use a high-SRI white TPO roofing membrane (SRI value of 105) and wall insulation achieving U-0.25 W/m\u00b2K. Windows in office areas have external louvers, and data halls have no external windows. This strategy is projected to reduce solar heat gain by over 30% compared to a baseline design.",
        "llmGeneratedAnswer": "Yes, for our new 'Terra Facility' build, minimizing direct solar heat gain was a key architectural design principle, documented in 'TF_Building_Envelope_Design_Spec_v2.pdf'. This includes:\n1.  Optimal building orientation to reduce east-west facade exposure.\n2.  High-performance insulation: R-30 for walls and R-40 for the roof.\n3.  A 'cool roof' system utilizing a white membrane with a Solar Reflectance Index (SRI) of 0.85.\n4.  Strategic solar shading using architectural overhangs for the limited office area glazing.\n5.  No windows in data hall or primary M&E plant room areas.\nThese measures are calculated to reduce the building's peak solar heat gain by approximately 25%, thus lowering the overall cooling load.",
        "badExampleAnswer": "The building has standard insulation and windows. We haven't done anything special to block the sun; the cooling system handles any heat."
      },
      "question_130": {
        "id": 130,
        "old_question_text": null,
        "questionText": "When selecting a site for a new data centre build, has the organization actively considered and prioritized locations where waste heat from the data centre can be effectively and economically reused by nearby heat consumers (e.g., district heating networks, industrial processes, greenhouses, residential or commercial buildings)? Does this consideration involve proactive engagement with local authorities, utility providers, and potential heat off-takers during the site selection and planning phase?",
        "simplifiedQuestion": "When choosing a new DC site, do you prioritize locations where its waste heat can be reused by nearby facilities/networks?",
        "reference": "2025 CoC Guide 8.2.1",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_14",
        "recommendationTitle": "Consider locating the Data Centre where waste heat can be reused",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Site selection criteria for new data centres including waste heat reuse potential. Feasibility studies for heat reuse at considered/selected sites. Correspondence with local authorities/utilities/heat off-takers regarding heat reuse."
        },
        "docRequired": "Yes, if new build",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds: Site selection documentation showing that the potential for waste heat reuse was a significant consideration. Feasibility studies and communications with potential heat off-takers or district heating networks.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of site selection criteria, feasibility studies for heat reuse, and correspondence with potential heat consumers. Interviews with personnel involved in site selection and strategic planning.",
        "nonconformity": {
          "major": "For new builds, the potential for waste heat reuse was not considered or investigated as part of the site selection process, despite available methodologies or obvious local opportunities.",
          "minor": "Waste heat reuse potential was considered superficially in site selection but not prioritized, or engagement with potential heat off-takers was minimal."
        },
        "majorNonconformity": "For new data centre builds, failure to actively consider and prioritize locations with viable opportunities for waste heat reuse during the site selection process.",
        "minorNonconformity": "While heat reuse may have been a minor consideration in site selection, no proactive engagement with potential local heat consumers or district heating networks was undertaken, or the assessment of reuse potential was not thorough.",
        "whatThisMeans": "When deciding where to build a new data centre, your organization should give serious thought to places where the waste heat produced by the data centre can be put to good use. This could mean locating near district heating systems that can use the heat, or close to industries, greenhouses, or buildings that need low-temperature heat. This involves talking to local councils, utility companies, and potential users of the heat early in the planning stages.",
        "complianceRequires": "- When selecting a site for a new data centre build, actively consider and prioritize locations that offer opportunities for the effective and economic reuse of data centre waste heat.\n- Identify potential nearby heat consumers such as district heating networks, industrial processes requiring low-grade heat, greenhouses, or large residential/commercial buildings.\n- Engage proactively with local authorities, utility providers, and potential heat off-takers during the site selection and initial planning phases to explore these opportunities.",
        "expertTip": "Collaborate with local urban planners and energy agencies during site selection. They can often provide insights into existing or planned district heating networks or large heat consumers who could benefit from data centre waste heat. Early partnerships can facilitate the necessary infrastructure development for heat reuse.",
        "recommendedEvidence": "- Site selection criteria documents for new data centres that explicitly include 'potential for waste heat reuse' as a key factor.\n- Feasibility studies or market research conducted to identify potential heat consumers near considered sites.\n- Records of communication (e.g., meeting minutes, letters) with local authorities, district heating operators, or industrial/commercial entities regarding waste heat reuse possibilities.\n- If a site was chosen based on heat reuse potential, documentation supporting this decision.",
        "recommendedActions": "- Incorporate 'waste heat reuse potential' as a formal criterion in your site selection process for all new data centre builds.\n- Actively research local heat demands and existing/planned heat networks in potential locations.\n- Initiate discussions with local municipalities, energy providers, and potential industrial/commercial heat users early in the site selection process.\n- Prioritize sites where partnerships for heat reuse are feasible and economically viable.",
        "sampleResponseTemplate": "Our site selection process for new data centres ([Doc ID: SiteSelect-PROC-001]) includes a mandatory assessment of waste heat reuse potential. For our new [Cityville] facility, we engaged with [Local District Heating Company] and identified a viable connection point. The proximity to their network was a key factor in selecting the site (Ref: '[Cityville Site Eval Report]', Section 4.2).",
        "humanInputAnswer": "During the site selection for our 'Nordic Hub' data centre (completed 2024), we prioritized locations adjacent to existing district heating networks. The chosen site in [City Name] allowed for direct connection to the municipal heating loop. A feasibility study (Ref: 'NH_HeatReuse_FS_2022.pdf') and subsequent agreement with [City Energy Corp] were key factors in this decision. The system is now operational, supplying up to 5MW of low-grade heat.",
        "llmGeneratedAnswer": "Yes, consideration for waste heat reuse is a significant factor in our site selection for new data centre builds. Our 'New Site Acquisition Protocol (NSAP-2025, Section 3.1.e)' mandates an evaluation of proximity to potential heat off-takers, such as district heating systems or industrial facilities. For our planned 'EcoDataPark Vienna', proactive engagement with Vienna's municipal energy provider, Wien Energie, regarding integration with their district heating network was initiated in the pre-selection phase. The positive outcome of this engagement (documented in 'Vienna_HeatReuse_MoU_2024.pdf') was a determining factor in finalizing the site choice.",
        "badExampleAnswer": "We choose sites based on power availability, fiber connectivity, and land cost. Reusing heat is something we might look at later if an opportunity comes up."
      },
      "question_131": {
        "id": 131,
        "old_question_text": null,
        "questionText": "When selecting a site for a new data centre build, has the organization prioritized locations in areas with low average ambient temperatures where feasible? Is it understood that this can significantly increase the potential for using 'Free Cooling' or economization techniques (as per Section 5.4.1), thereby reducing reliance on energy-intensive mechanical refrigeration and lowering overall energy consumption?",
        "simplifiedQuestion": "When choosing a new DC site, do you prioritize locations with cooler climates to maximize 'Free Cooling' opportunities?",
        "reference": "2025 CoC Guide 8.2.2",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_14",
        "recommendationTitle": "Locate the Data Centre in an area of low ambient temperature",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Site selection criteria for new data centres including analysis of local climate data (average ambient temperatures, humidity). Feasibility studies showing how chosen location impacts free cooling potential. Justification if a location with less favorable ambient conditions is selected over alternatives."
        },
        "docRequired": "Yes, if new build",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds: Site selection documentation showing that local climate (average ambient temperatures) was a significant consideration to maximize free cooling potential. Climate data analysis for selected and alternative sites.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of site selection criteria, climate data analysis, and free cooling feasibility studies. Interviews with personnel involved in site selection and data centre design.",
        "nonconformity": {
          "major": "For new builds, site selection does not prioritize or adequately consider local ambient temperature conditions and their impact on maximizing free cooling potential, leading to missed energy saving opportunities.",
          "minor": "Local climate is considered in site selection, but its impact on specific free cooling technology choices or achievable economization hours is not thoroughly analyzed or documented."
        },
        "majorNonconformity": "For new data centre builds, failure to prioritize or give significant consideration to locating in areas with low average ambient temperatures that would enhance 'Free Cooling' opportunities, where such site options are reasonably available.",
        "minorNonconformity": "While ambient temperature may be a minor factor in site selection, a detailed analysis of how different climates would impact free cooling hours and overall cooling energy consumption for the planned facility is not performed or documented.",
        "whatThisMeans": "When deciding where to build a new data centre, your organization should try to choose locations where the average outdoor temperature is low, if that's practical. Colder climates allow you to use 'free cooling' methods (using outside air or water for cooling instead of just chillers) for more of the year. This significantly reduces the energy used by mechanical refrigeration systems and lowers your overall energy bill.",
        "complianceRequires": "- When selecting a site for a new data centre build, prioritize locations in areas with low average ambient temperatures, where feasible and aligned with other business requirements.\n- Understand that lower ambient temperatures significantly increase the potential for using 'Free Cooling' or economization techniques (as detailed in Section 5.4.1).\n- Recognize that maximizing free cooling reduces reliance on energy-intensive mechanical refrigeration and lowers overall data centre energy consumption.",
        "expertTip": "Utilize long-term climate data (e.g., ASHRAE climate zones, TMY weather files) to compare potential sites. Model the expected annual free cooling hours for different free cooling technologies (direct air, indirect air, water-side economizers) at each candidate location to quantify the potential energy savings. Balance this with other critical site selection factors like power availability/cost, network connectivity, and land cost.",
        "recommendedEvidence": "- Site selection criteria documents that list 'favorable climate for free cooling' or 'low average ambient temperature' as a significant factor.\n- Comparative climate data analysis (e.g., temperature bin hours, wet-bulb hours) for different sites considered.\n- Feasibility studies for free cooling (as per 5.4.1.8) for the selected site, demonstrating how the local climate enables the chosen economization strategy.\n- If a location with a less favorable climate is chosen over a cooler alternative, provide justification based on other overriding business or technical factors.",
        "recommendedActions": "- Incorporate detailed local climate analysis (temperature, humidity, air quality) into the site selection process for all new data centres.\n- Prioritize locations that offer a high number of potential free cooling hours based on this analysis.\n- Model the energy impact of different climates on the planned cooling system design and PUE targets.\n- Ensure that the chosen free cooling technology is well-suited to the specific climatic conditions of the selected site.",
        "sampleResponseTemplate": "Our site selection for new data centres includes a 'Climatic Advantage Assessment' (Ref: '[Assessment Proc ID]'). For our new [Nordic/Alpine Facility Name], we prioritized a location with [e.g., >6000 annual hours below 15\u00b0C dry bulb] to maximize [direct air/indirect water] free cooling. This was a key factor over other sites with [e.g., higher power costs but warmer climates], as detailed in '[Site Selection Report ID: SSR-NF-001]'.",
        "humanInputAnswer": "The location for our 'Northern European Hub' (NEH) was selected in part due to its favorable climate, offering an average annual temperature of 8\u00b0C. Our site selection report (Ref: 'NEH_Site_Selection_Analysis_2022.pdf') includes a comparative analysis of free cooling potential across three shortlisted regions, showing the chosen location offered approximately 1,500 more full free cooling hours per year for our planned indirect air-to-air heat exchanger system.",
        "llmGeneratedAnswer": "Yes, when selecting sites for new data centre builds, our 'Global Site Selection Framework (GSSF-2025, Criterion 3.4: Climatic Suitability)' assigns a high weighting to locations with low average ambient temperatures to maximize 'Free Cooling' potential. For instance, our 'Arctic Circle Facility' (ACF), planned for 2027, was chosen after a detailed climate analysis (Ref: 'ACF_Climate_Impact_Study.pdf') demonstrated over 8,000 hours per year suitable for direct or indirect air-side economization, significantly reducing projected reliance on mechanical refrigeration. This directly supports our strategy outlined in Section 5.4.1 of the CoC.",
        "badExampleAnswer": "We build data centres where our customers need them or where land is cheap. The weather is just something our cooling system has to deal with."
      },
      "question_132": {
        "id": 132,
        "old_question_text": null,
        "questionText": "When selecting a site for a new data centre build, has the organization considered and, where feasible, avoided locating in areas with consistently high ambient humidity levels? Is it understood that high ambient humidity can reduce the effectiveness or increase the operational cost of some free cooling technologies (e.g., direct air economization due to latent load, evaporative/adiabatic cooling systems due to reduced evaporative potential), and may also increase the risk of corrosion for IT equipment if not properly managed, potentially necessitating more energy-intensive humidity control within the data centre?",
        "simplifiedQuestion": "When choosing a new DC site, do you try to avoid areas with constantly high outdoor humidity to improve free cooling effectiveness and reduce corrosion risks?",
        "reference": "2025 CoC Guide 8.2.3",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_14",
        "recommendationTitle": "Avoid locating the data centre in high ambient humidity areas",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Site selection criteria for new data centres including analysis of local climate data (ambient humidity levels, wet-bulb temperatures). Feasibility studies showing how chosen location's humidity profile impacts selected free cooling technology and internal humidity control strategies."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: Site selection documentation showing analysis of ambient humidity levels and consideration of its impact on free cooling efficiency and potential corrosion risks. Justification if a high humidity location is chosen.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of site selection criteria and climate data analysis, specifically regarding humidity. Interviews with personnel involved in site selection and HVAC design.",
        "nonconformity": {
          "major": null,
          "minor": "New data centre site selection does not include an assessment of local ambient humidity levels and their potential impact on free cooling performance or internal humidity control energy, even if other climate factors are considered (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to adequately consider or prioritize avoiding areas with consistently high ambient humidity during new data centre site selection, where such humidity levels could negatively impact the chosen free cooling technology's effectiveness or increase internal humidity control energy.",
        "whatThisMeans": "When your organization is picking a spot for a new data centre, it's a good idea to try and avoid places where the outside air is constantly very humid. High humidity can make some types of 'free cooling' (like those using direct outside air or evaporative cooling) less effective or more costly to run. It can also increase the risk of corrosion for your IT gear if not managed well, potentially requiring more energy to control humidity inside the data centre.",
        "complianceRequires": "- When selecting a site for a new data centre build, consider and, where feasible, avoid locations with consistently high ambient humidity levels.\n- Understand that high ambient humidity can:\n    - Reduce the effectiveness of certain free cooling technologies (e.g., direct air economization may be limited by upper humidity limits for IT equipment; evaporative/adiabatic cooling effectiveness is reduced when ambient wet-bulb temperature is high).\n    - Increase the operational cost of such free cooling systems.\n    - Potentially increase the risk of corrosion for IT equipment if external air is introduced without adequate humidity management.\n    - May necessitate more energy-intensive humidity control within the data centre itself.",
        "expertTip": "Focus on wet-bulb temperature data for site selection, as this is a key indicator for the potential of evaporative/adiabatic cooling and also influences the risk of condensation. If considering direct air economization, analyze the number of hours within the acceptable humidity range for your IT equipment (e.g., ASHRAE A2 allowable humidity).",
        "recommendedEvidence": "- Site selection criteria that include assessment of local ambient humidity profiles (e.g., average RH, wet-bulb temperature distributions).\n- Climate data analysis for considered sites, specifically highlighting humidity characteristics.\n- Feasibility studies for free cooling technologies that factor in the impact of local humidity levels on performance and achievable economization hours.\n- If a site with high ambient humidity is chosen, provide justification and details of the humidity management strategy for both free cooling systems and internal data centre environment.",
        "recommendedActions": "- Incorporate analysis of local ambient humidity levels (including seasonal variations and extreme values) into the site selection process for new data centres.\n- Evaluate how humidity will impact the performance and operational cost of the planned free cooling system.\n- If selecting a site with known high humidity, ensure the chosen cooling technology and internal humidity control systems are robust and energy-efficient under those conditions.\n- Consider the potential for increased filtration and corrosion mitigation measures if direct air economization is used in humid environments.",
        "sampleResponseTemplate": "Our site selection process includes an analysis of local humidity data (Ref: '[Climate Analysis Report ID]'). For our new [Facility Name], we [chose a location with moderate humidity / implemented indirect free cooling to isolate from high external humidity]. The impact of humidity on our [chosen free cooling system] was modeled, showing [e.g., X% reduction in economizer hours during peak summer humidity, which was deemed acceptable due to Y other factors].",
        "humanInputAnswer": "During the site selection for our 'Gulf Coast Facility', local high ambient humidity was a significant concern. Our analysis ('GCF_Climate_Study_2023.pdf') showed that direct air economization would be frequently curtailed by humidity limits. Therefore, we opted for a closed-loop chilled water system with cooling towers (see 5.4.1.3), which is less directly impacted by ambient air humidity for internal conditions, though tower efficiency is still wet-bulb dependent. This was deemed a more reliable and manageable approach for this specific climate.",
        "llmGeneratedAnswer": "Yes, when evaluating sites for new data centres, our 'Site Selection Environmental Criteria (SSEC-2025, Addendum B: Climatic Factors)' includes an assessment of ambient humidity levels. We prioritize locations where consistently high humidity (e.g., average relative humidity >75% for extended periods, or high average wet-bulb temperatures) is less prevalent, as this can negatively impact the efficacy of certain free cooling technologies like direct air or open-loop evaporative systems. For instance, a potential site in a tropical coastal region was deprioritized due to its sustained high humidity, which would have significantly limited economization hours and increased the latent load management requirements for the facility, as detailed in 'SiteCandidate_Humid_Impact_Analysis.pdf'.",
        "badExampleAnswer": "We look at temperature for free cooling, but humidity is harder to deal with so we just use standard AC if it's too humid outside."
      }
    }
  },
  "step_15": {
    "description": "2025 Guide Section 8: Data Centre Building (Practices 8.2.4-8.2.6 from Sec 8.2 'Building Geographic Location'; and 8.3.1-8.3.3 from Sec 8.3 'Water sources')",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Building_2"
    ],
    "critical_questions": [],
    "questions": {
      "question_133": {
        "id": 133,
        "old_question_text": null,
        "questionText": "When selecting a site for a new data centre build, has the organization considered proximity to an abundant source of 'free cooling' that can be practically and economically utilized, such as a large body of cold water (lake, river, sea) or areas with readily accessible cool groundwater? Is this evaluation done in conjunction with an assessment of the feasibility and environmental impact of using such a source (as per Practice 5.4.1.7 'Alternative cooling sources')?",
        "simplifiedQuestion": "When choosing a new DC site, do you consider locating near abundant natural 'free cooling' sources like cold water bodies or groundwater?",
        "reference": "2025 CoC Guide 8.2.4",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_15",
        "recommendationTitle": "Locate near a source of free cooling",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Site selection criteria including proximity to natural free cooling sources. Feasibility studies for utilizing such sources (e.g., lake/river/sea water cooling, geothermal). Environmental impact assessments related to the use of these sources."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: Site selection documentation showing consideration of proximity to abundant natural free cooling sources (large water bodies, groundwater). Feasibility studies and environmental assessments if this option was seriously evaluated or pursued.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of site selection evaluations and feasibility studies for alternative cooling sources. Interviews with personnel involved in site selection and sustainable design.",
        "nonconformity": {
          "major": null,
          "minor": "Potential for locating near an abundant natural free cooling source was overlooked or not adequately investigated during site selection, where such opportunities might exist (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to adequately investigate or leverage proximity to abundant natural 'free cooling' sources (e.g., large cold water bodies, accessible cool groundwater) during site selection, where such an approach could be practical and economically beneficial.",
        "whatThisMeans": "When picking a location for a new data centre, your organization should think about whether there are any nearby natural sources that could provide 'free cooling'. This might be a large, cold lake, river, or the sea, or even cool water underground. Using these sources could be very energy efficient, but you'd need to study if it's practical, economical, and environmentally okay (as mentioned in Practice 5.4.1.7 about alternative cooling sources).",
        "complianceRequires": "- When selecting a site for a new data centre build, consider proximity to abundant sources of 'free cooling' that are practical and economical to utilize.\n- Examples include large bodies of cold water (lakes, rivers, sea) or areas with readily accessible cool groundwater.\n- This evaluation should be conducted in conjunction with a feasibility and environmental impact assessment for using such sources, as described in Practice 5.4.1.7 ('Alternative cooling sources').",
        "expertTip": "Engage with hydrogeological and environmental consultants early in the site selection process if considering natural water bodies or groundwater for cooling. Permitting, sustainable abstraction rates, and potential impacts on local ecosystems are critical factors that need thorough investigation. Consider both direct and indirect use of these water sources.",
        "recommendedEvidence": [
          "- Site selection reports or feasibility studies that explicitly evaluate proximity to and potential use of natural free cooling sources (lakes, rivers, sea, groundwater).\n- Correspondence or consultations with environmental agencies or water authorities regarding the use of such resources.\n- If implemented, design documents for the system utilizing the natural free cooling source and any associated environmental permits."
        ],
        "recommendedActions": [
          "- During site selection for new data centres, identify if any locations offer practical access to abundant natural free cooling sources.\n- Conduct a preliminary assessment of the technical feasibility, economic viability, and environmental sustainability of utilizing such sources.\n- If promising, undertake detailed engineering and environmental impact studies.\n- Ensure full compliance with all local regulations and water usage rights if proceeding with such a solution."
        ],
        "sampleResponseTemplate": "Our site selection criteria include an assessment for proximity to natural free cooling sources. For [Project X], we evaluated [e.g., 'using local river water for condenser cooling / geothermal potential']. The outcome was [e.g., 'deemed not viable due to environmental restrictions' / 'found to be feasible, and is detailed in Feasibility Study Y - see also 5.4.1.7'].",
        "humanInputAnswer": "For our 'FjordSide Data Centre' project, a primary site selection criterion was direct access to deep cold fjord water for indirect heat exchange. The feasibility study (Ref: 'FS_FjordCool_v1.2') confirmed year-round availability of water at 4-7\u00b0C, allowing for 100% free cooling via titanium plate heat exchangers. All environmental permits (Ref: Permit #ENV-FJORD-001) have been secured. This aligns with our strategy for alternative cooling sources (5.4.1.7).",
        "llmGeneratedAnswer": "Yes, during the site selection phase for our 'BlueWater Data Hub', proximity to an abundant source of free cooling was a key consideration. We evaluated several coastal locations for the potential use of seawater cooling (indirect via heat exchangers). The chosen site (detailed in 'BWDH_Site_Selection_Report_2024.pdf') offers access to deep seawater at consistently low temperatures. A comprehensive environmental impact assessment and technical feasibility study (Ref: 'BWDH_SeawaterCooling_EIA_FS.pdf'), as per Practice 5.4.1.7, confirmed the viability and significant energy saving potential of this approach. The system is currently in the detailed design phase.",
        "badExampleAnswer": "We always look for sites with good road access and power. Using lake or river water seems too complicated and probably has too many environmental rules."
      },
      "question_134": {
        "id": 134,
        "old_question_text": null,
        "questionText": "When selecting a site for a new data centre build, has the organization given consideration to locating the facility near a renewable or lower carbon power source (e.g., solar farm, wind farm, hydroelectric plant, nuclear power station, or other low carbon generation capacity)? Is it understood that while this does not directly improve the PUE of the data centre, it significantly reduces the carbon footprint associated with its energy consumption and supports overall sustainability goals?",
        "simplifiedQuestion": "When choosing a new DC site, do you consider locating near renewable or low-carbon power sources to reduce your carbon footprint?",
        "reference": "2025 CoC Guide 8.2.5",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_15",
        "recommendationTitle": "Co-locate with power source",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Site selection criteria including proximity to renewable/low-carbon power sources. Feasibility studies or evaluations of co-locating with such power sources. Agreements or MOUs if co-location or direct power purchase from such sources is implemented."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: Site selection documentation showing consideration of proximity to renewable or low-carbon power sources. If co-location or direct sourcing is pursued, provide relevant studies, agreements, or plans.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of site selection criteria and feasibility studies regarding co-location with or proximity to renewable/low-carbon power generation. Interviews with personnel involved in site selection and energy procurement.",
        "nonconformity": {
          "major": null,
          "minor": "Opportunities for co-locating new data centres with or near significant renewable/low-carbon power sources are not actively investigated during site selection, missing potential for reduced carbon footprint (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Site selection for new data centres does not adequately consider or prioritize co-location with, or proximity to, renewable or lower carbon power sources, where such opportunities might be available and economically feasible.",
        "whatThisMeans": "When your organization is choosing a location for a new data centre, you should think about placing it near a source of renewable or low-carbon electricity. This could be near a solar farm, wind farm, hydroelectric plant, or even a nuclear power station. While this won't directly make your data centre more energy efficient (improve its PUE), it can greatly reduce the carbon emissions associated with the energy it uses, which is good for overall sustainability.",
        "complianceRequires": "- When selecting a site for a new data centre build, consider locating the facility near a renewable or lower carbon power source.\n- Examples include solar farms, wind farms, hydroelectric plants, nuclear power stations, or other low carbon electricity generation capacity.\n- Understand that while this co-location does not directly improve the PUE of the data centre itself, it significantly reduces the carbon footprint associated with its energy consumption.\n- This practice supports broader corporate sustainability goals and contributes to decarbonization.",
        "expertTip": "Explore opportunities for direct Power Purchase Agreements (PPAs) with nearby renewable energy generators. Co-location can sometimes simplify the contractual and physical connections for such agreements. Also, consider sites with strong grid connections that have a high percentage of renewables in their existing energy mix.",
        "recommendedEvidence": [
          "- Site selection criteria documents that include 'proximity to renewable/low-carbon power sources' or 'availability of direct renewable PPAs' as a factor.\n- Feasibility studies evaluating the benefits and challenges of co-locating with specific power sources.\n- If co-location or a direct PPA with a nearby renewable source is implemented, provide documentation of the agreement and the expected carbon footprint reduction.\n- Maps or analysis showing the proximity of selected sites to relevant power generation facilities."
        ],
        "recommendedActions": [
          "- Incorporate the assessment of proximity to renewable or low-carbon power sources into the site selection process for new data centres.\n- Identify regions or specific sites that offer advantages in terms of access to renewable energy (either direct co-location or through grid mix).\n- Engage with renewable energy developers and utility providers to explore opportunities for direct sourcing or co-location.\n- Factor the long-term carbon footprint reduction benefits into the overall business case for site selection."
        ],
        "sampleResponseTemplate": "Our site selection due diligence for new data centres includes an analysis of proximity to renewable/low-carbon power sources (Ref: '[Site Selection Checklist ID]', criterion 5.3). For our [Project Alpha], we are [e.g., 'actively seeking a site adjacent to a planned solar farm for a direct PPA' / 'prioritizing locations in regions with high grid renewable content']. While not directly impacting PUE, this aligns with our corporate CO2 reduction targets.",
        "humanInputAnswer": "Our 'Sustainable Siting Policy SSP-001' requires an evaluation of co-location opportunities with renewable energy sources for all new Tier 1 data centres. For our upcoming 'Prairie Hub' facility, we are in advanced discussions for a direct PPA with a new 200MW wind farm being developed 5km from the shortlisted site. This is projected to supply over 60% of the data centre's annual energy needs with certified renewable power, significantly lowering its Scope 2 emissions. (MoU Ref: PH-WindPPA-MoU-2024).",
        "llmGeneratedAnswer": "Yes, when selecting sites for new data centre builds, our 'Strategic Site Acquisition Framework (SSAF-2025, Section 4.2)' includes a criterion for 'Proximity to Low-Carbon Power Generation'. This involves assessing the feasibility of locating near renewable energy sources (e.g., solar, wind, hydro) or other low-carbon power plants like nuclear facilities. While this doesn't directly alter the data centre's PUE, it is a key factor in our strategy to minimize the overall carbon footprint of our operations. For our 'Nordic Arc' project, for instance, the site was chosen in part due to its direct access to a grid predominantly powered by hydroelectricity and its proximity to a major new wind farm development, with whom we are exploring a long-term PPA.",
        "badExampleAnswer": "We choose data centre sites based purely on real estate cost, power grid stability, and network latency to our main business operations."
      },
      "question_135": {
        "id": 135,
        "old_question_text": null,
        "questionText": "Has the organization, when selecting a site for a new data centre build or undertaking major retrofits, considered the availability of consistent and compatible local opportunities for waste heat reuse (as also described in Practice 8.2.1)? Does this evaluation involve assessing the technical and commercial opportunities for using the 'waste' heat produced by the data centre, such as for district heating systems or industrial processes, with potential benefits for the data centre operator, the heat user, and the wider community?",
        "simplifiedQuestion": "When choosing/retrofitting a DC site, do you specifically look for consistent local opportunities (like district heating, industry) to reuse your waste heat?",
        "reference": "2025 CoC Guide 8.2.6",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_15",
        "recommendationTitle": "Consider Heat re-use",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Site selection criteria/retrofit planning documents explicitly including evaluation of local heat reuse opportunities. Feasibility studies, market analysis, or correspondence with potential local heat off-takers (district heating, industry, etc.). Technical and commercial viability assessments for heat reuse at specific sites."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Mandatory for new builds/retrofits: Documentation demonstrating that consistent and compatible local opportunities for data centre waste heat reuse (e.g., for district heating, industrial processes) were actively considered and evaluated as part of site selection or retrofit planning.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of site selection reports, retrofit planning documents, feasibility studies for heat reuse, and any communications with potential heat consumers. Interviews with personnel involved in strategic planning and site development.",
        "nonconformity": {
          "major": "For new builds/retrofits, no active consideration or evaluation of local opportunities for waste heat reuse was undertaken during site selection or planning, despite potential for such schemes.",
          "minor": "Local heat reuse opportunities were considered but assessment was superficial, or viable technical/commercial opportunities were dismissed without strong justification."
        },
        "majorNonconformity": "For new data centre builds or major retrofits, failure to actively consider and evaluate available, consistent, and compatible local opportunities for reusing the facility's waste heat (e.g., for district heating or industrial processes).",
        "minorNonconformity": "While local heat reuse might be mentioned in planning, there is no documented, thorough technical and commercial assessment of specific local opportunities, or engagement with potential heat off-takers is lacking.",
        "whatThisMeans": "When you're picking a location for a new data centre or planning a major upgrade, your organization needs to specifically look for reliable and suitable local options to use the waste heat it will produce. This means checking if there are nearby district heating networks, industries, or other facilities that could consistently use this heat. You should evaluate both if it's technically possible and if it makes commercial sense for everyone involved. This is a more direct focus on finding heat reuse opportunities compared to just generally considering locations where reuse *might* be possible (as in Practice 8.2.1).",
        "complianceRequires": "- When selecting a site for a new data centre build or planning major retrofits, actively consider the availability of consistent and compatible local opportunities for waste heat reuse (this practice is closely related to, and an extension of, Practice 8.2.1).\n- Evaluate both the technical and commercial opportunities for using the 'waste' heat produced by the data centre.\n- Potential applications include district heating systems, industrial processes, greenhouses, aquaculture, etc.\n- Assess the potential benefits for the data centre operator (e.g., revenue, enhanced sustainability profile), the heat user (e.g., lower cost energy), and the wider community (e.g., reduced local carbon emissions).",
        "expertTip": "Develop a 'heat map' of potential data centre locations overlaid with existing or planned district heating networks and major industrial heat users. Proactively engage with city planners and energy agencies to identify symbiotic opportunities where data centre waste heat can become a valuable resource for the local community or industry. Consider the temperature grade of your waste heat (higher is often better for reuse) when designing your cooling systems.",
        "recommendedEvidence": [
          "- Site selection reports or retrofit planning documents that include a specific section on the evaluation of local waste heat reuse opportunities.\n- Market research or feasibility studies identifying potential local off-takers for data centre waste heat (e.g., district heating operators, industrial facilities, agricultural businesses).\n- Records of discussions or correspondence with potential heat consumers regarding their requirements and interest.\n- Technical and commercial viability assessments for specific heat reuse schemes at considered or selected sites.\n- If implemented, contracts or agreements for heat supply."
        ],
        "recommendedActions": [
          "- Make the active evaluation of local, consistent, and compatible heat reuse opportunities a mandatory part of the site selection process for new data centres and planning for major retrofits.\n- Identify potential heat off-takers in the vicinity of candidate sites.\n- Assess the quantity, quality (temperature), and consistency of waste heat that the data centre will produce.\n- Evaluate the technical requirements and costs for heat recovery and distribution infrastructure.\n- Engage in commercial discussions with potential heat consumers to determine viability."
        ],
        "sampleResponseTemplate": "As part of our site selection for [Project Indigo], we specifically evaluated local heat reuse opportunities (Ref: '[Indigo Site Eval - Heat Reuse Addendum]'). We identified [e.g., 'a municipal district heating network with expansion plans' / 'a large greenhouse complex within 1km'] as a potential consistent off-taker. Technical and preliminary commercial discussions [are underway/concluded X]. The potential to supply [Y MW] of heat was a factor in [decision].",
        "humanInputAnswer": "For our 'City Central DC' retrofit project, we proactively assessed local heat reuse opportunities. Our study (Ref: 'CCDC_HeatReuse_Eval_2024.pdf') identified a new residential development 500m away with a planned district heating loop. We are now in detailed design to supply up to 2MW of upgraded waste heat (using heat pumps, see 5.7.2) to this network. This was a key sustainability objective for the retrofit, considered from the initial planning stages.",
        "llmGeneratedAnswer": "Yes, during the site selection for our 'Metro North Data Hub' (new build initiated 2024), a primary evaluation criterion was the availability of consistent and compatible local opportunities for waste heat reuse. Our 'Site Selection Due Diligence Report (MNDH-SS-DDR-001, Section 6.2)' details the assessment of connecting to the 'City Metro District Energy System'. This involved technical feasibility studies for heat export (e.g., required temperature, flow rates) and commercial discussions with the district energy operator. The positive outcome of this evaluation significantly influenced the final site choice, aligning with both practice 8.2.1 and reinforcing 8.2.6 by actively seeking a compatible reuse opportunity.",
        "badExampleAnswer": "If someone local asks us for heat after we build, we might consider it, but we don't usually look for it beforehand."
      },
      "question_136": {
        "id": 136,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented systems for capturing rainwater from the data centre roof or other suitable surfaces? Is this captured rainwater primarily intended for use in adiabatic or evaporative cooling systems (if used), or for other non-potable uses such as grey water flushing, to reduce reliance on mains water supply and improve water usage efficiency?",
        "simplifiedQuestion": "Do you capture rainwater from your DC roof/surfaces for use in cooling systems or other non-potable uses to save mains water?",
        "reference": "2025 CoC Guide 8.3.1",
        "priority": "Low (Value: 1, Expected: Optional Practice)",
        "stepSection": "step_15",
        "recommendationTitle": "Capture rainwater",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx"
          ],
          "description": "Feasibility studies for rainwater harvesting. Design documents if implemented, showing collection areas, storage, filtration, and usage (e.g., for adiabatic cooling, grey water). Water balance calculations showing contribution of rainwater."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, dwg, xlsx",
        "documentDescriptor": "Optional: If implemented or assessed: Design and specification of rainwater harvesting system. Data on collected volume and usage (e.g., for adiabatic cooling make-up, toilet flushing). Calculations of mains water savings.",
        "verification_method": [
          "Document Review",
          "Site Inspection (if implemented)"
        ],
        "verificationMethods": "Review of feasibility studies or design documents for rainwater harvesting systems. Physical inspection of collection surfaces, storage tanks, and points of use if implemented.",
        "nonconformity": {
          "major": null,
          "minor": "Opportunities for rainwater harvesting are not evaluated, or if a system is implemented, it is poorly maintained or underutilized, providing minimal benefit to mains water reduction (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to adequately evaluate or implement rainwater capture systems where feasible and potentially beneficial for reducing mains water consumption for cooling or other non-potable uses.",
        "whatThisMeans": "Your organization should think about collecting rainwater from the roof of your data centre or other large surfaces. This collected water can then be used for things like adiabatic or evaporative cooling systems (if you use them), or for other uses where drinking-quality water isn't needed, like flushing toilets (grey water). This helps reduce how much water you take from the main supply.",
        "complianceRequires": "- Consider or implement systems for capturing rainwater from the data centre roof or other suitable collection surfaces.\n- Primarily intend the use of this captured rainwater for adiabatic or evaporative cooling systems (if such cooling technologies are employed at the facility).\n- Alternatively, or additionally, use captured rainwater for other non-potable purposes such as grey water flushing systems (e.g., for toilets).\n- The overall aim is to reduce reliance on mains water supply and improve the data centre's overall water usage efficiency.",
        "expertTip": "Ensure any rainwater harvesting system includes adequate filtration and treatment to prevent contamination and protect equipment (e.g., nozzles in adiabatic systems, build-up in storage tanks). Calculate the potential rainwater yield based on local rainfall patterns and roof area to correctly size storage tanks and assess the contribution to water demand.",
        "recommendedEvidence": [
          "- Feasibility study or design documentation for a rainwater harvesting system.\n- Specifications for rainwater collection surfaces, filtration, storage tanks, and distribution pipework.\n- If used for cooling: details of integration with adiabatic/evaporative cooling systems.\n- If used for grey water: details of integration with relevant plumbing systems.\n- Records of collected rainwater volume and its usage, demonstrating reduction in mains water consumption."
        ],
        "recommendedActions": [
          "- Assess the feasibility of rainwater harvesting based on local rainfall, available collection area (roof size), and potential water demands (e.g., adiabatic cooling make-up, toilet flushing).\n- If viable, design and install a rainwater harvesting system including appropriate filtration, storage, and distribution.\n- Implement a maintenance plan for the rainwater harvesting system to ensure water quality and system reliability.\n- Monitor and record rainwater collection and usage to quantify mains water savings."
        ],
        "sampleResponseTemplate": "We have implemented a rainwater harvesting system at our [Facility Name] (commissioned [Date]). Rainwater from [X m\u00b2 of roof area] is collected, filtered, and stored in a [Y m\u00b3 tank]. This water is primarily used for [e.g., 'make-up water for our adiabatic cooling system / toilet flushing in the admin block'], reducing our mains water consumption by an estimated [Z m\u00b3/year]. The system design is documented in '[RWH Design Doc ID]'.",
        "humanInputAnswer": "Our 'EcoDataCentre One' facility incorporates a rainwater harvesting system. Runoff from the 5,000 m\u00b2 roof is filtered and stored in a 100 m\u00b3 underground tank. This water is primarily used as make-up for our adiabatic cooling units on the dry coolers, reducing our annual mains water intake for cooling by approximately 60%. The system design and water balance calculations are detailed in 'EDC1_WaterManagementPlan_Rev2.pdf'.",
        "llmGeneratedAnswer": "Yes, our organization has implemented rainwater capture at our 'Greenfield Park' facility. As detailed in the 'GP_Sustainability_Features_Report.pdf', rainwater collected from the main data hall roof (approximately 8,000 m\u00b2) is channeled into a 250 m\u00b3 storage tank after initial filtration. This harvested water is then primarily utilized as make-up water for our adiabatic cooling system on the external heat rejection units, significantly reducing our reliance on municipal water sources for this purpose. Any surplus is directed towards site irrigation. This system is projected to offset 30% of our non-potable water demand.",
        "badExampleAnswer": "We just use mains water for everything. Rainwater harvesting seems like a lot of effort for the amount of water we might collect."
      },
      "question_137": {
        "id": 137,
        "old_question_text": null,
        "questionText": "Has the organization considered other alternative water sources where they are available, practical to utilize, and offer genuine benefits? Examples include treated surface water (e.g., from rivers, canals), boreholes, grey water recycling from within the facility (e.g., from sinks, showers if present), or recycled water from other industrial processes (subject to quality and treatment). Is any use of such sources subject to local environmental regulations and water abstraction licensing?",
        "simplifiedQuestion": "Have you considered other alternative water sources (treated surface water, boreholes, grey water recycling) if practical and beneficial, following environmental rules?",
        "reference": "2025 CoC Guide 8.3.2",
        "priority": "Low (Value: 1, Expected: Optional Practice)",
        "stepSection": "step_15",
        "recommendationTitle": "Other water sources",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx"
          ],
          "description": "Feasibility studies for alternative water sources (treated surface water, boreholes, grey water recycling). Design documents if implemented. Water quality analysis and treatment plans. Environmental permits or abstraction licenses if required."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx",
        "documentDescriptor": "Optional: If implemented or assessed: Feasibility studies, design documents, water quality assessments, and any permits for using alternative water sources (e.g., treated surface water, borehole water, recycled grey water).",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of feasibility studies and design documents for alternative water source utilization. Examination of water abstraction licenses or environmental permits if applicable. Interviews with facilities management.",
        "nonconformity": {
          "major": null,
          "minor": "Potentially viable alternative water sources are not evaluated, or if used, water quality, treatment, or regulatory compliance is not adequately managed (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Failure to adequately evaluate or utilize available and practical alternative water sources (e.g., treated surface water, borehole, grey water recycling) where doing so could offer water conservation benefits and is environmentally permissible.",
        "whatThisMeans": "Besides rainwater, your organization should explore if other alternative water sources could be used for your data centre, provided they are available, practical, and genuinely beneficial. Examples include properly treated water from rivers or canals, water from boreholes (wells), or recycling 'grey water' from within your facility (like from sinks or showers, if you have them). Any use of such sources must follow local environmental rules and water usage licenses.",
        "complianceRequires": "- Consider other alternative water sources where they are available, practical to utilize, and offer genuine benefits (e.g., water conservation, cost reduction).\n- Examples include:\n    - Treated surface water (e.g., from rivers, canals \u2013 requiring appropriate filtration and treatment).\n    - Boreholes (groundwater \u2013 requiring assessment of sustainable yield and quality).\n    - Grey water recycling from within the facility (e.g., from sinks, showers, hand basins \u2013 requiring treatment before reuse for non-potable applications like toilet flushing or irrigation).\n    - Recycled water from other nearby industrial processes (subject to suitable quality and necessary treatment).\n- Ensure that any use of such alternative water sources is subject to, and compliant with, local environmental regulations and any required water abstraction licensing or permits.",
        "expertTip": "Before investing in alternative water sources, conduct a thorough water balance study for your facility to understand current consumption patterns and identify the largest non-potable water demands (e.g., cooling tower make-up, adiabatic cooling, irrigation, toilet flushing). This will help prioritize which alternative sources and uses offer the best return on investment and water savings.",
        "recommendedEvidence": [
          "- Feasibility studies or reports evaluating the potential for using other alternative water sources.\n- If implemented: design documents for the water abstraction, treatment, storage, and distribution systems for the alternative water source.\n- Water quality testing results for the alternative source and any treated water.\n- Copies of environmental permits, abstraction licenses, or agreements related to the use of the alternative water source.\n- Records of water volumes sourced from alternative supplies and the corresponding reduction in mains water usage."
        ],
        "recommendedActions": [
          "- Identify potential alternative water sources available at or near the data centre location.\n- Conduct feasibility assessments including water quality analysis, required treatment processes, sustainable abstraction rates, regulatory requirements, and cost-benefit analysis.\n- If a viable alternative water source is identified, design and implement appropriate abstraction, treatment, storage, and distribution systems.\n- Ensure ongoing monitoring of water quality and compliance with all relevant environmental regulations and permits."
        ],
        "sampleResponseTemplate": "We have evaluated the use of [e.g., borehole water / recycled grey water from our admin building] for [Application, e.g., cooling tower make-up / irrigation] (Ref: '[Alternative Water Study ID]'). The study concluded [Outcome]. [If implemented: We currently source X m\u00b3/year from this source, meeting Y% of the demand for Z, under permit #PermitID].",
        "humanInputAnswer": "For our 'EcoCampus DC', we evaluated using treated effluent from an adjacent municipal wastewater treatment plant for cooling tower make-up (Study Ref: 'ECDC_RecycledWater_FS_2023'). While technically feasible with additional on-site polishing treatment, the current cost and complexity led us to defer this option. We do, however, collect condensate from our CRAH units, which is filtered and reused for humidification, saving approximately 5m\u00b3 of mains water per month. This is documented in 'Condensate_Recycling_SOP.pdf'.",
        "llmGeneratedAnswer": "Yes, our organization has explored alternative water sources. A feasibility study ('AltWaterSource_Study_SiteX_2024.pdf') was conducted for our 'Site X' facility to assess the viability of using borehole groundwater for cooling tower make-up. The study included hydrogeological surveys and water quality testing. While a suitable aquifer was identified, the required abstraction license and ongoing treatment costs currently make it less economically viable than mains water supplemented by our rainwater harvesting system (Practice 8.3.1). We continue to monitor local regulations and technology advancements in this area. We do not currently recycle grey water due to the limited grey water production at our typically unmanned facilities.",
        "badExampleAnswer": "Using water from rivers or the ground seems too risky for our equipment. We only use clean mains water."
      },
      "question_138": {
        "id": 138,
        "old_question_text": null,
        "questionText": "Has the organization implemented water consumption metering and monitoring within the data centre where water is used for cooling (e.g., cooling towers, adiabatic/evaporative systems) or other significant purposes? Is this data regularly reviewed to track usage, identify leaks or inefficiencies, and inform water conservation efforts? Is this practice understood to be a prerequisite for calculating Water Usage Effectiveness (WUE) as defined in ISO/IEC 30134-9 and EN 50600-4-9, and is it noted that WUE reporting is required for EU Commission EED (Energy Efficiency Directive) reporting?",
        "simplifiedQuestion": "Do you meter and monitor your data centre's water use (especially for cooling) to track usage, find leaks/inefficiencies, and inform conservation, understanding its role in WUE and EED reporting?",
        "reference": "2025 CoC Guide 8.3.3",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre - where water used for cooling. 'Note: Required for EU Commission EED reporting' makes metering/monitoring important if EED applies)",
        "stepSection": "step_15",
        "recommendationTitle": "Metering of water consumption",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv",
            "screenshots"
          ],
          "description": "Water metering plan showing locations of water meters. Specifications of water meters. Water consumption logs or BMS/DCIM reports. Procedures for reviewing water usage data and identifying anomalies. WUE calculation methodology and reports if performed. EED reports showing water data."
        },
        "docRequired": "Yes, if water used for cooling/EED applies",
        "acceptedFiles": "pdf, docx, xlsx, csv, screenshots",
        "documentDescriptor": "Mandatory (if water used for cooling/other significant DC purposes, or EED reporting applicable): Documentation of water metering and monitoring for water consumed by cooling systems (e.g., cooling towers, adiabatic units) and other major uses. Records of regular data review and any actions taken. If WUE is calculated (per ISO/IEC 30134-9, EN 50600-4-9), provide methodology and results.",
        "verification_method": [
          "Document Review",
          "Data Analysis",
          "Site Inspection"
        ],
        "verificationMethods": "Review of water metering plans, water consumption records, and WUE calculations if performed. Inspection of water meter installations. Interviews with facilities staff regarding water monitoring and conservation practices.",
        "nonconformity": {
          "major": "Significant water consumption for cooling or other data centre purposes occurs without any metering or monitoring, preventing tracking of usage, identification of leaks/inefficiencies, or calculation of WUE (especially if EED reporting is required).",
          "minor": "Water metering is in place but data is not regularly reviewed, or leaks/inefficiencies are not acted upon, or WUE calculation/reporting is not performed despite significant water use for cooling."
        },
        "majorNonconformity": "Absence of water consumption metering and monitoring for significant water uses within the data centre (e.g., cooling towers, adiabatic systems), particularly where such data is required for EED reporting or calculating Water Usage Effectiveness (WUE).",
        "minorNonconformity": "Water meters are installed, but data is not regularly collected, reviewed, or used to identify leaks, inefficiencies, or inform water conservation efforts. WUE calculation, if applicable, is not performed or is inaccurate.",
        "whatThisMeans": "If your data centre uses a significant amount of water, especially for cooling (like in cooling towers or adiabatic/evaporative systems \u2013 see Practice 8.3.3 for metering), your organization needs to report its Water Usage Effectiveness (WUE). This metric is defined in standards like ISO/IEC 30134-9 or EN 50600-4-9 and shows how efficiently your data centre uses water in relation to the energy consumed by its IT equipment. Reporting WUE is also a requirement for the EU Energy Efficiency Directive (EED).",
        "complianceRequires": "- Implement water consumption metering and monitoring within the data centre where water is significantly used (e.g., for cooling towers, adiabatic/evaporative cooling systems, humidification, or other major non-potable uses).\n- Regularly review this collected water usage data to:\n    - Track consumption patterns and trends.\n    - Identify potential leaks or system inefficiencies.\n    - Inform and evaluate water conservation efforts.\n- Understand that water metering and monitoring is a prerequisite for calculating Water Usage Effectiveness (WUE) as defined in ISO/IEC 30134-9 and EN 50600-4-9.\n- Be aware that WUE reporting is noted as being required for EU Commission EED (Energy Efficiency Directive) reporting.",
        "expertTip": "Install sub-meters on major water consuming systems (e.g., cooling tower make-up, adiabatic system supply, humidifiers) in addition to the main facility water meter. This granular data helps pinpoint specific areas of high consumption or identify leaks more effectively. Integrate water meter readings into your BMS or DCIM for automated data collection, trending, and alarming.",
        "recommendedEvidence": [
          "- A water map or plan of the data centre showing the location of all significant water uses and installed water meters.\n- Specifications for the installed water meters (e.g., accuracy, type).\n- Logs or reports from BMS/DCIM or manual readings showing regular water consumption data.\n- Documented procedures for reviewing water usage data, identifying anomalies, and taking corrective actions.\n- If WUE is calculated, provide the methodology, input data, and resulting WUE values.\n- Evidence of water data reporting for EED compliance, if applicable."
        ],
        "recommendedActions": [
          "- Install water meters on all significant water consumption points within the data centre, particularly for cooling systems.\n- Implement a system for regular collection and review of water meter data.\n- Establish baseline water consumption patterns and set targets for water conservation.\n- Investigate any unexplained increases in water usage promptly to identify and rectify leaks or inefficiencies.\n- If applicable, calculate and track Water Usage Effectiveness (WUE) as per ISO/IEC 30134-9 / EN 50600-4-9 and use it for internal benchmarking and external reporting (e.g., EED)."
        ],
        "sampleResponseTemplate": "We have installed water meters on [e.g., 'the make-up lines for our cooling towers', 'our adiabatic cooling system supply', 'main facility inlet']. Data is collected [daily/weekly] via [BMS/manual readings] and reviewed [monthly] by the facilities team to identify trends and anomalies (Ref: '[Water Monitoring Procedure ID]'). This data is used for our WUE calculation (current WUE is [Value]) which is reported for EED compliance. Last quarter, this monitoring helped us identify and fix a [e.g., 'stuck float valve in cooling tower X'].",
        "humanInputAnswer": "Our 'Site Beta' facility has water sub-meters on cooling tower make-up, adiabatic cooler supply, and landscape irrigation. Data is logged hourly in our BMS and reviewed weekly by the facilities team (Procedure: 'WaterUse_Monitoring_SOP_SB.pdf'). This data is essential for our monthly WUE calculation (ISO/IEC 30134-9), which averaged 1.8 L/kWh for IT equipment energy in 2024. This WUE figure is part of our annual EED reporting.",
        "llmGeneratedAnswer": "Yes, our organization has implemented comprehensive water consumption metering and monitoring. All significant water usage points within our data centres, particularly make-up water for cooling towers and adiabatic cooling systems, are equipped with calibrated water meters that are integrated into our Building Management System (BMS). As per our 'Water Management Protocol (WMP-2025)', this data is collected continuously and reviewed weekly by the facilities team to track usage, identify anomalies indicative of leaks or inefficiencies, and inform our water conservation initiatives. This metered data is a critical input for calculating our Water Usage Effectiveness (WUE) according to ISO/IEC 30134-9 and EN 50600-4-9, which is reported annually for EU Commission EED compliance.",
        "badExampleAnswer": "We get a water bill for the whole site. We don't have separate meters just for the data centre cooling."
      }
    }
  },
  "step_16": {
    "description": "2025 Guide Section 9: Monitoring (Practices 9.1.1 through 9.1.9 - Energy Use and Environmental Performance Measurement)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Monitoring_1"
    ],
    "critical_questions": [],
    "questions": {
      "question_139": {
        "id": 139,
        "old_question_text": null,
        "questionText": "Has the organization installed a calibrated energy consumption meter on the incoming mains electrical supply to the data centre (or to the data centre's dedicated area within a mixed-use building)? Is this meter capable of accurately measuring total energy consumed by the entire data centre facility, including IT load, cooling, power distribution losses, lighting, and other ancillary loads, to provide a baseline for overall energy efficiency metrics like PUE (Power Usage Effectiveness)?",
        "simplifiedQuestion": "Do you have an accurate energy meter on the main power supply for your entire data centre to measure total energy use?",
        "reference": "2025 CoC Guide 9.1.1",
        "priority": "High (Value: 5, Expected: Entire Data Centre)",
        "stepSection": "step_16",
        "recommendationTitle": "Incoming energy consumption meter",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "xlsx"
          ],
          "description": "Electrical single-line diagrams showing location of main incoming energy meter. Meter specifications and calibration certificates. Sample energy consumption reports or data from this meter."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, jpg, xlsx",
        "documentDescriptor": "Mandatory: Electrical diagrams showing the main incoming energy meter for the DC. Meter specifications (accuracy class) and recent calibration certificate. Sample energy readings (kWh).",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "Data Verification"
        ],
        "verificationMethods": "Review of electrical schematics, meter specifications, and calibration records. Physical inspection of the main incoming meter. Verification of data collection from this meter.",
        "nonconformity": {
          "major": "No dedicated, calibrated energy meter is installed on the incoming mains supply for the entire data centre, preventing accurate measurement of total facility energy consumption.",
          "minor": "Main incoming meter exists but is not of appropriate accuracy, calibration is out of date, or data is not reliably collected or used for overall efficiency calculations."
        },
        "majorNonconformity": "Absence of a calibrated energy consumption meter on the incoming mains electrical supply dedicated to the data centre, making accurate measurement of total facility energy use (PUE_TOTAL) impossible.",
        "minorNonconformity": "A main incoming meter is present but lacks proper calibration, is of insufficient accuracy class for its rating, or its readings are not consistently used as the basis for total data centre energy consumption figures.",
        "whatThisMeans": "Your organization must have an accurate electricity meter installed on the main power feed coming into your data centre (or the part of a building your data centre uses). This meter needs to measure all the electricity used by everything in the facility \u2013 IT equipment, cooling systems, power system losses, lights, etc. This total energy measurement is crucial for calculating overall energy efficiency metrics like PUE.",
        "complianceRequires": "- Install a calibrated energy consumption meter on the incoming mains electrical supply to the data centre (or the data centre's dedicated area if in a mixed-use building).\n- Ensure this meter accurately measures the total energy consumed by the entire data centre facility.\n- This total energy includes IT load, cooling system energy, power distribution losses, lighting, and any other ancillary loads within the defined data centre boundary.\n- This measurement provides the essential baseline for overall energy efficiency metrics like Power Usage Effectiveness (PUE).",
        "expertTip": "Select a utility-grade or high-accuracy (e.g., Class 0.5S or better for IEC standards) smart meter that can log interval data (e.g., every 15 minutes) and communicate readings to your BMS or energy management system. Ensure regular calibration as per manufacturer or regulatory requirements.",
        "recommendedEvidence": [
          "- Electrical single-line diagram clearly showing the location and designation of the main incoming data centre energy meter(s).\n- Technical specifications for the installed meter(s), including accuracy class.\n- Recent calibration certificate(s) for the meter(s).\n- Examples of energy consumption data (e.g., kWh readings over time) collected from this meter and used in PUE calculations."
        ],
        "recommendedActions": [
          "- Install a dedicated, calibrated, high-accuracy energy meter at the main incoming power feed(s) for the data centre.\n- Ensure the meter captures the total energy consumption of the entire facility as defined for PUE calculations.\n- Integrate the meter readings into a monitoring system for regular data collection and analysis.\n- Establish a schedule for periodic recalibration of the meter."
        ],
        "sampleResponseTemplate": "Our data centre has a [Meter Make/Model] (Accuracy Class [e.g., 0.5S]) installed on the main incoming utility supply at [Location], measuring total facility energy consumption (kWh). It was last calibrated on [Date] (Cert No: [XYZ]). Data is logged [e.g., every 15 minutes] to our [BMS/EMS] and is the basis for our PUE_TOTAL calculation. Electrical drawing [Drawing No.] shows its location.",
        "humanInputAnswer": "A dedicated Schneider ION9000 (Class 0.1S) utility-grade meter is installed on the main 10kV incomer feeding our data centre. It measures total facility energy (TFE) in kWh. The meter was calibrated by an accredited third party on 15/01/2025 (Calibration Cert: #METCAL-2025-003). Data is polled every 5 minutes by our SCADA system. This TFE value is used as the denominator for our PUE calculations.",
        "llmGeneratedAnswer": "Yes, a calibrated energy consumption meter (Model: [Specific Meter Model], Accuracy Class: [e.g., 0.2S]) is installed on the primary incoming mains electrical supply dedicated to our data centre facility. This meter (Meter ID: [Specific ID], as shown on Electrical Single Line Diagram [Diagram Ref No.]) accurately measures the total energy consumed (kWh) by the entire data centre, encompassing all IT load, cooling systems, power infrastructure losses, lighting, and ancillary services. The meter underwent its last certified calibration on [Date], (Certificate: [Cert_ID]). The data from this meter serves as the PUE_TOTAL for our energy efficiency reporting.",
        "badExampleAnswer": "We get an electricity bill for the whole building, and we estimate what percentage the data centre uses."
      },
      "question_140": {
        "id": 140,
        "old_question_text": null,
        "questionText": "Has the organization installed calibrated energy consumption meters to measure the power supplied to the IT equipment load? Is this measurement taken at the output of the Power Distribution Units (PDUs) that directly feed the IT equipment cabinets/racks, or alternatively, at the output of the Uninterruptible Power Supply (UPS) systems if PDU output metering is not feasible (understanding that this would include downstream PDU losses but exclude UPS losses which would be part of PUE overhead)? Is this data used to calculate key energy efficiency metrics like Power Usage Effectiveness (PUE) or Data Centre Infrastructure Efficiency (DCiE)?",
        "simplifiedQuestion": "Do you have accurate energy meters to measure the power going just to your IT equipment (e.g., at rack PDUs or UPS output) for calculating PUE/DCiE?",
        "reference": "2025 CoC Guide 9.1.2",
        "priority": "High (Value: 5, Expected: Entire Data Centre)",
        "stepSection": "step_16",
        "recommendationTitle": "IT Energy consumption meter",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "xlsx",
            "csv"
          ],
          "description": "Electrical single-line diagrams showing location of IT load energy meters (PDU outputs or UPS outputs). Meter specifications and calibration certificates. Sample energy consumption reports or data from these meters used for PUE/DCiE calculation."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, jpg, xlsx, csv",
        "documentDescriptor": "Mandatory: Electrical diagrams showing IT load energy metering points (PDU outputs preferred, or UPS outputs). Meter specifications and calibration certificates. Evidence of this data being used for PUE/DCiE calculation.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "Data Verification"
        ],
        "verificationMethods": "Review of electrical schematics for IT load metering points, meter specifications, and calibration records. Inspection of PDU/UPS metering. Verification of data collection and its use in PUE/DCiE calculations.",
        "nonconformity": {
          "major": "No dedicated, calibrated energy metering is in place to measure the IT equipment load, preventing accurate calculation of PUE/DCiE.",
          "minor": "IT load metering exists but is not at the recommended points (PDU outputs), is not accurately calibrated, or data is not reliably used for PUE/DCiE calculations."
        },
        "majorNonconformity": "Absence of calibrated energy meters to accurately measure the total power supplied to the IT equipment load (either at PDU outputs or UPS outputs), making accurate PUE/DCiE calculation impossible.",
        "minorNonconformity": "IT load metering is in place but not at the preferred PDU output level (e.g., only at UPS output including PDU losses), or meters lack proper calibration, or the collected data is not consistently used for PUE/DCiE reporting.",
        "whatThisMeans": "Your organization must install accurate electricity meters to measure the power that goes only to the IT equipment (servers, storage, network gear). The best place to measure this is at the output of the Power Distribution Units (PDUs) that feed the server racks. If that's not possible, measuring at the output of the UPS systems is an alternative, though this will include energy losses from the PDUs downstream. This IT load measurement is critical for calculating efficiency metrics like PUE or DCiE.",
        "complianceRequires": "- Install calibrated energy consumption meters to measure the power supplied to the IT equipment load.\n- Preferably, measure IT load at the output of the Power Distribution Units (PDUs) that directly feed the IT equipment cabinets/racks.\n- Alternatively, if PDU output metering is not feasible, measure IT load at the output of the Uninterruptible Power Supply (UPS) systems. (Note: this method will include downstream PDU losses in the IT load measurement, whereas measuring at PDU output excludes PDU losses from IT load, attributing them to infrastructure overhead for PUE calculation).\n- Ensure this data is used for the calculation of key energy efficiency metrics such as PUE (Power Usage Effectiveness) or DCiE (Data Centre Infrastructure Efficiency).",
        "expertTip": "For the most accurate IT load measurement (PUE Category 1 or 2), deploy intelligent Rack PDUs (rPDUs) with per-outlet or PDU-level energy metering (kWh) capabilities. Ensure these rPDUs are regularly calibrated or verified. Aggregate the data from all rPDUs serving IT equipment to get the total IT load.",
        "recommendedEvidence": [
          "- Electrical single-line diagrams clearly showing the location of IT load energy meters (e.g., at PDU outputs or UPS outputs).\n- Technical specifications for the installed IT load meters, including accuracy class.\n- Recent calibration certificates for these meters.\n- Examples of energy consumption data (e.g., kWh readings) collected from these meters and their use in PUE/DCiE calculation spreadsheets or reports."
        ],
        "recommendedActions": [
          "- Install calibrated energy meters at the appropriate points to measure total IT equipment energy consumption (PDU outputs preferred for PUE Cat 1/2).\n- Ensure the accuracy and calibration of these meters are maintained.\n- Integrate the IT load meter readings into a monitoring system for regular data collection.\n- Use this data consistently for calculating and reporting PUE/DCiE."
        ],
        "sampleResponseTemplate": "We measure IT equipment energy consumption at [the output of our rack PDUs / the output of our UPS systems] using [Meter Make/Model] (Accuracy Class [e.g., 1.0]). These meters were last calibrated on [Date] (Cert No: [ABC]). This data is collected by our [DCIM/EMS] and used as the numerator (IT Equipment Energy) for our PUE calculations, as shown in our '[PUE Calculation Report ID]'.",
        "humanInputAnswer": "Our IT load is measured using calibrated branch circuit monitoring within our Schneider APC Rack PDUs (per-outlet metering, accuracy \u00b11%). Data from all IT rack PDUs is aggregated by our StruxureWare DCIM to provide the total IT equipment energy consumption (kWh). This is the 'Total IT Equipment Energy' figure used in our PUE Category 2 calculations. PDU meter calibrations are verified annually as part of our PM schedule.",
        "llmGeneratedAnswer": "Yes, calibrated energy consumption meters are installed to measure the power supplied to the IT equipment load. Our primary measurement point is at the output of the Power Distribution Units (PDUs) directly feeding the IT cabinets, utilizing intelligent PDUs with \u00b11% metering accuracy. This data is aggregated by our Building Management System (BMS) to determine the total IT load for PUE Category 2 calculations. For legacy areas where PDU-level metering is not yet deployed, IT load is measured at the output of the respective UPS systems, with downstream PDU losses acknowledged as part of the IT load in these specific PUE calculations. All metering devices undergo annual calibration checks (Ref: 'Metering Calibration Schedule MCS-2025').",
        "badExampleAnswer": "We estimate IT load based on the number of servers we have and their average power draw from datasheets."
      },
      "question_141": {
        "id": 141,
        "old_question_text": null,
        "questionText": "Has the organization installed calibrated temperature and relative humidity sensors at a room level within data centre technical spaces to monitor the general environment? Is this done as a basic minimum level of environmental monitoring, with the understanding that more granular (e.g., cabinet/row level) monitoring provides better control and efficiency opportunities?",
        "simplifiedQuestion": "Do you have calibrated temperature and humidity sensors to monitor the general room environment in your data centre technical spaces as a basic minimum?",
        "reference": "2025 CoC Guide 9.1.3",
        "priority": "Low (Value: 2, Expected: Entire Data Centre)",
        "stepSection": "step_16",
        "recommendationTitle": "Room level metering of supply air temperature and humidity",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "xlsx",
            "screenshots"
          ],
          "description": "Data centre layout plans showing locations of room-level temperature and humidity sensors. Sensor specifications and calibration certificates. BMS/monitoring system screenshots showing data from these room-level sensors."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, jpg, xlsx, screenshots",
        "documentDescriptor": "Mandatory: Evidence of calibrated room-level temperature and humidity sensors in data centre technical spaces (both supply and return air as a minimum). Documentation of sensor locations and calibration records.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Review"
        ],
        "verificationMethods": "Review of sensor layout plans, specifications, and calibration records. Physical inspection of sensor locations. Verification of data collection in BMS or monitoring system.",
        "nonconformity": {
          "major": "No room-level temperature and humidity sensors are installed in data centre technical spaces, or sensors are uncalibrated and provide unreliable data.",
          "minor": "Room-level sensors are present but coverage is inadequate (e.g., only one sensor for a large room, not capturing supply/return effectively), or calibration is infrequent, or data is not actively monitored."
        },
        "majorNonconformity": "Absence of calibrated temperature and relative humidity sensors to monitor the general room-level environment (both supply and return air as a minimum) within data centre technical spaces.",
        "minorNonconformity": "Room-level environmental sensors are installed but are not regularly calibrated, their placement does not adequately represent overall conditions (e.g., insufficient number or poor locations), or the data is not actively used for operational awareness or control.",
        "whatThisMeans": "As a basic starting point for environmental monitoring, your organization must have temperature and humidity sensors in your data centre technical areas that measure the general room conditions. The guide specifies at least measuring supply and return air temperature and humidity at a room level. While more detailed monitoring (like at each rack) is better for fine-tuning, room-level sensing provides essential overall environmental awareness.",
        "complianceRequires": "- Install calibrated temperature and relative humidity sensors at a room level within data centre technical spaces.\n- This is considered a basic minimum level of environmental monitoring.\n- At minimum, these sensors should monitor both return air temperature and humidity, and supply air temperature and humidity for the room or zone.\n- Understand that more granular monitoring (e.g., at cabinet or row level, as per Practice 9.1.7) provides better opportunities for control and efficiency optimization.",
        "expertTip": "Strategically place room-level sensors to capture representative conditions, avoiding direct airflow from CRAC/CRAH units or hotspots near specific equipment. Ensure these sensors are integrated into your BMS/DCIM for continuous logging, trending, and alarming on out-of-range conditions. Regular calibration (e.g., annually) is crucial for data accuracy.",
        "recommendedEvidence": [
          "- Data centre layout diagrams showing the locations of room-level temperature and humidity sensors (clearly indicating if they monitor supply air, return air, or general ambient conditions).\n- Specifications for the installed sensors, including accuracy and calibration requirements.\n- Recent calibration certificates for these room-level sensors.\n- Examples of data (e.g., BMS screenshots, trend logs) collected from these sensors."
        ],
        "recommendedActions": [
          "- Install calibrated temperature and humidity sensors to monitor general room-level environmental conditions in all data centre technical spaces, ensuring both supply and return air paths are monitored as a minimum.\n- Ensure sensor placement provides a representative overview of the environment in each zone.\n- Integrate sensor readings into a monitoring system for data logging, trending, and alerting.\n- Establish a schedule for periodic recalibration of these sensors."
        ],
        "sampleResponseTemplate": "Each of our data halls is equipped with room-level temperature and humidity sensors monitoring [e.g., 'overall supply and return air conditions for the zone'], as shown on drawing [Layout Plan ID]. These sensors ([Make/Model]) are calibrated [Annually] (last calibration [Date], Certs: [File Ref]). Data is logged in our BMS and provides a baseline for environmental control. We understand more granular sensing offers further benefits.",
        "humanInputAnswer": "Our data halls are equipped with calibrated room-level temperature and humidity sensors located at the main return air paths to the CRAH units and in representative cold aisle supply points. These are specified in 'DC_Sensor_Layout_Plan_v2.pdf'. Sensors are 'VendorX EnviroSense' model, calibrated annually (last: Jan 2025). Data is logged in our BMS, providing overall zone conditions. This is our minimum baseline before more granular rack-level sensing.",
        "llmGeneratedAnswer": "Yes, all data centre technical spaces within our facilities are equipped with calibrated room-level temperature and relative humidity sensors as a foundational layer of environmental monitoring. These sensors (Model: [SpecificSensorModel], calibrated: [Date]) are strategically placed to monitor both the general supply air conditions delivered to the IT equipment zones and the overall return air temperature/humidity to the cooling units. This data, logged continuously by our Building Management System (BMS), provides essential oversight of the general environment, complementing our more granular row/cabinet level monitoring where deployed. This aligns with Practice 9.1.3, understanding it as a baseline.",
        "badExampleAnswer": "There's a thermostat on the wall in the server room that shows the temperature. It seems to work fine."
      },
      "question_142": {
        "id": 142,
        "old_question_text": null,
        "questionText": "Has the organization installed calibrated temperature sensors at the Computer Room Air Conditioner (CRAC) or Computer Room Air Handler (CRAH) unit level to measure either the supply air temperature or the return air temperature (or both)? Is this data used to monitor and control the performance of individual cooling units and to ensure they are operating according to the defined cooling strategy and setpoints?",
        "simplifiedQuestion": "Do you have calibrated temperature sensors on your in-room AC units (CRACs/CRAHs) measuring their supply and/or return air to monitor and control their performance?",
        "reference": "2025 CoC Guide 9.1.4",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_16",
        "recommendationTitle": "CRAC / CRAH unit level metering of supply or return air temperature",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "screenshots",
            "xlsx"
          ],
          "description": "CRAC/CRAH specifications showing built-in or added temperature sensors. BMS configuration showing these sensor inputs. Sensor calibration records. Examples of how this data is used for unit control or performance monitoring."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, screenshots, xlsx",
        "documentDescriptor": "Mandatory: Documentation or evidence (e.g., BMS screenshots, sensor lists) of calibrated temperature sensors measuring supply and/or return air at individual CRAC/CRAH units. Records of sensor calibration. Examples of data use.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "BMS Data Review"
        ],
        "verificationMethods": "Review of CRAC/CRAH specifications and BMS configuration for temperature sensor inputs. Inspection of sensor placement on units. Review of BMS trend data for supply/return temperatures.",
        "nonconformity": {
          "major": "No temperature sensors are installed at the CRAC/CRAH unit level to monitor supply or return air, or sensors are present but uncalibrated and unused for control or performance monitoring.",
          "minor": "Sensors are present but only measure one parameter (supply or return, not both where beneficial), calibration is infrequent, or data is not effectively used to optimize individual unit performance."
        },
        "majorNonconformity": "Absence of calibrated temperature sensors at the CRAC/CRAH unit level measuring either supply or return air temperature (or both), preventing effective monitoring and control of individual cooling unit performance.",
        "minorNonconformity": "Temperature sensors are present at CRAC/CRAH units but are not regularly calibrated, or the data collected is not actively used for performance monitoring, diagnostics, or optimizing control of individual units.",
        "whatThisMeans": "Your organization needs to have accurate temperature sensors installed on each of your in-room air conditioning units (CRACs or CRAHs). These sensors should measure at least the temperature of the air the unit is supplying to the room or the temperature of the air it's taking back in (return air), or ideally both. This information is vital for monitoring how well each cooling unit is working and for controlling it effectively according to your overall cooling plan.",
        "complianceRequires": "- Install calibrated temperature sensors at the CRAC/CRAH unit level.\n- These sensors should measure either the supply air temperature from each unit, or the return air temperature to each unit, or preferably both.\n- Use this data to monitor and control the performance of individual cooling units.\n- Ensure that unit operation, as informed by these sensors, aligns with the defined cooling strategy and setpoints (e.g., Practice 5.5.2 for supply air control).",
        "expertTip": "Monitoring both supply and return air temperature at each CRAC/CRAH allows for calculation of the Delta-T across each unit, which is a key indicator of its individual cooling performance and airflow efficiency. Deviations from expected Delta-T can indicate issues like incorrect fan speed, coil problems, or airflow blockages.",
        "recommendedEvidence": [
          "- List of CRAC/CRAH units with details of installed supply and/or return air temperature sensors.\n- Specifications of the temperature sensors used, including accuracy.\n- Recent calibration certificates for these sensors.\n- BMS screenshots or trend data showing supply/return air temperatures for individual CRAC/CRAH units.\n- Examples of how this data is used in control loops or for performance diagnostics (e.g., identifying underperforming units)."
        ],
        "recommendedActions": [
          "- Install (or ensure existing) calibrated temperature sensors on all CRAC/CRAH units to measure, at a minimum, supply or return air temperature (ideally both).\n- Integrate these sensor readings into the Building Management System (BMS) or a dedicated monitoring system.\n- Use this data for real-time control of individual units (e.g., modulating cooling capacity, fan speed based on supply temperature - see 5.5.2).\n- Regularly review the performance data from these sensors to identify units that may require maintenance or optimization.\n- Maintain a schedule for periodic sensor calibration."
        ],
        "sampleResponseTemplate": "All our CRAC/CRAH units ([Number] units, [Vendor/Model]) are equipped with calibrated temperature sensors for both supply and return air, as specified in our '[Monitoring Standard ID]'. These sensors (last calibrated [Date], Ref: '[Calibration Log ID]') feed data into our BMS, which is used to [e.g., 'control supply air temperature per unit' and 'monitor individual unit Delta-T for performance anomalies'].",
        "humanInputAnswer": "Each of our CRAH units (Liebert DSE model) has factory-installed, calibrated temperature sensors for both supply and return air streams. These sensors are polled every minute by our BMS (Schneider EcoStruxure). Supply air temperature is the primary control variable for each CRAH. Return air temperatures are trended to monitor overall room conditions and unit Delta-T for performance diagnostics. Calibration is verified annually during PM visits (last: March 2025).",
        "llmGeneratedAnswer": "Yes, our organization has installed calibrated temperature sensors at each Computer Room Air Handler (CRAH) unit. As per 'Data Centre Monitoring Standard (DCMS-2025, Section 3.1)', each CRAH is equipped with sensors to measure both supply air temperature and return air temperature. These sensors (Model: [SensorModel XYZ], accuracy: \u00b10.2\u00b0C) undergo annual calibration (Last calibration cycle: Q1 2025, records available). The data is continuously fed into our Building Management System (BMS) and is utilized for individual unit control (maintaining supply air setpoints) and for performance monitoring, allowing us to identify deviations from our defined cooling strategy.",
        "badExampleAnswer": "The CRAC units have their own thermostats built in, so they control themselves. We don't have separate sensors on them."
      },
      "question_143": {
        "id": 143,
        "old_question_text": null,
        "questionText": "Has the organization installed calibrated energy consumption meters at the distribution board level to measure the energy consumed by different parts of the Mechanical and Electrical (M&E) infrastructure, such as chillers, pumps, CRAC/CRAH units, humidifiers, and lighting? Is this sub-metering used to understand the energy consumption patterns of these major M&E components, identify inefficiencies, and support accurate calculation of partial PUE (pPUE) for different subsystems?",
        "simplifiedQuestion": "Do you have energy sub-meters on your main M&E equipment (chillers, pumps, AC units, etc.) to track their individual energy use and identify inefficiencies?",
        "reference": "2025 CoC Guide 9.1.5",
        "priority": "Medium (Value: 3, Expected: New build or retrofit)",
        "stepSection": "step_16",
        "recommendationTitle": "Distribution board level metering of Mechanical and Electrical energy consumption",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx",
            "csv"
          ],
          "description": "Electrical single-line diagrams showing sub-metering for M&E components. Meter specifications and calibration certificates. Examples of energy consumption reports for different M&E subsystems. Use of this data for pPUE or inefficiency analysis."
        },
        "docRequired": "Yes, if new build/retrofit",
        "acceptedFiles": "pdf, docx, dwg, xlsx, csv",
        "documentDescriptor": "Mandatory for new builds/retrofits: Electrical diagrams showing energy sub-metering at distribution boards for major M&E components (chillers, pumps, CRACs/CRAHs, humidifiers, lighting). Meter specifications and calibration. Evidence of data use for analysis or pPUE.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "Data Verification"
        ],
        "verificationMethods": "Review of electrical drawings, meter specifications, and calibration records. Inspection of sub-meter installations. Verification of data collection and its use for M&E energy analysis or pPUE calculations.",
        "nonconformity": {
          "major": "In new builds/retrofits, no sub-metering is installed for major M&E components, preventing detailed understanding of energy consumption patterns and inefficiencies within the facility infrastructure.",
          "minor": "Sub-metering is implemented for some M&E components but not comprehensively, or meters are not calibrated, or data is not effectively used for analysis and optimization."
        },
        "majorNonconformity": "For new builds or retrofits, failure to install calibrated energy consumption meters at the distribution board level for major Mechanical and Electrical (M&E) infrastructure components, hindering the ability to analyze subsystem energy performance.",
        "minorNonconformity": "Sub-metering for M&E components is incomplete (e.g., only chillers metered but not pumps or CRACs), meters lack regular calibration, or the collected data is not actively used to identify inefficiencies or calculate partial PUE metrics.",
        "whatThisMeans": "Your organization should install separate electricity meters on the power feeds to different parts of your data centre's mechanical and electrical (M&E) systems. This means having meters for things like your chillers, pumps, in-room AC units (CRACs/CRAHs), humidifiers, and lighting. This 'sub-metering' helps you understand how much energy each of these major components is using, identify any that are inefficient, and can help you calculate more detailed PUE figures for different parts of your infrastructure (partial PUE).",
        "complianceRequires": "- Install calibrated energy consumption meters at the distribution board level for different parts of the Mechanical and Electrical (M&E) infrastructure.\n- This sub-metering should cover major energy-consuming components such as:\n    - Chillers.\n    - Pumps (chilled water, condenser water, etc.).\n    - CRAC/CRAH units.\n    - Humidifiers (if separate and significant).\n    - Lighting circuits.\n- Use the data from this sub-metering to:\n    - Understand the energy consumption patterns of these M&E components.\n    - Identify potential inefficiencies within these subsystems.\n    - Support accurate calculation of partial PUE (pPUE) for different subsystems (e.g., pPUE_Cooling, pPUE_Power).",
        "expertTip": "Integrate sub-meter readings into your BMS or energy management system for automated data collection, trending, and alarming. Use this granular data to create energy baselines for different M&E subsystems and track the impact of efficiency improvement measures over time. For pPUE, ensure your metering boundaries align with the definitions in relevant standards like The Green Grid or ISO/IEC 30134 series.",
        "recommendedEvidence": [
          "- Electrical single-line diagrams showing the locations of sub-meters for major M&E components (chillers, pumps, CRAC/CRAH groups, humidifiers, lighting distribution boards).\n- Technical specifications for the installed sub-meters, including accuracy class.\n- Recent calibration certificates for these sub-meters.\n- Examples of energy consumption reports broken down by M&E subsystem, derived from this sub-metering data.\n- If pPUE is calculated, documentation of the methodology and results."
        ],
        "recommendedActions": [
          "- For new builds and major retrofits, incorporate sub-metering for all significant M&E loads into the electrical design.\n- Select calibrated meters of appropriate accuracy for each application.\n- Integrate sub-meter data into a central monitoring system.\n- Regularly analyze the energy consumption of different M&E subsystems to identify inefficiencies, optimize performance, and validate savings from improvement projects.\n- Utilize sub-metered data for calculating partial PUE metrics to gain deeper insights into infrastructure efficiency."
        ],
        "sampleResponseTemplate": "In our new [Facility Name], we have installed sub-meters ([Meter Make/Model]) at the distribution boards for [all chiller plants, main CHW pump sets, CRAH unit groups per zone, and lighting panels], as shown in electrical drawing [Drawing No. X]. These meters (last calibrated [Date]) feed data to our [BMS/EMS], enabling us to track energy consumption per M&E subsystem and calculate pPUE for our cooling system. For example, in [Month/Year], our cooling pPUE was [Value].",
        "humanInputAnswer": "Our 'Site Gamma' new build includes calibrated Schneider PM5000 series sub-meters on all main mechanical distribution boards (MDBs) feeding our chiller skids, primary/secondary pump sets, and CRAH unit groups in each data hall. Lighting panels are also sub-metered. This data is logged in our Power Monitoring Expert system and used to calculate pPUE_Cooling and pPUE_Power monthly. Last month, pPUE_Cooling was 1.15. Meter locations are on SLD 'GAMMA-ELEC-003'.",
        "llmGeneratedAnswer": "Yes, for our 'DataHub Central' facility (commissioned Q2 2024), calibrated energy consumption meters (Model: [SpecificMeterType], Accuracy: [e.g., Class 1]) have been installed at the distribution board level for all major Mechanical and Electrical (M&E) infrastructure components. This includes dedicated sub-metering for:\n- Each chiller unit.\n- Primary and secondary chilled water pump groups.\n- Each Computer Room Air Handler (CRAH) unit group serving a specific zone.\n- Central humidification plant (if applicable).\n- Data hall lighting circuits.\nThis granular data, as detailed in 'DHC_Metering_Strategy_Rev1.pdf', is collected by our BMS and used for detailed energy consumption analysis of M&E subsystems, identification of operational inefficiencies, and the calculation of partial PUE (pPUE) metrics for cooling and power distribution paths. All sub-meters are on a 3-year calibration cycle.",
        "badExampleAnswer": "We only have a main meter for the whole data centre. We don't know how much energy each individual AC unit or chiller uses."
      },
      "question_144": {
        "id": 144,
        "old_question_text": null,
        "questionText": "Has the organization installed calibrated energy consumption meters at the IT cabinet or rack level (e.g., via intelligent rack PDUs with metering capabilities) to measure the energy consumed by the IT equipment within those specific cabinets or racks? Is this granular data used for purposes such as detailed energy analysis, capacity planning, customer billing (in colocation environments), or identifying underutilized or inefficient IT equipment?",
        "simplifiedQuestion": "Do you have energy meters at the IT cabinet/rack level (e.g., smart PDUs) to measure energy use of specific IT gear for detailed analysis, planning, or billing?",
        "reference": "2025 CoC Guide 9.1.6",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_16",
        "recommendationTitle": "Cabinet level metering of IT Energy consumption",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv",
            "screenshots"
          ],
          "description": "Specifications for intelligent rack PDUs with metering. Data centre layout showing deployment of metered PDUs. Examples of cabinet-level energy consumption reports. Documentation on how this data is used (analysis, planning, billing)."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, csv, screenshots",
        "documentDescriptor": "Optional: If implemented: Specifications for intelligent rack PDUs with metering. Evidence of their deployment. Examples of how cabinet-level energy data is used for detailed analysis, capacity planning, or billing.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "Data Verification"
        ],
        "verificationMethods": "Review of PDU specifications and deployment plans. Inspection of rack PDUs for metering capabilities. Examination of how cabinet-level energy data is collected, analyzed, and utilized.",
        "nonconformity": {
          "major": null,
          "minor": "No cabinet-level IT energy metering is implemented, or if intelligent PDUs are used, their metering features are not utilized, missing opportunities for granular energy analysis and management (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Intelligent Rack PDUs with metering capabilities are installed, but the data is not systematically collected, analyzed, or used for purposes such as detailed energy profiling, capacity planning at the rack level, or customer billing in colocation settings.",
        "whatThisMeans": "Your organization should consider installing special Power Distribution Units (PDUs) in your IT racks that can measure the energy being used by the equipment in each specific cabinet or rack. This detailed information can be very useful for in-depth energy analysis, planning how much more gear you can fit, billing customers in a colocation facility based on actual use, or spotting servers and other IT gear that are not being used much or are running inefficiently.",
        "complianceRequires": "- Install calibrated energy consumption meters at the IT cabinet or rack level.\n- This is typically achieved through the use of intelligent Rack Power Distribution Units (rPDUs) with built-in metering capabilities (e.g., per PDU, per phase, or per outlet group).\n- Utilize this granular data for purposes such as:\n    - Detailed energy analysis of IT equipment and racks.\n    - More accurate capacity planning at the cabinet/rack level.\n    - Customer billing in colocation environments based on actual consumption.\n    - Identifying underutilized or inefficient IT equipment within specific cabinets.",
        "expertTip": "Select intelligent rPDUs that offer not only energy (kWh) and power (kW) metering but also other useful parameters like current (A), voltage (V), and power factor (PF) at the inlet or outlet level. Ensure these rPDUs can be networked for centralized data collection and management via SNMP or other standard protocols.",
        "recommendedEvidence": [
          "- Specifications for intelligent rack PDUs (rPDUs) deployed, highlighting their metering capabilities (e.g., kWh, kW, A, V, per PDU, per outlet group).\n- Data centre rack layout diagrams indicating the deployment of metered rPDUs.\n- Examples of reports or dashboards from a monitoring system showing cabinet-level energy consumption data.\n- Documentation or case studies illustrating how this granular data is used for energy analysis, capacity planning, customer billing, or identifying inefficient IT assets."
        ],
        "recommendedActions": [
          "- When procuring new rack PDUs, prioritize models with accurate, built-in energy metering capabilities.\n- Implement a system for collecting and storing cabinet-level energy consumption data from these metered rPDUs.\n- Develop processes to regularly analyze this data to identify trends, anomalies, and opportunities for optimization (e.g., identify power-hungry or underutilized racks).\n- In colocation settings, use this data to implement fair and transparent energy billing for customers.\n- Correlate cabinet-level energy data with IT workload and utilization data to gain deeper insights."
        ],
        "sampleResponseTemplate": "We utilize [Vendor/Model] intelligent rack PDUs with [per-outlet/per-PDU] energy metering (kWh, kW, Amps) in all our [production/new] cabinets. Data is collected via [SNMP/DCIM Tool] and used for [e.g., 'detailed power trending per rack', 'capacity planning for new server deployments', 'identifying high-consumption racks for review']. This is part of our '[Granular Monitoring Strategy ID]'.",
        "humanInputAnswer": "All production racks in our primary data halls are equipped with APC AP89xx series metered-by-outlet Rack PDUs. These provide kWh, kW, Amps, and Volts per outlet, aggregated at the PDU level. Data is polled every 5 minutes by our StruxureWare DCIM. This granular data is used for detailed energy profiling of server types, capacity planning within rows, and identifying 'ghost' power draw from decommissioned-but-not-unplugged servers. We provide this data to customers in our colocation space for their energy usage visibility.",
        "llmGeneratedAnswer": "Yes, our organization has deployed intelligent Rack Power Distribution Units (rPDUs) with cabinet-level energy metering across all new data hall builds since 2023. These rPDUs (Model: [Specific rPDU Model]) provide real-time monitoring of power (kW) and energy consumption (kWh) at the PDU inlet level, and in some critical racks, at the outlet group level. This data is collected via SNMP and integrated into our Data Centre Infrastructure Management (DCIM) platform. It is utilized for: \n1.  Granular energy analysis to identify high-consumption racks/equipment. \n2.  Precise capacity planning at the rack and row level. \n3.  In our colocation services, for transparent customer energy billing (Ref: 'Colo_Billing_Policy_2025'). \n4.  Identifying underutilized servers by correlating power draw with CPU utilization.",
        "badExampleAnswer": "We plug servers into basic power strips in the racks. We only measure power at the main UPS that feeds a whole row or area."
      },
      "question_145": {
        "id": 145,
        "old_question_text": null,
        "questionText": "Has the organization installed calibrated temperature sensors to monitor air intake temperatures at the IT equipment cabinet or row level? Is this implemented with a sufficient number of sensors per cabinet/row (e.g., top, middle, bottom for cabinets) to accurately represent the thermal environment experienced by the IT equipment, enabling better airflow management and optimization of cooling delivery?",
        "simplifiedQuestion": "Do you have calibrated temperature sensors at your IT racks/rows (e.g., top/middle/bottom of cabinets) to monitor server intake air temperature accurately?",
        "reference": "2025 CoC Guide 9.1.7",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_16",
        "recommendationTitle": "Row or Cabinet level metering of temperature",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "dwg",
            "xlsx",
            "screenshots"
          ],
          "description": "Sensor layout plan showing placement of cabinet/row level temperature sensors. Sensor specifications and calibration records. BMS/DCIM screenshots showing data from these sensors. Procedures for using this data for airflow/cooling optimization."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, dwg, xlsx, screenshots",
        "documentDescriptor": "Mandatory: Documentation of cabinet/row level temperature sensor deployment (layouts, sensor specs). Calibration records. Evidence of data being used for thermal management and cooling optimization.",
        "verification_method": [
          "Document Review",
          "Site Inspection",
          "System Data Review"
        ],
        "verificationMethods": "Review of sensor deployment plans, calibration records, and procedures for data utilization. Physical inspection of sensor installations at cabinet/row level. Review of BMS/DCIM temperature data and trends.",
        "nonconformity": {
          "major": "No cabinet or row-level temperature monitoring is implemented, relying solely on room-level sensing, which provides insufficient granularity for effective airflow management or identification of localized thermal issues.",
          "minor": "Cabinet/row level sensors are deployed but coverage is sparse (e.g., only one sensor per row, or only on a few cabinets), calibration is lacking, or the data is not actively used to optimize local cooling delivery or identify hotspots."
        },
        "majorNonconformity": "Absence of calibrated temperature sensors at the IT equipment cabinet or row level, preventing detailed monitoring of intake air temperatures and hindering effective local airflow management.",
        "minorNonconformity": "Cabinet/row level temperature sensors are installed but with insufficient density (e.g., only one sensor per rack instead of multiple points like top/middle/bottom), are not regularly calibrated, or the collected data is not systematically used to optimize cooling delivery or identify thermal anomalies.",
        "whatThisMeans": "To better manage cooling, your organization should install accurate temperature sensors that measure the air going into your IT equipment, either at each row of racks or ideally at each individual cabinet. For cabinets, this often means sensors at the top, middle, and bottom of the front. This detailed temperature information helps you fine-tune airflow and cooling delivery much more effectively than just room-level sensors.",
        "complianceRequires": "- Install calibrated temperature sensors to monitor air intake temperatures at the IT equipment cabinet level or, at a minimum, at the row level.\n- Ensure a sufficient number of sensors are deployed per cabinet (e.g., typically at the top, middle, and bottom of the front/intake side) or per row to accurately represent the thermal environment experienced by the IT equipment.\n- Use this granular temperature data to enable better airflow management and optimization of cooling delivery to the IT equipment.",
        "expertTip": "Deploy wireless temperature sensors for easier installation and flexibility in placement, especially in existing facilities. Integrate this cabinet/row level temperature data with your BMS/DCIM to create thermal maps of your data halls. Use these maps to identify hotspots, overcooled areas, and opportunities for airflow optimization (e.g., adjusting vented tile placement, fan speeds).",
        "recommendedEvidence": [
          "- Data centre layout diagrams or documentation showing the placement strategy and density of cabinet/row level temperature sensors.\n- Specifications for the temperature sensors used, including accuracy and calibration procedures.\n- Recent calibration records for these sensors.\n- BMS/DCIM screenshots or reports showing temperature readings from these sensors (e.g., thermal maps, rack inlet temperature profiles).\n- Examples of how this granular temperature data is used to make cooling adjustments or identify airflow issues."
        ],
        "recommendedActions": [
          "- Develop a strategy for deploying calibrated temperature sensors at the cabinet or row level, ensuring adequate coverage (e.g., multiple sensors per cabinet front).\n- Install sensors according to the plan and integrate them into a centralized monitoring system.\n- Establish procedures for regularly reviewing cabinet/row level temperature data to identify thermal anomalies, hotspots, or overcooled areas.\n- Use this data to optimize local airflow (e.g., vented tile adjustments, blanking panel placement) and cooling unit setpoints/fan speeds.\n- Ensure regular calibration of these sensors."
        ],
        "sampleResponseTemplate": "We have deployed [Number] temperature sensors per [cabinet/row] in [Data Hall X], typically at [top, middle, bottom of the front face], as per our '[Thermal Monitoring Plan ID]'. These sensors ([Make/Model], last calibrated [Date]) feed data into our [DCIM/BMS], enabling us to [e.g., 'generate thermal maps, identify racks nearing temperature thresholds, and fine-tune CRAH fan speeds']. Data showing consistent intake temperatures is available from [Report Y].",
        "humanInputAnswer": "All our production racks are equipped with three calibrated temperature sensors (top, middle, bottom) at the front (intake). This data is logged via our StruxureWare DCIM and used to generate real-time thermal maps of each aisle. This allows us to proactively manage airflow and identify any potential hotspots or overcooling. For example, last month, this data helped us pinpoint an airflow imbalance in Row B, Pod 3, which was then corrected by adjusting perforated tile placement. Sensor calibration is annual (Ref: 'Sensor_Cal_Schedule_2025').",
        "llmGeneratedAnswer": "Yes, our organization has implemented cabinet-level temperature monitoring. Each IT cabinet in our primary data halls is fitted with a minimum of three calibrated temperature sensors (Model: [SensorType], accuracy \u00b10.5\u00b0C) positioned at the top, middle, and bottom of the front air intake. This deployment strategy, detailed in 'Data Centre Environmental Monitoring Standard (DCEMS-2025, Section 4.2)', ensures an accurate representation of the thermal environment experienced by the IT equipment. Data is continuously streamed to our DCIM platform, enabling granular thermal mapping, proactive hotspot detection, and optimized airflow management. Sensor calibration is performed annually, with records maintained in '[Calibration_Database_Ref]'.",
        "badExampleAnswer": "We have a temperature sensor for the whole room, and if it gets too warm, we turn up the main AC. We don't monitor individual racks."
      },
      "question_146": {
        "id": 146,
        "old_question_text": null,
        "questionText": "Has the organization implemented the capability to monitor the air inlet temperature of individual IT devices (servers, storage, networking equipment) through their built-in sensors, typically accessible via system management interfaces like IPMI, Redfish, or SNMP (as encouraged by Practice 4.1.11)? Is this device-level temperature data utilized for detailed thermal analysis, identifying equipment at risk, or for dynamic control of cooling resources based on actual IT device needs rather than general room or rack conditions?",
        "simplifiedQuestion": "Do you monitor the individual air intake temperatures of your IT devices using their built-in sensors (via IPMI, Redfish, SNMP) for detailed thermal analysis or dynamic cooling control?",
        "reference": "2025 CoC Guide 9.1.8",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_16",
        "recommendationTitle": "IT Device level metering of temperature",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "Documentation of systems/tools used to collect IT device-level inlet temperature data (e.g., via IPMI, Redfish, SNMP). Examples of collected data and how it's used for thermal analysis, risk identification, or dynamic cooling control. Confirmation that IT equipment supports such reporting (Practice 4.1.11)."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Optional: If implemented: Documentation of methods used to collect and utilize IT device-level inlet temperature data (via IPMI, Redfish, SNMP etc.). Examples of analysis or control actions based on this data. Link to Practice 4.1.11 (reporting hardware).",
        "verification_method": [
          "Document Review",
          "System Demonstration",
          "Interview"
        ],
        "verificationMethods": "Review of IT monitoring system configurations and capabilities for collecting device-level temperature data. Demonstration of data collection and analysis. Interviews with IT operations and monitoring teams.",
        "nonconformity": {
          "major": null,
          "minor": "IT equipment has built-in temperature reporting capabilities (per 4.1.11), but this device-level data is not collected or utilized for thermal analysis or to inform cooling control strategies (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: While IT equipment supports inlet temperature reporting (as per Practice 4.1.11), this device-level data is not systematically collected or used for detailed thermal analysis, identifying equipment at thermal risk, or for potential dynamic control of cooling resources.",
        "whatThisMeans": "Modern IT equipment (servers, etc.) often has its own built-in sensors that can report the temperature of the air going into it. Your organization should try to collect this information, usually through management interfaces like IPMI, Redfish, or SNMP (as encouraged in Practice 4.1.11). This very detailed temperature data from each device can be used for in-depth thermal analysis, spotting equipment that might be getting too hot, or even for dynamically adjusting your cooling systems based on the actual real-time needs of the IT gear, rather than just general room or rack temperatures.",
        "complianceRequires": "- Implement the capability to monitor the air inlet temperature of individual IT devices (servers, storage arrays, network switches).\n- Utilize the built-in temperature sensors within these devices, typically accessible via system management interfaces such as:\n    - IPMI (Intelligent Platform Management Interface).\n    - DMTF Redfish API.\n    - SNMP (Simple Network Management Protocol).\n- This leverages the capabilities encouraged by Practice 4.1.11 ('Energy & temperature reporting hardware' for IT equipment).\n- Use this device-level temperature data for purposes such as:\n    - Detailed thermal analysis of the IT environment.\n    - Identifying specific IT equipment at thermal risk.\n    - Potentially enabling dynamic control of cooling resources based on the actual thermal needs of individual IT devices.",
        "expertTip": "Integrate device-level temperature data into your DCIM or centralized monitoring platform. Correlate this data with IT workload, power consumption, and server fan speeds to gain comprehensive insights into thermal performance and identify opportunities for fine-tuning cooling delivery or addressing airflow inefficiencies at a very granular level.",
        "recommendedEvidence": [
          "- Documentation of the monitoring system or tools used to collect IT device-level inlet temperature data (e.g., from server BMCs via IPMI/Redfish, or from network devices via SNMP).\n- Configuration details showing which devices are polled and what temperature metrics are collected.\n- Examples of reports or dashboards displaying device-level inlet temperatures.\n- Case studies or examples of how this data has been used to identify thermal issues, optimize cooling, or inform capacity planning.\n- Evidence that the IT equipment supports this reporting capability (linking back to Practice 4.1.11)."
        ],
        "recommendedActions": [
          "- Ensure that new IT equipment is procured with capabilities for reporting inlet temperature via standard management protocols (as per Practice 4.1.11).\n- Implement or configure monitoring tools to collect and store inlet temperature data from individual IT devices.\n- Develop methods for analyzing this granular thermal data to identify trends, hotspots, and correlations with IT load or airflow conditions.\n- Explore using device-level temperature feedback to dynamically adjust cooling setpoints or airflow delivery where feasible and safe."
        ],
        "sampleResponseTemplate": "We collect IT device-level inlet temperatures for [all critical servers / a subset of high-density racks] using [IPMI/Redfish/SNMP] via our [Monitoring System Name]. This data (as shown in '[Sample Report ID]') is used for [e.g., 'detailed thermal profiling of new server models', 'identifying individual servers approaching warning thresholds', 'validating airflow improvements after cabinet changes']. This capability is supported by our hardware selection as per practice 4.1.11.",
        "humanInputAnswer": "Our Nagios XI monitoring system polls inlet temperature sensors from all production servers via IPMI every 5 minutes. This data is stored, trended, and used to generate alerts if any server reports inlet temperatures exceeding 28\u00b0C. For our HPC cluster, we also use this data as an input to a custom script that can request increased localized airflow from adjacent CRAH units if multiple nodes in a rack show rising temperatures. This relies on servers compliant with Practice 4.1.11.",
        "llmGeneratedAnswer": "Yes, our organization leverages the built-in temperature sensors of individual IT devices. As per our 'Advanced Thermal Monitoring Protocol (ATMP-2025)', air inlet temperatures are collected from servers (via Redfish API), storage arrays (via proprietary APIs where available or SNMP), and core network switches (via SNMP). This data, aggregated in our [DCIM/IT Monitoring Platform Name], is used for: \n1.  Granular thermal mapping and hotspot identification at the device level. \n2.  Early warning alerts for equipment approaching thermal thresholds. \n3.  Input for our planned dynamic cooling adjustment algorithms (currently in pilot). \nThis capability is contingent on IT equipment supporting such reporting, as specified in our procurement Practice 4.1.11.",
        "badExampleAnswer": "We don't check individual server temperatures unless there's an alert from the server itself. The room temperature is what we mainly monitor."
      },
      "question_147": {
        "id": 147,
        "old_question_text": null,
        "questionText": "Has the organization implemented the capability to monitor the energy consumption of individual IT devices (servers, storage, networking equipment) through their built-in power reporting features, typically accessible via system management interfaces like IPMI, Redfish, or SNMP (as encouraged by Practice 4.1.11)? Is this device-level energy data utilized for detailed energy analysis, identifying inefficient equipment, attributing energy costs, or for dynamic power management strategies like power capping based on actual device consumption?",
        "simplifiedQuestion": "Do you monitor the individual energy use of your IT devices using their built-in reporting (via IPMI, Redfish, SNMP) for detailed analysis, cost attribution, or power management?",
        "reference": "2025 CoC Guide 9.1.9",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_16",
        "recommendationTitle": "IT Device level metering of energy consumption",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "screenshots"
          ],
          "description": "Documentation of systems/tools used to collect IT device-level energy consumption data (e.g., via IPMI, Redfish, SNMP). Examples of collected data and how it's used (energy analysis, cost allocation, power capping). Confirmation that IT equipment supports such reporting (Practice 4.1.11)."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, screenshots",
        "documentDescriptor": "Optional: If implemented: Documentation of methods used to collect and utilize IT device-level energy consumption data (via IPMI, Redfish, SNMP etc.). Examples of how this data informs energy analysis, cost attribution, or dynamic power management. Link to Practice 4.1.11 (reporting hardware).",
        "verification_method": [
          "Document Review",
          "System Demonstration",
          "Interview"
        ],
        "verificationMethods": "Review of IT monitoring system configurations for collecting device-level energy data. Demonstration of data collection and analysis. Interviews with IT operations, energy management, and capacity planning teams.",
        "nonconformity": {
          "major": null,
          "minor": "IT equipment has built-in energy reporting capabilities (per 4.1.11), but this device-level data is not collected or utilized for energy analysis, cost attribution, or power management strategies (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: While IT equipment supports energy consumption reporting (as per Practice 4.1.11), this device-level data is not systematically collected, or if collected, it is not used for detailed energy analysis, identifying inefficient devices, attributing energy costs, or implementing dynamic power management strategies like power capping.",
        "whatThisMeans": "Modern IT equipment often has built-in features to report its own energy consumption (power draw). Your organization should try to collect this information from individual devices (servers, storage, network gear) using their management interfaces (like IPMI, Redfish, or SNMP \u2013 as encouraged in Practice 4.1.11). This very detailed energy data can be used for in-depth energy analysis, spotting inefficient equipment, accurately assigning energy costs to different services or departments, or for dynamically managing power, for example, by setting power limits (capping) on devices based on their actual usage.",
        "complianceRequires": "- Implement the capability to monitor the energy consumption of individual IT devices (servers, storage arrays, network switches).\n- Utilize the built-in power reporting features within these devices, typically accessible via system management interfaces such as:\n    - IPMI (Intelligent Platform Management Interface).\n    - DMTF Redfish API.\n    - SNMP (Simple Network Management Protocol).\n- This leverages the capabilities encouraged by Practice 4.1.11 ('Energy & temperature reporting hardware' for IT equipment).\n- Use this device-level energy data for purposes such as:\n    - Detailed energy analysis and profiling of IT equipment.\n    - Identifying inefficient or underperforming IT devices.\n    - Attributing energy costs to specific services, applications, or users.\n    - Implementing dynamic power management strategies, such as power capping, based on actual device-level consumption.",
        "expertTip": "Correlate device-level energy consumption data with device-level utilization (CPU, memory, I/O) and performance metrics. This allows for the calculation of energy efficiency metrics per device (e.g., transactions per kWh, workload units per kWh) and helps identify opportunities for optimizing both software and hardware for energy performance. This data is also invaluable for accurate TCO modeling.",
        "recommendedEvidence": [
          "- Documentation of the monitoring system or tools used to collect IT device-level energy consumption data (e.g., from server BMCs via IPMI/Redfish, or from network devices via SNMP).\n- Configuration details showing which devices are polled and what energy/power metrics are collected (e.g., Watts, kWh).\n- Examples of reports or dashboards displaying device-level energy consumption.\n- Case studies or examples of how this data has been used for energy optimization, cost allocation, identifying inefficient equipment, or implementing power capping.\n- Evidence that the IT equipment supports this reporting capability (linking back to Practice 4.1.11)."
        ],
        "recommendedActions": [
          "- Ensure that new IT equipment is procured with capabilities for reporting energy consumption via standard management protocols (as per Practice 4.1.11).\n- Implement or configure monitoring tools to collect and store energy consumption data from individual IT devices.\n- Develop methods for analyzing this granular energy data to identify high-consuming or inefficient devices and opportunities for optimization.\n- Explore using device-level energy data for internal energy cost showback/chargeback to business units or services.\n- If appropriate for the workload, implement policy-based power capping using device-level controls."
        ],
        "sampleResponseTemplate": "We collect IT device-level energy consumption (Watts and cumulative kWh) for [all new servers / critical systems] using [IPMI/Redfish] via our [DCIM/Energy Management Platform Name]. This data, enabled by hardware compliant with Practice 4.1.11, is used for [e.g., 'identifying power-hungry applications during specific processing windows', 'internal energy accounting per department', 'validating power budgets for new deployments']. We are currently piloting dynamic power capping on our [Test/Dev cluster] based on this real-time data.",
        "humanInputAnswer": "Our 'Server Monitoring Standard (SMS-002)' requires all new servers to report power consumption (Watts) via their Redfish API, which is collected by our OpenManage Enterprise system. This data is used for energy consumption trending per application stack and helps us identify outliers or potential hardware issues. For example, we recently identified a server consuming 30% more power than identical peers, which was traced to a faulty DIMM. This leverages Practice 4.1.11.",
        "llmGeneratedAnswer": "Yes, our organization has implemented the capability to monitor energy consumption at the individual IT device level for a significant portion of our server fleet. Utilizing the DMTF Redfish API on compliant hardware (as per Practice 4.1.11), our custom monitoring scripts collect real-time power draw (Watts) and accumulated energy (kWh) for servers in our 'High-Density Compute' and 'Virtualization' clusters. This data, stored in our central analytics platform, is used for:\n1.  Detailed energy profiling of different hardware models and workloads.\n2.  Identifying and investigating anomalous energy consumption patterns.\n3.  Providing input for our internal energy cost allocation model for research groups.\n4.  Evaluating the effectiveness of power management policies (e.g., C-state utilization).",
        "badExampleAnswer": "We only monitor power at the PDU level for groups of servers. Getting data from each server is too complicated."
      }
    }
  },
  "step_17": {
    "description": "2025 Guide Section 9: Monitoring (Practices 9.2.1-9.2.4 from Sec 9.2 'Data Collection & Logging'; and 9.3.1-9.3.5 from Sec 9.3 'Performance Reporting')",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Monitoring_2"
    ],
    "critical_questions": [],
    "questions": {
      "question_148": {
        "id": 148,
        "old_question_text": null,
        "questionText": "Has the organization established a process for the periodic manual collection of readings from non-networked energy meters and environmental sensors (e.g., temperature, humidity, airflow, pressure) within the data centre? Is this data logged consistently and used for basic monitoring, trend analysis, and identifying potential issues where automated collection is not yet in place?",
        "simplifiedQuestion": "Do you have a process for manually collecting and logging readings from energy/environmental sensors that aren't networked?",
        "reference": "2025 CoC Guide 9.2.1",
        "priority": "Low (Value: 1, Expected: Optional Practice)",
        "stepSection": "step_17",
        "recommendationTitle": "Periodic manual readings",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv",
            "log"
          ],
          "description": "Procedures for manual data collection from non-networked meters/sensors. Logs or spreadsheets of manually collected data. Examples of how this data is used for monitoring or analysis."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, csv, log",
        "documentDescriptor": "Optional: If practiced: Documented procedures for manual data collection from non-networked meters and sensors. Logs of collected data and evidence of its use for basic monitoring or trend analysis.",
        "verification_method": [
          "Document Review",
          "Interview",
          "Log Review"
        ],
        "verificationMethods": "Review of any procedures for manual data collection and logs of this data. Interviews with facilities staff regarding manual monitoring practices.",
        "nonconformity": {
          "major": null,
          "minor": "Non-networked meters/sensors exist, but data is not collected manually or is collected inconsistently, or logged data is not utilized for any monitoring or analysis (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If non-networked meters/sensors providing useful data exist and this practice is adopted: Failure to establish or follow a process for periodic manual collection and consistent logging of readings, or failure to use this data for basic monitoring or trend analysis.",
        "whatThisMeans": "If you have energy meters or environmental sensors in your data centre that aren't connected to a network for automatic data collection, your organization should have a set procedure for someone to go around and manually read them periodically. This data (like energy use, temperature, humidity) should be written down (logged) consistently so it can be used for basic monitoring, looking for trends, or spotting potential problems, especially if you don't have automated systems yet.",
        "complianceRequires": "- Establish a process for periodic manual collection of readings from non-networked energy meters (as per Section 9.1).\n- Extend this process to include manual collection from non-networked environmental sensors (e.g., temperature, humidity, airflow, pressure sensors from Section 9.1).\n- Ensure that this manually collected data is logged consistently.\n- Utilize the logged data for basic monitoring, trend analysis, and identifying potential operational issues or anomalies where automated data collection (Practices 9.2.2, 9.2.3) is not yet implemented.",
        "expertTip": "Use standardized log sheets (paper or digital, like a spreadsheet on a tablet) for manual data collection to ensure consistency. Define the reading frequency based on the criticality of the measurement and the stability of the system. Even if data is collected manually, inputting it into a spreadsheet or simple database allows for easier trending and analysis over time.",
        "recommendedEvidence": [
          "- Documented procedure for manual data collection, specifying which meters/sensors are read, frequency, and logging method.\n- Examples of log sheets or spreadsheets used for manual data recording.\n- Trend charts or analysis reports derived from manually collected data.\n- Evidence that this data is reviewed and used to inform operational decisions or identify issues."
        ],
        "recommendedActions": [
          "- Identify all non-networked energy meters and environmental sensors that provide useful data for energy management or operational reliability.\n- Establish a schedule and clear procedure for manually collecting readings from these devices.\n- Implement a consistent method for logging the collected data (e.g., dedicated logbooks, spreadsheets).\n- Assign responsibility for collecting, logging, and periodically reviewing this data.\n- Use the data to supplement automated monitoring and to identify areas where automated collection might be beneficial in the future."
        ],
        "sampleResponseTemplate": "For our [older/specific non-networked systems, e.g., 'standalone PDU in Lab A', 'backup generator fuel tank level'], we have a 'Manual Data Logging Procedure' ([Doc ID: MDL-001]). Readings for [Parameter X, Y] are taken [daily/weekly] by [Role] and recorded in [Logbook/Spreadsheet Z]. This data is reviewed [monthly] for [e.g., 'basic trend analysis and anomaly detection'].",
        "humanInputAnswer": "While most of our critical monitoring is automated, for certain legacy standalone PDUs in our staging lab, energy readings (kWh) are taken manually on the first working day of each month by a facilities technician. These readings are logged into a shared spreadsheet (Ref: 'Manual_EnergyLog_StagingLab.xlsx'). This data is reviewed quarterly by the Lab Manager to track approximate energy consumption trends for that area. This is a supplementary process to our main BMS.",
        "llmGeneratedAnswer": "Yes, although our primary strategy is automated data collection, a documented process exists for 'Periodic Manual Data Acquisition from Non-Networked Devices (PMDA-ND-2025)'. This applies to a limited number of older, non-networked environmental sensors (e.g., standalone temperature/humidity loggers in storage areas) and some legacy sub-meters. Readings are collected weekly by facilities staff using a standardized form (Form PMDA-F01) and logged into a central Excel repository. This data is reviewed monthly for significant deviations or trends, primarily serving as a backup or gap-fill where automated systems are not yet fully deployed. This practice is considered secondary to our automated data collection (Practices 9.2.2 and 9.2.3).",
        "badExampleAnswer": "We don't have many old meters that aren't networked. If we need a reading, someone just goes and looks, but we don't usually write it down unless it's an issue."
      },
      "question_149": {
        "id": 149,
        "old_question_text": null,
        "questionText": "Has the organization implemented automated systems for data collection and performance logging of energy use and environmental data (as identified in Section 9.1) with a minimum frequency of daily readings? Is this data stored in a system that allows for historical trend analysis, comparison, and reporting?",
        "simplifiedQuestion": "Do you have automated systems collecting energy and environmental data at least daily, storing it for trend analysis and reporting?",
        "reference": "2025 CoC Guide 9.2.2",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_17",
        "recommendationTitle": "Automated daily readings",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "screenshots",
            "xlsx",
            "csv"
          ],
          "description": "Documentation of the automated data collection system (BMS, DCIM, EMS). Evidence of data logging frequency (minimum daily). Screenshots or examples of historical data storage, trend analysis capabilities, and reporting features."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, screenshots, xlsx, csv",
        "documentDescriptor": "Mandatory: Documentation of automated systems (BMS, DCIM, EMS) used for data collection and performance logging of energy and environmental data (from Section 9.1). Evidence of data being logged at least daily and stored for historical trend analysis and reporting.",
        "verification_method": [
          "Document Review",
          "System Demonstration",
          "Data Verification"
        ],
        "verificationMethods": "Review of BMS/DCIM/EMS documentation and configuration. Demonstration of automated data collection, logging frequency, and historical data access/analysis capabilities. Verification of data integrity and storage duration.",
        "nonconformity": {
          "major": "No automated system is in place for collecting and logging energy and environmental data daily, or data is collected but not stored in a way that allows for historical trend analysis or reporting.",
          "minor": "Automated data collection exists but logging frequency is less than daily for key parameters, data storage is limited, or system capabilities for trend analysis/reporting are not effectively utilized."
        },
        "majorNonconformity": "Absence of an automated system for collecting and logging key energy use and environmental data (as identified in Section 9.1) at a minimum frequency of daily readings, or failure to store this data in a system that enables historical trend analysis.",
        "minorNonconformity": "Automated data collection is implemented, but not all key parameters from Section 9.1 are included, the logging frequency for some critical data is less than daily, or the system's capabilities for historical analysis and reporting are underutilized.",
        "whatThisMeans": "Your organization should have automatic systems (like a Building Management System - BMS, Data Centre Infrastructure Management - DCIM, or Energy Management System - EMS) that collect and record energy use and environmental data (from the sensors and meters mentioned in section 9.1) at least once every day. This data needs to be stored in a way that lets you look back at historical trends, make comparisons, and create reports.",
        "complianceRequires": "- Implement automated systems for data collection and performance logging of energy use and environmental data (as identified in Section 9.1).\n- Ensure these systems collect and log data with a minimum frequency of daily readings for all relevant parameters.\n- Store the collected data in a system (e.g., BMS, DCIM, EMS, dedicated database) that allows for:\n    - Historical trend analysis.\n    - Comparison of data over time or between different points.\n    - Generation of reports.",
        "expertTip": "While daily readings are a minimum, aim for more frequent automated data collection for dynamic parameters (e.g., 5-15 minute intervals for power, temperature, humidity). This provides much richer data for operational control, detailed analysis, and accurate PUE calculations at higher categories. Ensure your data storage capacity can handle the increased volume from more frequent logging.",
        "recommendedEvidence": [
          "- System architecture or description of the automated data collection platform (BMS, DCIM, EMS).\n- Configuration settings or system documentation confirming that data logging frequency is at least daily for key energy and environmental parameters.\n- Screenshots or examples from the system demonstrating its capability to store historical data, display trends, and generate reports.\n- Data retention policy for this logged data."
        ],
        "recommendedActions": [
          "- Implement or upgrade a BMS, DCIM, or EMS to automate the collection and logging of all relevant energy and environmental data points identified in Section 9.1.\n- Configure the system to collect data at least daily, and more frequently for critical or rapidly changing parameters.\n- Ensure the system has adequate storage capacity for long-term historical data retention.\n- Train relevant staff on how to access, analyze, and report on the collected data using the system's tools."
        ],
        "sampleResponseTemplate": "Our [BMS/DCIM System Name, e.g., Schneider EcoStruxure IT Expert] automatically collects and logs all energy and environmental data points listed in Section 9.1. Key parameters like [PUE_TOTAL, IT_Load_kWh, average cold aisle temp] are logged at [e.g., 15-minute intervals], well exceeding the daily minimum. The system stores data for [e.g., 7 years] and provides tools for trend analysis and custom reporting, as shown in the attached [System Screenshot/Sample Report].",
        "humanInputAnswer": "Our Building Management System (Siemens Desigo CC) automatically collects all energy meter readings (main, IT, M&E sub-meters) and environmental sensor data (room, CRAH, cabinet level temperatures/humidity) at 15-minute intervals. This data is stored in a SQL database with a 5-year retention policy. The BMS has built-in trending and reporting tools, and we also export data to a central data warehouse for more advanced analytics. Daily summary reports are automatically generated.",
        "llmGeneratedAnswer": "Yes, our organization has implemented an automated data collection and performance logging system. Our Data Centre Infrastructure Management (DCIM) platform, [DCIM Vendor/Product Name], interfaces with all critical energy meters (Practice 9.1.1, 9.1.2, 9.1.5, 9.1.6) and environmental sensors (Practice 9.1.3, 9.1.4, 9.1.7, 9.1.8) across the facility. Data points, including energy consumption (kWh), power (kW), temperature (\u00b0C), and relative humidity (%), are logged at a minimum frequency of every 15 minutes (exceeding the daily requirement). This data is stored within the DCIM's historical database for a minimum of 3 years, supporting comprehensive trend analysis, performance comparison, and automated report generation as per our 'Data Logging and Archival Policy (DLAP-2025)'.",
        "badExampleAnswer": "We write down the main electricity meter reading once a week. For temperatures, someone walks around and checks them daily if they have time."
      },
      "question_150": {
        "id": 150,
        "old_question_text": null,
        "questionText": "Beyond daily readings (Practice 9.2.2), has the organization implemented automated systems for data collection and performance logging of the key energy use and environmental data points (identified in Section 9.1) with a minimum frequency of hourly readings? Is this higher frequency data stored in a system that allows for more granular historical trend analysis, detailed comparison, diagnostics, and more responsive operational control or alarming?",
        "simplifiedQuestion": "Do your automated systems collect key energy and environmental data at least hourly (more frequently than daily) for detailed analysis and control?",
        "reference": "2025 CoC Guide 9.2.3",
        "priority": "Medium (Value: 3, Expected: Optional Practice)",
        "stepSection": "step_17",
        "recommendationTitle": "Automated hourly readings",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "screenshots",
            "xlsx",
            "csv"
          ],
          "description": "Documentation of automated data collection system (BMS, DCIM, EMS) showing hourly (or more frequent) logging for key parameters. Examples of granular trend analysis, diagnostic use cases, or responsive controls based on this higher frequency data."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, screenshots, xlsx, csv",
        "documentDescriptor": "Optional: If implemented: Documentation of automated data collection systems logging key energy/environmental data at least hourly. Evidence of how this granular data is used for detailed trend analysis, diagnostics, or improved operational control.",
        "verification_method": [
          "Document Review",
          "System Demonstration",
          "Data Verification"
        ],
        "verificationMethods": "Review of BMS/DCIM/EMS documentation and configuration for logging frequencies. Demonstration of data granularity and use in analysis or control. Verification of data storage and accessibility for hourly readings.",
        "nonconformity": {
          "major": null,
          "minor": "Automated data collection logs key parameters less frequently than hourly, or if hourly data is collected, it's not effectively utilized for granular analysis, diagnostics, or enhancing operational responsiveness (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: While automated data collection exists (per 9.2.2), key energy and environmental parameters are not logged at a minimum frequency of hourly, or if they are, this higher frequency data is not leveraged for more detailed analysis, faster diagnostics, or more responsive control actions.",
        "whatThisMeans": "Going beyond just daily readings (from Practice 9.2.2), your organization should consider setting up its automated systems to collect and record key energy and environmental data much more often \u2013 at least every hour. This more frequent data allows for a much more detailed look at trends over time, helps in comparing performance more closely, assists in diagnosing problems quicker, and can enable your control systems to react more swiftly and appropriately to changing conditions in the data centre.",
        "complianceRequires": "- Implement automated systems for data collection and performance logging of key energy use and environmental data points (as identified in Section 9.1).\n- Ensure these systems collect and log this data with a minimum frequency of hourly readings (or more frequently where beneficial, e.g., 15-minute intervals for power and critical temperatures).\n- Store this higher frequency data in a system that allows for:\n    - More granular historical trend analysis.\n    - Detailed comparison of performance over shorter timeframes.\n    - Improved diagnostics of operational issues.\n    - More responsive operational control and alarming.",
        "expertTip": "For critical parameters like PUE components (total facility power, IT load), key temperatures (cold aisle, hot aisle, chiller water), and humidity, aim for logging intervals of 15 minutes or less. This level of granularity is often required for advanced PUE calculations (Category 2 or 3) and enables much more effective real-time operational decision-making and diagnostics.",
        "recommendedEvidence": [
          "- System architecture or description of the automated data collection platform (BMS, DCIM, EMS) demonstrating its capability for hourly or more frequent data logging.\n- Configuration settings or system documentation confirming the logging frequency for key energy and environmental parameters (e.g., PUE_TOTAL, IT_Load_kWh, average cold aisle temperature, CRAC/H supply/return temps, humidity, etc.).\n- Examples from the system showing granular (e.g., hourly) historical data, trend charts derived from this data, and reports or dashboards utilizing this level of detail.\n- Examples of how hourly data has been used for diagnostics or to improve control responsiveness."
        ],
        "recommendedActions": [
          "- Configure existing BMS, DCIM, or EMS systems to log key energy and environmental data at least hourly, and preferably at 5 to 15-minute intervals for dynamic parameters.\n- Ensure sufficient data storage and processing capability to handle the increased volume of high-frequency data.\n- Train staff on how to use the granular data for detailed trend analysis, troubleshooting, and optimizing control system performance.\n- Develop automated reports and dashboards that leverage this higher frequency data to provide actionable insights to operators and managers."
        ],
        "sampleResponseTemplate": "Our [BMS/DCIM System Name] logs key energy metrics (Total Facility Power, IT Load Power) and critical environmental data (e.g., [cold aisle temperatures, CRAH supply/return temperatures]) at [e.g., 15-minute / 5-minute] intervals. This exceeds the hourly minimum and allows for detailed trend analysis, as shown in the attached [Sample Trend Report / BMS Screenshot]. This data is stored for [X years] and is used for [e.g., 'real-time PUE calculation', 'diagnostic alarming for cooling anomalies', 'fine-tuning control loop performance'].",
        "humanInputAnswer": "Our Schneider Electric Power Monitoring Expert (PME) system collects all electrical sub-meter data (total, IT, M&E) at 1-minute intervals, and our Siemens Desigo BMS collects all environmental sensor data (temperatures, humidity, pressures) at 5-minute intervals. This data is stored for 13 months at full granularity and then aggregated to hourly for long-term storage (7 years). This allows for very detailed forensic analysis of incidents and precise tracking of energy performance initiatives.",
        "llmGeneratedAnswer": "Yes, our organization's Data Centre Infrastructure Management (DCIM) platform ([Specific DCIM Platform Name]) is configured for automated high-frequency data collection. Key energy parameters (e.g., total facility load, IT equipment load, UPS efficiency) and critical environmental data points (e.g., server inlet temperatures, CRAC/H supply/return temperatures, differential pressures) are logged at 15-minute intervals (exceeding the daily requirement). This practice, documented in our 'Data Collection & Logging Standard (DCLS-2025)', exceeds the hourly minimum and provides the granularity required for detailed historical trend analysis, rapid diagnostics of anomalies, and supports our near real-time operational dashboards and PUE Category 2 calculations. Data is archived with this granularity for 18 months, then summarized to hourly for long-term retention.",
        "badExampleAnswer": "Our BMS takes readings once a day. We find that's enough to see if everything is generally okay."
      },
      "question_151": {
        "id": 151,
        "old_question_text": null,
        "questionText": "Has the organization implemented a system to monitor and log the hours of operation for any installed 'Free Cooling' or economized cooling systems (as described in Section 5.4.1)? Is this data used to verify the effectiveness of the free cooling system against its design assumptions or targets, to identify opportunities for increasing free cooling utilization, and to quantify the energy savings achieved?",
        "simplifiedQuestion": "Do you monitor and log how many hours your 'Free Cooling' / economizer systems run, to check their effectiveness and savings?",
        "reference": "2025 CoC Guide 9.2.4",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre - if Free Cooling is installed)",
        "stepSection": "step_17",
        "recommendationTitle": "Achieved free cooling / economised cooling hours",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv",
            "screenshots"
          ],
          "description": "BMS/DCIM configuration or operational logs showing monitoring of free cooling system operational hours (full and partial modes). Reports analyzing achieved free cooling hours against design targets or ambient conditions. Calculations of energy savings attributed to free cooling."
        },
        "docRequired": "Yes, if Free Cooling installed",
        "acceptedFiles": "pdf, docx, xlsx, csv, screenshots",
        "documentDescriptor": "Mandatory (if free cooling/economization is used): System for monitoring and logging the operational hours of free cooling/economized cooling systems. Reports showing achieved hours and analysis of effectiveness against design or potential.",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "BMS Configuration Review"
        ],
        "verificationMethods": "Review of BMS/DCIM configuration for free cooling mode logging. Analysis of operational data (logs, reports) showing hours of free cooling operation. Comparison with design expectations or climate potential.",
        "nonconformity": {
          "major": "Free cooling systems are installed but their operational hours (full/partial modes) are not monitored or logged, preventing assessment of their effectiveness and energy savings.",
          "minor": "Free cooling hours are logged but the data is not regularly analyzed to verify performance against design, identify optimization opportunities, or quantify energy savings."
        },
        "majorNonconformity": "Failure to monitor and log the operational hours of installed 'Free Cooling' or economized cooling systems, making it impossible to assess their effectiveness or quantify energy savings.",
        "minorNonconformity": "While free cooling operational hours may be logged, this data is not systematically analyzed to compare actual performance against design targets, identify opportunities for increasing utilization, or rigorously quantify the achieved energy savings.",
        "whatThisMeans": "If your data centre uses any form of 'free cooling' or economizer system (like those discussed in section 5.4.1), you must have a way to automatically track and record how many hours it's actually running (both in full free cooling mode and any partial or mixed modes). This information is vital to check if the system is working as well as it was designed to, to see if you can use it even more, and to calculate how much energy it's actually saving.",
        "complianceRequires": "- Implement a system to monitor and log the hours of operation for any installed 'Free Cooling' or economized cooling systems (as detailed in Section 5.4.1).\n- This data should differentiate between full free cooling, partial free cooling (mixed mode), and full mechanical cooling operation where applicable.\n- Utilize this logged data to:\n    - Verify the effectiveness of the free cooling system against its original design assumptions or operational targets.\n    - Identify opportunities for increasing the utilization of free cooling (e.g., by adjusting changeover setpoints, improving controls).\n    - Quantify the energy savings achieved through the use of free cooling.",
        "expertTip": "Integrate free cooling mode status and operational hours into your BMS/DCIM. Create dashboards that visualize achieved free cooling hours against theoretically available hours (based on ambient conditions) to identify potential control system inefficiencies or missed opportunities. Correlate this with energy sub-metering for chillers/compressors to directly measure savings.",
        "recommendedEvidence": [
          "- BMS or control system configuration showing parameters logged for free cooling operation (e.g., mode status, hours counters for different modes).\n- Trend data or reports from the monitoring system showing daily, monthly, or annual hours of operation in full free cooling, partial/mixed mode, and full mechanical cooling.\n- Analysis comparing actual achieved free cooling hours with design targets or theoretical potential based on weather data.\n- Calculations or reports quantifying the energy savings (e.g., kWh reduction in chiller consumption) attributed to the free cooling system."
        ],
        "recommendedActions": [
          "- Ensure that the control system for any free cooling/economizer installation includes logic to monitor and log its operational mode and runtime hours.\n- Collect and store this data regularly (e.g., daily or hourly summaries).\n- Periodically analyze the data to assess performance, comparing achieved free cooling hours against design expectations and historical performance.\n- Use the findings to optimize control setpoints, maintenance schedules, or identify potential system enhancements to maximize free cooling utilization.\n- Report on achieved free cooling hours and associated energy savings as part of regular energy performance reviews."
        ],
        "sampleResponseTemplate": "Our [BMS/DCIM System Name] logs the operational mode (full free cooling, partial free cooling, full mechanical) of our [Type of Free Cooling System, e.g., 'waterside economizer'] on an hourly basis. In [Year], we achieved [X hours] of full free cooling and [Y hours] of partial free cooling, which exceeded our design target of [Z total hours]. This data, detailed in our '[Annual Energy Report ID]', allowed us to quantify [kWh/\u20ac] savings of [Amount].",
        "humanInputAnswer": "Our BMS tracks the operational state of our air-side economizer system for Data Hall 3, logging hours in 'Full Economizer', 'Mixed Mode', and 'Full Mechanical Cooling' daily. In 2024, we achieved 5,250 hours of full economization and 1,100 hours of mixed-mode operation. This performance is reviewed quarterly against our climate-based theoretical model (Ref: 'Economizer_Performance_Analysis_Q4_2024.pdf') and used to fine-tune control setpoints. We estimate this saved 1.2 GWh in chiller energy last year.",
        "llmGeneratedAnswer": "Yes, our Building Management System (BMS) is configured to monitor and log the operational hours of our 'Free Cooling' systems. Specifically, for our indirect air-to-air heat exchanger system, the BMS records daily hours of operation in '100% Free Cooling Mode' and 'Partial Economizer Mode' (mixed with mechanical cooling). This data (example in 'FC_Ops_Report_March2025.csv') is reviewed monthly to assess system effectiveness against design targets based on ambient conditions. For instance, in Q1 2025, we achieved 1,850 hours of full free cooling, aligning closely with our climate model's projection. This data is then used to quantify energy savings by comparing against baseline mechanical cooling energy models.",
        "badExampleAnswer": "Our free cooling system runs automatically when it's cold outside. We don't specifically track how many hours it runs, but we know it saves energy."
      },
      "question_152": {
        "id": 152,
        "old_question_text": null,
        "questionText": "Has the organization implemented a system of regular (e.g., monthly, quarterly) written reporting on data centre energy consumption, operational parameters (like PUE, IT utilization if measured), and environmental performance (e.g., renewable energy use, water consumption if significant, carbon emissions)? Are these reports distributed to relevant stakeholders, including senior management, to raise awareness, track progress against targets, and inform decision-making related to energy efficiency and sustainability?",
        "simplifiedQuestion": "Do you have regular written reports on your data centre's energy use, PUE, and environmental performance that go to management and relevant teams?",
        "reference": "2025 CoC Guide 9.3.1",
        "priority": "Medium (Value: 3, Expected: Entire Data Centre)",
        "stepSection": "step_17",
        "recommendationTitle": "Written Reporting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "pptx"
          ],
          "description": "Examples of regular written reports on data centre energy consumption, PUE, and other environmental performance metrics. Distribution lists or evidence of communication to stakeholders. Meeting minutes where these reports are discussed."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, pptx",
        "documentDescriptor": "Mandatory: Examples of regular (e.g., monthly, quarterly) written reports detailing data centre energy consumption, operational parameters (PUE), and environmental performance. Evidence of distribution to relevant stakeholders.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of sample energy and environmental performance reports, their content, frequency, and distribution lists. Interviews with management and operations staff regarding the use and impact of these reports.",
        "nonconformity": {
          "major": "No regular written reporting on data centre energy consumption or key environmental performance indicators is conducted or distributed to relevant stakeholders.",
          "minor": "Written reports are produced sporadically, lack key information (e.g., PUE trends, environmental metrics), are not distributed effectively, or are not used to inform decision-making."
        },
        "majorNonconformity": "Absence of a system for regular written reporting of data centre energy consumption, operational parameters (like PUE), and environmental performance to relevant stakeholders, including management.",
        "minorNonconformity": "Written reports are generated but are infrequent, lack comprehensive data (e.g., only energy use without PUE or environmental metrics), are not consistently distributed, or there is no evidence that the reports are reviewed or acted upon by stakeholders.",
        "whatThisMeans": "Your organization needs to create regular written reports (e.g., monthly or quarterly) that summarize your data centre's energy use, key operational numbers (like PUE, and IT equipment usage if you measure it), and environmental performance (like use of renewable energy, water consumption if it's significant for cooling, and carbon emissions). These reports should be shared with important people in your organization, including senior managers, to keep them informed, track how you're doing against any targets, and help them make decisions about energy efficiency and sustainability.",
        "complianceRequires": "- Implement a system of regular written reporting on data centre energy consumption and operational parameters.\n- Include key metrics such as Power Usage Effectiveness (PUE) and IT utilization (if measured).\n- Include environmental performance indicators such as renewable energy use, water consumption (especially if significant for cooling), and carbon emissions.\n- Define the frequency of these reports (e.g., monthly, quarterly).\n- Ensure reports are distributed to all relevant stakeholders, including senior management.\n- The purpose of these reports is to raise awareness, track progress against defined targets, and inform energy efficiency and sustainability-related decision-making.",
        "expertTip": "Design reports to be clear, concise, and visually engaging, using charts and dashboards to highlight key trends, performance against targets, and areas needing attention. Tailor the level of detail and focus of the reports to different stakeholder groups (e.g., executive summaries for senior management, detailed operational metrics for facilities and IT teams).",
        "recommendedEvidence": [
          "- Examples of regular (e.g., monthly, quarterly) written reports covering data centre energy consumption, PUE, IT utilization (if available), renewable energy fraction, water usage (if applicable), and carbon emissions.\n- Standard report templates or outlines.\n- Distribution lists or communication records showing who receives these reports.\n- Minutes from management meetings or operational reviews where these reports were presented and discussed."
        ],
        "recommendedActions": [
          "- Establish a schedule and clear responsibilities for producing regular written reports on data centre energy and environmental performance.\n- Define the key metrics and content to be included in these reports, ensuring they align with organizational goals and stakeholder needs.\n- Implement a process for distributing these reports to relevant parties, including senior management.\n- Create a feedback loop to ensure that the information in the reports is understood, acted upon, and contributes to continuous improvement in energy efficiency and sustainability."
        ],
        "sampleResponseTemplate": "We produce a '[Monthly/Quarterly] Data Centre Sustainability & Efficiency Report' ([Report Template ID]) which details energy consumption (kWh), PUE (calculated [daily/monthly]), [IT utilization %], [renewable energy %], [water consumption if applicable], and [carbon emissions]. This report is distributed to [e.g., DC Operations team, Head of IT, Sustainability Committee, CFO] by the [5th working day of the following month/quarter]. The latest report for [Period] is attached. Action items from these reports are tracked in [System/Meeting].",
        "humanInputAnswer": "Our 'Monthly Data Centre Performance Review' report includes sections on energy consumption (total kWh, IT kWh), calculated PUE (Category 2), server CPU utilization averages, renewable energy fraction from our PPA, and WUE for our evaporative cooling systems. This report is distributed to the VP of Infrastructure and the Sustainability Steering Committee by the 10th of each month. The Q1 2025 summary report (Ref: 'DCPR_Q1_2025.pdf') highlights a 5% improvement in PUE year-over-year.",
        "llmGeneratedAnswer": "Yes, our organization has implemented a 'Quarterly Data Centre Energy & Environmental Performance Report'. This written report, compiled by the Data Centre Operations team, includes detailed analysis of:\n- Total energy consumption (kWh) (Practice 9.1.1).\n- IT equipment energy consumption (kWh) (Practice 9.1.2).\n- Calculated Power Usage Effectiveness (PUE) (Practice 9.3.5).\n- Water Usage Effectiveness (WUE) for facilities with significant water use (Practice 9.3.12).\n- Percentage of renewable energy utilized (Practice 9.3.6).\n- Achieved free cooling hours (Practice 9.2.4).\nThese reports are distributed to senior IT and Facilities management, as well as our Corporate Sustainability Officer, within 15 days of quarter end. They serve to track performance against targets (e.g., PUE target of 1.4) and inform strategic decisions. An example is 'Q1_2025_DC_Performance_Report.pdf'.",
        "badExampleAnswer": "We collect a lot of data in our BMS, and managers can look at it if they want to. We don't usually write separate reports."
      },
      "question_153": {
        "id": 153,
        "old_question_text": null,
        "questionText": "Has the organization implemented or considered an energy and environmental reporting console or dashboard that provides authorized stakeholders with real-time or near real-time access to key performance indicators (KPIs) related to data centre energy use and environmental performance? Does this console allow for visualization of trends, comparison against targets, and potentially drill-down into more granular data for analysis?",
        "simplifiedQuestion": "Do you have a dashboard or console giving stakeholders real-time (or near real-time) access to key data centre energy/environmental KPIs and trends?",
        "reference": "2025 CoC Guide 9.3.2",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_17",
        "recommendationTitle": "Energy and environmental reporting console",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "screenshots",
            "url"
          ],
          "description": "Screenshots or descriptions of the energy/environmental reporting console or dashboard. List of KPIs displayed. Information on data sources and update frequency. Examples of how stakeholders use the console."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, jpg, screenshots, url",
        "documentDescriptor": "Optional: If implemented: Screenshots or access to the energy and environmental reporting console/dashboard. Documentation detailing KPIs displayed, data refresh rates, and user access controls.",
        "verification_method": [
          "System Demonstration",
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Demonstration of the reporting console/dashboard. Review of system documentation, KPI definitions, and user guides. Interviews with stakeholders who use the console.",
        "nonconformity": {
          "major": null,
          "minor": "No real-time or near real-time reporting console/dashboard for energy and environmental KPIs is available, or if one exists, it is not effectively utilized by stakeholders or lacks key functionality (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: An energy and environmental reporting console/dashboard is not implemented despite having suitable data sources, or an existing console provides limited KPIs, infrequent updates, or poor usability, hindering its value to stakeholders.",
        "whatThisMeans": "Beyond static written reports, your organization should think about providing a dynamic way for important people to see how your data centre is doing in terms of energy use and environmental performance. This could be an online dashboard or console that shows key numbers (KPIs) in real-time or very frequently. This allows stakeholders to easily see trends, check progress against goals, and maybe even dig into more detailed data if they need to.",
        "complianceRequires": "- Consider or implement an energy and environmental reporting console or dashboard.\n- This console should provide authorized stakeholders with real-time or near real-time access to Key Performance Indicators (KPIs) related to data centre energy use and environmental performance.\n- The console should ideally allow for:\n    - Visualization of historical trends for KPIs.\n    - Comparison of actual performance against defined targets or benchmarks.\n    - Potential drill-down capabilities into more granular data for further analysis.",
        "expertTip": "Select a dashboarding tool that can integrate data from multiple sources (BMS, DCIM, IT monitoring, utility meters). Focus on presenting a few critical KPIs clearly for executive stakeholders, with options to drill down for operational teams. Ensure data accuracy and timely refresh rates to maintain user confidence.",
        "recommendedEvidence": [
          "- Screenshots or a live demonstration of the energy and environmental reporting console/dashboard.\n- A list of the KPIs displayed on the console (e.g., real-time PUE, total power draw, cooling load, renewable energy fraction, server utilization).\n- Information on the data sources feeding the console and the data refresh frequency.\n- Examples of how different stakeholders (e.g., DC managers, executives, sustainability team) use the console to monitor performance and make decisions."
        ],
        "recommendedActions": [
          "- Evaluate the need and potential benefits of an energy and environmental reporting console for your key stakeholders.\n- Identify the critical KPIs that should be displayed.\n- Select or develop a dashboarding solution that can integrate with your existing data collection systems (BMS, DCIM, EMS, etc.).\n- Design intuitive visualizations and user interfaces.\n- Provide training and access to relevant stakeholders."
        ],
        "sampleResponseTemplate": "We have implemented an [Energy Management Dashboard using PowerBI / DCIM-integrated console ([DCIM Vendor])] which provides [near real-time (e.g., 15-minute updates)] visibility of KPIs including [PUE, total kW, cooling kW, IT kW, renewable energy %]. Authorized stakeholders ([List roles]) can access it via [URL/System]. It allows for [trend visualization over X period / drill-down to Y level]. Screenshots are attached.",
        "humanInputAnswer": "Our Schneider EcoStruxure IT Expert platform serves as our primary energy and environmental reporting console. It provides near real-time dashboards (updated every 5 minutes) showing live PUE, total facility power, IT load, cooling system power, server inlet temperature heatmaps, and our current renewable energy import percentage. Managers and operations staff have role-based access. The console allows for historical trend analysis up to 2 years and drill-down to individual sub-meters and sensors.",
        "llmGeneratedAnswer": "Yes, our organization utilizes a centralized 'Data Centre Sustainability Dashboard', accessible to authorized stakeholders via our corporate intranet. This dashboard, powered by [BI Tool Name, e.g., Tableau] and refreshed every hour, displays key performance indicators (KPIs) such as real-time PUE, total energy consumption (rolling 24hr, MTD, YTD), IT load trends, cooling system efficiency metrics, water usage effectiveness (WUE), and the current percentage of renewable energy contributing to our supply. Users can visualize trends over selectable time periods, compare against set targets (e.g., PUE < 1.3), and have limited drill-down capability to view zonal energy consumption. This complements our quarterly written reports by providing more immediate operational insights.",
        "badExampleAnswer": "We have a lot of data in different systems. If someone needs a specific number, they can ask the facilities team to try and find it."
      },
      "question_154": {
        "id": 154,
        "old_question_text": null,
        "questionText": "Has the organization implemented or considered an integrated IT and M&E (Mechanical & Electrical) energy and environmental reporting console or dashboard? Does this system combine data from both IT systems (e.g., server utilization, application performance, virtual machine density) and M&E infrastructure (e.g., power distribution, cooling system performance, environmental conditions) to provide a holistic view of data centre efficiency and performance, enabling better correlation analysis and decision-making?",
        "simplifiedQuestion": "Do you have a dashboard that combines IT data (like server use, app performance) with M&E data (power, cooling, environment) for a complete view of DC efficiency?",
        "reference": "2025 CoC Guide 9.3.3",
        "priority": "Low (Value: 1, Expected: Optional Practice)",
        "stepSection": "step_17",
        "recommendationTitle": "Integrated IT / M&E energy and environmental reporting console",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "screenshots",
            "url"
          ],
          "description": "Screenshots or descriptions of the integrated IT/M&E reporting console. List of integrated IT and M&E KPIs. Examples of how combined data is used for correlation analysis or holistic efficiency optimization."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, jpg, screenshots, url",
        "documentDescriptor": "Optional: If implemented: Screenshots or access to an integrated IT and M&E reporting console. Documentation of data sources, integrated KPIs, and examples of how this holistic view supports efficiency analysis and decision-making.",
        "verification_method": [
          "System Demonstration",
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Demonstration of the integrated console. Review of system architecture, KPI definitions, and use cases. Interviews with IT and facilities staff who use the integrated data.",
        "nonconformity": {
          "major": null,
          "minor": "IT and M&E data are collected and reported separately without integration, or if an integrated console exists, its capabilities for correlation analysis and holistic decision-making are not effectively utilized (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: While separate IT and M&E monitoring may exist, there is no integrated reporting console or dashboard that combines data from both domains to provide a holistic view of data centre efficiency. Correlation analysis between IT workload and M&E energy consumption is manual or not performed.",
        "whatThisMeans": "Building on the idea of a reporting dashboard (Practice 9.3.2), this practice suggests going a step further by creating a console that brings together data from *both* your IT systems and your Mechanical & Electrical (M&E) infrastructure. This means combining information like server usage and application performance with data on power distribution, cooling system efficiency, and environmental conditions. Having this integrated view helps you see the bigger picture of your data centre's efficiency, understand how IT and M&E affect each other, and make better decisions.",
        "complianceRequires": "- Consider or implement an integrated IT and M&E (Mechanical & Electrical) energy and environmental reporting console or dashboard.\n- This system should combine data from both IT systems (e.g., server CPU utilization, memory usage, network traffic, application performance metrics, virtual machine density, workload levels) and M&E infrastructure (e.g., power distribution efficiency, cooling system performance, PUE, CUE, WUE, environmental conditions).\n- The goal is to provide a holistic view of overall data centre efficiency and performance.\n- Such integration enables better correlation analysis between IT demand and infrastructure response, supporting more informed decision-making for optimization.",
        "expertTip": "Leverage DCIM (Data Centre Infrastructure Management) platforms that are designed to integrate IT and M&E data. Alternatively, use data warehousing and business intelligence (BI) tools to combine data from disparate IT monitoring systems (e.g., APM, virtualization management) and facilities monitoring systems (BMS, EPMS). Focus on visualizing key correlations, like IT load vs. cooling energy, or VM density vs. host power draw.",
        "recommendedEvidence": [
          "- Screenshots or a live demonstration of the integrated IT/M&E reporting console.\n- A list of specific IT metrics and M&E metrics that are combined and displayed on the console.\n- Examples of dashboards or reports generated from the integrated system that show correlations between IT workload/performance and M&E energy consumption or environmental conditions.\n- Case studies or examples of how this integrated view has led to specific efficiency improvements or better operational decisions."
        ],
        "recommendedActions": [
          "- Identify key IT metrics (e.g., server utilization, workload levels) and M&E metrics (e.g., PUE, cooling energy, power distribution losses) that would provide valuable insights when correlated.\n- Evaluate existing monitoring platforms (DCIM, BMS, IT monitoring tools) for their ability to integrate data from both domains, or consider data aggregation solutions.\n- Develop dashboards and reports that visualize these integrated KPIs and correlations.\n- Use these insights to drive collaborative efficiency initiatives between IT and facilities teams."
        ],
        "sampleResponseTemplate": "We have implemented an integrated reporting console using [DCIM Tool Name / BI Platform Name] that combines IT workload data (from [VMware vCenter/Nagios]) with M&E energy and environmental data (from our BMS). This allows us to correlate [e.g., 'CPU utilization with CRAH fan speeds', 'VM density per host with rack power consumption']. An example dashboard showing [Specific Correlation] is attached. This helps us identify [e.g., 'opportunities for server consolidation based on both utilization and power draw'].",
        "humanInputAnswer": "Our 'Data Centre Performance Portal' (built on Grafana with InfluxDB backend) integrates IT metrics (from SolarWinds and vCenter - CPU/mem util, VM counts) with M&E data from our BMS (Schneider PME/EBO - power, PUE, cooling temps). This allows us to, for example, correlate average VM CPU utilization per physical host with the host's actual power draw and the cooling load of its respective aisle. This integrated view is used for capacity planning and identifying efficiency anomalies. (Screenshot: 'DC_PerfPortal_Overview.jpg')",
        "llmGeneratedAnswer": "Yes, our organization utilizes an integrated IT and Mechanical & Electrical (M&E) energy and environmental reporting console, built upon our [Specify Platform, e.g., 'Enterprise DCIM Suite']. This platform (Ref: 'DCIM_Integration_Architecture_v1.2.pdf') combines data from IT systems (e.g., server CPU/memory utilization from VMware vRealize, application transaction volumes from Dynatrace) with M&E infrastructure data (e.g., power consumption from intelligent PDUs and sub-meters, cooling system performance from the BMS, live PUE calculations). This provides a holistic view, enabling, for instance, real-time correlation of specific application workloads with the energy draw of supporting IT hardware and the corresponding response from the cooling infrastructure. This integrated data is critical for our continuous efficiency optimization programs.",
        "badExampleAnswer": "The IT team has their monitoring tools, and the facilities team has theirs for the building systems. They share data sometimes if there's a problem."
      },
      "question_155": {
        "id": 155,
        "old_question_text": null,
        "questionText": "Has the organization implemented reporting of Power Usage Effectiveness (PUE) and partial PUE (pPUE) values, calculated in accordance with recognized standards such as ISO/IEC 30134-2 (PUE) and ISO/IEC 30134-7 (pPUE for cooling)? Is this reporting performed at an appropriate PUE category level (1, 2, or 3) based on the available metering (as per Practices 9.1.1, 9.1.2, 9.1.5)? Is it understood that PUE reporting is required for EU Commission EED (Energy Efficiency Directive) reporting?",
        "simplifiedQuestion": "Do you report PUE and partial PUE (like for cooling) using standard methods (e.g., ISO/IEC 30134-2, -7), especially for EED reporting?",
        "reference": "2025 CoC Guide 9.3.4",
        "priority": "High (Value: 5, Expected: Entire Data Centre, NOTE: Required for EU Commission EED reporting)",
        "stepSection": "step_17",
        "recommendationTitle": "PUE and Partial PUE Reporting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "PUE calculation methodology aligned with ISO/IEC 30134-2. pPUE calculation methodology (if used) aligned with ISO/IEC 30134-7. Reports showing calculated PUE and pPUE values and PUE category. Evidence of EED reporting including PUE."
        },
        "docRequired": "Yes",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory: Documented methodology for PUE calculation (ISO/IEC 30134-2) and pPUE (ISO/IEC 30134-7 if used). Reports showing PUE values, PUE category, and underlying data. Evidence of EED reporting if applicable.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of PUE/pPUE calculation procedures, source data from meters (9.1.1, 9.1.2, 9.1.5), and reported PUE values and category. Verification of consistency with ISO/IEC standards and EED reporting requirements.",
        "nonconformity": {
          "major": "PUE is not calculated or reported, or calculations do not follow ISO/IEC 30134-2, especially if required for EED reporting. No pPUE is reported where sub-metering allows.",
          "minor": "PUE is calculated but not at the highest achievable category based on metering, methodology is not fully documented, or pPUE for key subsystems like cooling is not reported despite available data."
        },
        "majorNonconformity": "Failure to report Power Usage Effectiveness (PUE) calculated in accordance with ISO/IEC 30134-2, particularly if required for EU Commission EED reporting. Or, reported PUE values are based on inaccurate or non-standard methodologies.",
        "minorNonconformity": "PUE is reported, but the PUE category (1, 2, or 3) is not clearly defined or justified based on metering capabilities, or partial PUE (pPUE) for subsystems like cooling (as per ISO/IEC 30134-7) is not reported despite the availability of relevant sub-metering data (from Practice 9.1.5).",
        "whatThisMeans": "Your organization must report its Power Usage Effectiveness (PUE), which is a key measure of data centre energy efficiency. This calculation needs to follow recognized standards like ISO/IEC 30134-2. You should also consider reporting partial PUE (pPUE) for different parts of your infrastructure, like the cooling system (using ISO/IEC 30134-7). The level of detail for your PUE (Category 1, 2, or 3) depends on how and where you measure your energy use (as covered in practices 9.1.1, 9.1.2, and 9.1.5). Reporting PUE is also a requirement for the EU Energy Efficiency Directive (EED).",
        "complianceRequires": "- Implement reporting of Power Usage Effectiveness (PUE) values.\n- Calculate PUE in accordance with recognized standards, primarily ISO/IEC 30134-2 ('Data centres - Key performance indicators - Part 2: Power usage effectiveness (PUE)').\n- Where sub-metering allows (Practice 9.1.5), consider reporting partial PUE (pPUE) values, for example, for the cooling subsystem, calculated in accordance with ISO/IEC 30134-7 ('Data centres - Key performance indicators - Part 7: Cooling Efficiency Ratio (CER)' which is related to pPUEcooling).\n- Report PUE at an appropriate PUE category level (Category 1, 2, or 3) based on the granularity and location of energy metering for total facility energy (Practice 9.1.1) and IT equipment energy (Practice 9.1.2).\n- Understand that PUE reporting is explicitly required for EU Commission EED (Energy Efficiency Directive) reporting.",
        "expertTip": "Aim for PUE Category 2 (metered values for both total facility energy and IT equipment energy, logged at least hourly) as a good balance between accuracy and implementation complexity. Regularly audit your PUE calculation methodology and data inputs to ensure consistency and accuracy. Use pPUE for cooling to specifically track and optimize the efficiency of your cooling systems.",
        "recommendedEvidence": [
          "- Documented procedure for calculating PUE, referencing ISO/IEC 30134-2 and specifying the PUE Category achieved.\n- Spreadsheets or system reports showing the calculation of PUE values, including raw energy data from total facility meters and IT load meters.\n- If pPUE is reported, similar documentation for its calculation (e.g., pPUEcooling based on cooling energy data from sub-meters, referencing ISO/IEC 30134-7).\n- Examples of internal or external reports where PUE and pPUE values are presented.\n- If applicable, copies of submissions for EU EED reporting that include PUE data."
        ],
        "recommendedActions": [
          "- Establish a robust methodology for calculating PUE in strict accordance with ISO/IEC 30134-2, ensuring clear definition of measurement points and PUE category.\n- Implement automated collection of total facility energy and IT equipment energy data at appropriate intervals (at least hourly for PUE Cat 2).\n- If sub-metering for M&E is in place (Practice 9.1.5), develop procedures to calculate and report relevant pPUE values (e.g., for cooling, power distribution).\n- Regularly calculate, track, and report PUE and pPUE to stakeholders.\n- Ensure PUE reporting meets the requirements of the EU Energy Efficiency Directive."
        ],
        "sampleResponseTemplate": "We calculate and report PUE monthly at Category [2/1] in accordance with ISO/IEC 30134-2. Our total facility energy is from [Meter ID for 9.1.1] and IT equipment energy is from [Meter ID(s) for 9.1.2]. Our average PUE for [Last Period] was [Value]. We also report pPUE for cooling (pPUE_C) using data from our cooling sub-meters ([Meter IDs for 9.1.5]), which was [Value]. This data is included in our EED reporting (Ref: '[EED Report 202X]').",
        "humanInputAnswer": "Our PUE is calculated monthly at Category 2, following ISO/IEC 30134-2. Total facility energy is from utility meter M-001, and IT equipment energy is aggregated from all metered rack PDUs (as per 9.1.2). For April 2025, our PUE was 1.38. We also calculate partial PUE for our cooling systems (pPUE_cool) using sub-metered data from chiller plants and CRAH distribution boards (per 9.1.5), which was 1.15 for April. These metrics are reported to management and form part of our EED submission.",
        "llmGeneratedAnswer": "Yes, our organization reports Power Usage Effectiveness (PUE) in strict accordance with ISO/IEC 30134-2. We currently achieve PUE Category 2, with total data centre energy consumption (Practice 9.1.1) and IT equipment energy consumption (Practice 9.1.2) metered and logged at 15-minute intervals. Monthly PUE reports are generated and distributed internally (Ref: 'Monthly PUE Performance Report Template'). For example, the average PUE for Q1 2025 was 1.42. Additionally, using data from our M&E sub-metering (Practice 9.1.5), we calculate and report partial PUE for our cooling subsystem (pPUEcooling) as per ISO/IEC 30134-7. This PUE data is a mandatory component of our annual EU Energy Efficiency Directive (EED) reporting.",
        "badExampleAnswer": "We calculate PUE sometimes by taking the total electricity bill and dividing it by an estimate of what our servers use. It's usually around 2.0."
      },
      "question_156": {
        "id": 156,
        "old_question_text": null,
        "questionText": "Has the organization implemented reporting of the Renewable Energy Factor (REF) in accordance with ISO/IEC 30134-3? Is this done to quantify the proportion of data centre energy consumption sourced from renewable energy, and is it understood that REF reporting is required for EU Commission EED (Energy Efficiency Directive) reporting?",
        "simplifiedQuestion": "Do you report your Renewable Energy Factor (REF) using ISO/IEC 30134-3 to show renewable energy use, especially for EED reporting?",
        "reference": "2025 CoC Guide 9.3.5",
        "priority": "Low (Value: 2, Expected: Entire Data Centre, NOTE: Required for EU Commission EED reporting)",
        "stepSection": "step_17",
        "recommendationTitle": "REF Reporting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Methodology for calculating Renewable Energy Factor (REF) aligned with ISO/IEC 30134-3. Data inputs for REF calculation (total energy, renewable energy with Guarantees of Origin, on-site renewable generation/consumption). Reports showing calculated REF values. Evidence of EED reporting including REF."
        },
        "docRequired": "Yes, if EED reporting applicable or REF claimed",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory (if EED reporting applicable or REF claimed): Documented methodology for REF calculation (ISO/IEC 30134-3). Supporting data (e.g., GOs, on-site generation logs). Reports including REF values. Evidence of EED reporting.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of REF calculation procedures, source data (renewable energy certificates, on-site generation records), and reported REF values. Verification of consistency with ISO/IEC standard and EED reporting requirements if applicable.",
        "nonconformity": {
          "major": "Renewable energy is used but REF is not calculated or reported according to ISO/IEC 30134-3, especially if required for EED reporting.",
          "minor": "REF is calculated but methodology deviates from ISO/IEC 30134-3, or source data (e.g., GOs) is not adequately managed or documented."
        },
        "majorNonconformity": "Failure to report the Renewable Energy Factor (REF) calculated in accordance with ISO/IEC 30134-3, particularly if required for EU Commission EED reporting, despite sourcing renewable energy.",
        "minorNonconformity": "REF is reported, but the calculation methodology is not fully aligned with ISO/IEC 30134-3 (e.g., incorrect treatment of on-site generation or Guarantees of Origin), or supporting documentation for renewable energy claims is insufficient.",
        "whatThisMeans": "If your data centre uses renewable energy, your organization needs to report this using a standard metric called the Renewable Energy Factor (REF), calculated according to ISO/IEC 30134-3. This shows what proportion of your total data centre energy comes from renewable sources. Reporting REF is also a requirement for the EU Energy Efficiency Directive (EED).",
        "complianceRequires": "- Implement reporting of the Renewable Energy Factor (REF).\n- Calculate REF in accordance with ISO/IEC 30134-3 ('Data centres - Key performance indicators - Part 3: Renewable energy factor (REF)').\n- This metric quantifies the proportion of the data centre's energy consumption that is sourced from renewable energy.\n- Understand that REF reporting is explicitly noted as being required for EU Commission EED (Energy Efficiency Directive) reporting.",
        "expertTip": "Ensure accurate tracking of all renewable energy sources, including on-site generation and purchased renewable energy (with accompanying Guarantees of Origin or equivalent contractual instruments). Clearly differentiate between renewable energy consumed and any renewable energy generated on-site but exported to the grid, as per the ISO/IEC 30134-3 methodology.",
        "recommendedEvidence": [
          "- Documented procedure for calculating REF, referencing ISO/IEC 30134-3.\n- Data inputs for REF calculation: total data centre energy consumption, kWh of renewable energy from Guarantees of Origin (GOs) or other certificates, kWh of on-site renewable generation consumed by the data centre, kWh of on-site renewable generation exported.\n- Copies of GOs or other renewable energy procurement contracts/certificates.\n- Examples of internal or external reports where REF values are presented.\n- If applicable, copies of submissions for EU EED reporting that include REF data."
        ],
        "recommendedActions": [
          "- Establish a robust methodology for calculating REF in strict accordance with ISO/IEC 30134-3.\n- Implement systems for accurately tracking all sources of renewable energy contributing to the data centre's consumption.\n- Ensure proper documentation and retirement of Guarantees of Origin or other renewable energy certificates.\n- Regularly calculate, track, and report REF to stakeholders and for regulatory compliance (EED).\n- Ensure REF reporting meets the requirements of the EU Energy Efficiency Directive."
        ],
        "sampleResponseTemplate": "We calculate and report our Renewable Energy Factor (REF) [annually/quarterly] in accordance with ISO/IEC 30134-3. Our REF for [Reporting Period] was [Value], based on [details of renewable sources, e.g., X kWh from purchased wind energy with GOs, Y kWh from on-site solar]. This data is included in our EED reporting (Ref: '[EED Report 202X]'). Methodology documented in '[REF Calc Proc ID]'.",
        "humanInputAnswer": "Our REF is calculated annually as per ISO/IEC 30134-3. For 2024, our REF was 0.45, derived from our PPA for wind energy (supported by Guarantees of Origin) and on-site solar generation consumed directly by the facility. All calculations and supporting GOs are compiled for our EED submission and detailed in our 'Annual Sustainability Report 2024'.",
        "llmGeneratedAnswer": "Yes, our organization reports the Renewable Energy Factor (REF) according to the ISO/IEC 30134-3 standard. This metric is calculated annually to quantify the proportion of our data centre's total energy consumption derived from renewable sources, including procured renewable energy with Guarantees of Origin and on-site solar generation. For the calendar year 2024, our REF was calculated at 0.62. This REF value is a key component of our environmental performance reporting and is included in our mandatory submissions for the EU Energy Efficiency Directive (EED), as outlined in our 'EED Compliance Reporting Procedure (ECRP-2025)'.",
        "badExampleAnswer": "We buy some green energy tariffs, so we consider our data centre to be partly renewably powered. We don't do a specific REF calculation."
      }
    }
  },
  "step_18": {
    "description": "2025 Guide Section 9: Monitoring (Practices 9.3.6-9.3.12 from Sec 9.3 'Performance Reporting'; and 9.4.1-9.4.2 from Sec 9.4 'IT Reporting')",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Monitoring_3"
    ],
    "critical_questions": [],
    "questions": {
      "question_157": {
        "id": 157,
        "old_question_text": null,
        "questionText": "Has the organization implemented reporting of IT Equipment Energy Efficiency for servers (ITEEsv) in accordance with ISO/IEC 30134-4 (previously ISO/IEC 21836)? Is this done to understand and report on the energy efficiency of the server fleet, and is it understood that ITEEsv reporting is required for EU Commission EED (Energy Efficiency Directive) reporting?",
        "simplifiedQuestion": "Do you report server energy efficiency (ITEEsv) using ISO/IEC 30134-4 (formerly ISO/IEC 21836), especially for EED reporting?",
        "reference": "2025 CoC Guide 9.3.6",
        "priority": "Low (Value: 1, Expected: Entire Data Centre, NOTE: Required for EU Commission EED reporting)",
        "stepSection": "step_18",
        "recommendationTitle": "ITEESV Reporting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Methodology for calculating ITEEsv aligned with ISO/IEC 30134-4. Data inputs for ITEEsv (server inventory, utilization, power). Reports showing calculated ITEEsv values. Evidence of EED reporting including ITEEsv."
        },
        "docRequired": "Yes, if EED reporting applicable or ITEEsv claimed",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory (if EED reporting applicable or ITEEsv claimed): Documented methodology for ITEEsv calculation (ISO/IEC 30134-4). Server inventory data and inputs used. Reports including ITEEsv values. Evidence of EED reporting.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of ITEEsv calculation procedures, source data (server specifications, utilization, power measurements), and reported ITEEsv values. Verification of consistency with ISO/IEC standard and EED reporting requirements.",
        "nonconformity": {
          "major": "ITEESV is not calculated or reported, especially if required for EED compliance, despite having server infrastructure.",
          "minor": "ITEESV is calculated but methodology deviates significantly from ISO/IEC 30134-4, or source data is inadequate or inaccurate."
        },
        "majorNonconformity": "Failure to report IT Equipment Energy Efficiency for servers (ITEEsv) calculated in accordance with ISO/IEC 30134-4 (formerly ISO/IEC 21836), particularly if required for EU Commission EED reporting.",
        "minorNonconformity": "ITEESV is reported, but the calculation methodology or data inputs (e.g., server inventory, performance, power consumption) are not fully aligned with the ISO/IEC 30134-4 standard, or the scope of reporting is incomplete.",
        "whatThisMeans": "Your organization should measure and report on the energy efficiency of your servers using a standard metric called ITEEsv (IT Equipment Energy Efficiency for servers), which is defined in ISO/IEC 30134-4 (this standard used to be ISO/IEC 21836). This helps you understand how efficient your server fleet is. Reporting ITEEsv is also a requirement for the EU Energy Efficiency Directive (EED).",
        "complianceRequires": "- Implement reporting of IT Equipment Energy Efficiency for servers (ITEEsv).\n- Calculate ITEEsv in accordance with ISO/IEC 30134-4 ('Data centres - Key performance indicators - Part 4: IT Equipment Energy Efficiency for servers (ITEEsv)'), which superseded ISO/IEC 21836.\n- This metric helps understand and report on the energy efficiency of the installed server fleet.\n- Understand that ITEEsv reporting is explicitly noted as being required for EU Commission EED (Energy Efficiency Directive) reporting.",
        "expertTip": "Calculating ITEEsv requires detailed inventory data for your servers (model, CPU, memory, storage configuration), performance benchmark data (or suitable proxies), and power consumption data at various load levels. Utilize server vendor data, tools like SERT, or direct measurements where possible. Start with a representative sample of your server fleet if a full inventory analysis is initially too complex.",
        "recommendedEvidence": [
          "- Documented procedure for calculating ITEEsv, referencing ISO/IEC 30134-4.\n- Data inputs for ITEEsv calculation: server inventory details (make, model, configuration), server performance data (e.g., from benchmarks or standardized tests), and corresponding power consumption data at defined load points.\n- Examples of ITEEsv calculations and reported values for server groups or the entire fleet.\n- If applicable, copies of submissions for EU EED reporting that include ITEEsv data."
        ],
        "recommendedActions": [
          "- Establish a robust methodology for calculating ITEEsv in accordance with ISO/IEC 30134-4.\n- Implement processes for collecting the necessary server inventory, performance, and power consumption data.\n- Regularly calculate, track, and report ITEEsv to understand server fleet efficiency trends.\n- Use ITEEsv data to inform server procurement decisions (Practice 4.1.1) and refresh strategies.\n- Ensure ITEEsv reporting meets the requirements of the EU Energy Efficiency Directive."
        ],
        "sampleResponseTemplate": "We calculate and report IT Equipment Energy Efficiency for servers (ITEEsv) [annually/biannually] in accordance with ISO/IEC 30134-4. Our average ITEEsv for our primary server fleet in [Reporting Period] was [Value]. This is based on [data sources for server specs, performance, power]. This data is included in our EED reporting (Ref: '[EED Report 202X]'). Methodology is documented in '[ITEESV Calc Proc ID]'.",
        "humanInputAnswer": "We report ITEEsv annually for our two main server populations (Virtualization Hosts and HPC Cluster) as per ISO/IEC 30134-4. For 2024, the ITEEsv for our VMware hosts was 15.2 kSOPS/W (kilo Standard Operations Per Second per Watt), and for the HPC cluster, it was 2.5 GFLOPS/W. Data inputs include SERT results for representative configurations and measured power. This is part of our EED submission (Ref: 'EED_Data_Submission_2024.xlsx').",
        "llmGeneratedAnswer": "Yes, our organization reports IT Equipment Energy Efficiency for servers (ITEEsv) following the ISO/IEC 30134-4 standard. This KPI is calculated annually for our primary server fleet categories. The calculation methodology, documented in 'Server_Efficiency_Metrics_Protocol_SEMP-2025.pdf', utilizes server inventory data from our CMDB, standardized performance benchmarks (derived from SERT where applicable or equivalent workload-specific tests), and corresponding power consumption measurements. For the 2024 reporting period, the aggregated ITEEsv for our enterprise server fleet was [Calculated Value]. This metric is included in our mandatory reporting for the EU Energy Efficiency Directive (EED).",
        "badExampleAnswer": "We know our servers are pretty new and efficient because we buy from major vendors. We don't calculate a specific ITEEsv number."
      },
      "question_158": {
        "id": 158,
        "old_question_text": null,
        "questionText": "Has the organization implemented reporting of IT Equipment Utilisation for servers (ITEUsv) in accordance with ISO/IEC 30134-5? Is this done to understand and report on the utilization levels of the server fleet, and is it understood that ITEUsv reporting is required for EU Commission EED (Energy Efficiency Directive) reporting?",
        "simplifiedQuestion": "Do you report server utilization (ITEUsv) using ISO/IEC 30134-5, especially for EED reporting?",
        "reference": "2025 CoC Guide 9.3.7",
        "priority": "Low (Value: 1, Expected: Entire Data Centre, NOTE: Required for EU Commission EED reporting)",
        "stepSection": "step_18",
        "recommendationTitle": "ITEUSV Reporting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Methodology for calculating ITEUsv aligned with ISO/IEC 30134-5. Data inputs for ITEUsv (server inventory, utilization data from monitoring tools). Reports showing calculated ITEUsv values. Evidence of EED reporting including ITEUsv."
        },
        "docRequired": "Yes, if EED reporting applicable or ITEUsv claimed",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory (if EED reporting applicable or ITEUsv claimed): Documented methodology for ITEUsv calculation (ISO/IEC 30134-5). Server inventory and utilization data inputs. Reports including ITEUsv values. Evidence of EED reporting.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of ITEUsv calculation procedures, source data (server utilization monitoring), and reported ITEUsv values. Verification of consistency with ISO/IEC standard and EED reporting requirements.",
        "nonconformity": {
          "major": "ITEUSV is not calculated or reported, especially if required for EED compliance, despite having server infrastructure and utilization monitoring capabilities.",
          "minor": "ITEUSV is calculated but methodology deviates from ISO/IEC 30134-5, or source data (server utilization) is inadequate, inaccurate, or not representative."
        },
        "majorNonconformity": "Failure to report IT Equipment Utilisation for servers (ITEUsv) calculated in accordance with ISO/IEC 30134-5, particularly if required for EU Commission EED reporting.",
        "minorNonconformity": "ITEUsv is reported, but the calculation methodology or data inputs (e.g., server utilization data, server inventory details) are not fully aligned with the ISO/IEC 30134-5 standard, or the scope of reporting is incomplete.",
        "whatThisMeans": "Your organization should measure and report how much your servers are being used (their utilization) using a standard metric called ITEUsv (IT Equipment Utilisation for servers), which is defined in ISO/IEC 30134-5. This helps you understand how effectively your server fleet is being used. Reporting ITEUsv is also a requirement for the EU Energy Efficiency Directive (EED).",
        "complianceRequires": "- Implement reporting of IT Equipment Utilisation for servers (ITEUsv).\n- Calculate ITEUsv in accordance with ISO/IEC 30134-5 ('Data centres - Key performance indicators - Part 5: IT Equipment Utilisation for servers (ITEUsv)').\n- This metric helps understand and report on the utilization levels of the installed server fleet.\n- Understand that ITEUsv reporting is explicitly noted as being required for EU Commission EED (Energy Efficiency Directive) reporting.",
        "expertTip": "Accurate ITEUsv calculation requires robust server utilization monitoring (e.g., CPU utilization averaged over appropriate time intervals). Ensure your monitoring tools collect this data consistently for the server fleet. ITEUsv can help identify underutilized servers, supporting consolidation efforts (Practice 4.3.4) and decommissioning (Practice 4.3.2, 4.3.6).",
        "recommendedEvidence": [
          "- Documented procedure for calculating ITEUsv, referencing ISO/IEC 30134-5.\n- Data inputs for ITEUsv calculation: server inventory details, server utilization data (e.g., CPU utilization from monitoring systems) collected over defined periods.\n- Examples of ITEUsv calculations and reported values for server groups or the entire fleet.\n- If applicable, copies of submissions for EU EED reporting that include ITEUsv data."
        ],
        "recommendedActions": [
          "- Establish a robust methodology for calculating ITEUsv in accordance with ISO/IEC 30134-5.\n- Implement or enhance server utilization monitoring to collect accurate data required for ITEUsv calculation.\n- Regularly calculate, track, and report ITEUsv to understand server fleet utilization trends.\n- Use ITEUsv data to inform capacity planning, identify candidates for workload consolidation, and drive server decommissioning initiatives.\n- Ensure ITEUsv reporting meets the requirements of the EU Energy Efficiency Directive."
        ],
        "sampleResponseTemplate": "We calculate and report IT Equipment Utilisation for servers (ITEUsv) [annually/biannually] in accordance with ISO/IEC 30134-5. Our average ITEUsv for production servers in [Reporting Period] was [Value based on CPU utilization %]. This is based on data from our [Monitoring System, e.g., vCenter, Nagios]. This data is included in our EED reporting (Ref: '[EED Report 202X]'). Methodology is documented in '[ITEUSV Calc Proc ID]'.",
        "humanInputAnswer": "Our ITEUsv (server utilization) is calculated quarterly based on average CPU utilization data collected from all physical production servers via SolarWinds, as per ISO/IEC 30134-5. For Q1 2025, the fleet-wide average ITEUsv was 48%. This metric is reported internally to track progress against our server consolidation goals and is submitted as part of our EED compliance documentation (Ref: 'EED_Annex_Metrics_2025.xlsx').",
        "llmGeneratedAnswer": "Yes, our organization reports IT Equipment Utilisation for servers (ITEUsv) in alignment with the ISO/IEC 30134-5 standard. This KPI is calculated on a quarterly basis, utilizing average CPU utilization data collected from our server monitoring platform ([Platform Name, e.g., Zabbix]) across our primary server estates. The methodology, detailed in 'Server_Performance_Metrics_Protocol_SPMP-2025.pdf', ensures consistent data collection and calculation. For the last reporting period (Q4 2024), the aggregated ITEUsv for our production server fleet was [Calculated Value, e.g., 0.55 representing 55% average utilization]. This metric is a mandatory component of our reporting for the EU Energy Efficiency Directive (EED).",
        "badExampleAnswer": "Our servers are generally quite busy. We don't calculate a specific utilization number like ITEUsv."
      },
      "question_159": {
        "id": 159,
        "old_question_text": null,
        "questionText": "Where waste heat reuse is implemented (as per Practices 5.7.1, 5.7.2, 5.7.3), has the organization implemented reporting of the Energy Reuse Factor (ERF) in accordance with ISO/IEC 30134-6? Is this done to quantify and transparently communicate the effectiveness of these heat reuse initiatives, and is it understood that ERF reporting is required for EU Commission EED (Energy Efficiency Directive) reporting?",
        "simplifiedQuestion": "If you reuse waste heat, do you report the Energy Reuse Factor (ERF) using ISO/IEC 30134-6, especially for EED reporting?",
        "reference": "2025 CoC Guide 9.3.8",
        "priority": "Low (Value: 1, Expected: Entire Data Centre, NOTE: Required for EU Commission EED reporting)",
        "stepSection": "step_18",
        "recommendationTitle": "ERF Reporting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Methodology for calculating Energy Reuse Factor (ERF) aligned with ISO/IEC 30134-6. Data inputs for ERF (total DC energy, reused heat energy, type of primary energy offset by reuse). Reports showing calculated ERF values. Evidence of EED reporting including ERF."
        },
        "docRequired": "Yes, if heat reuse implemented AND EED reporting applicable/ERF claimed",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory (if heat reuse implemented AND EED reporting applicable/ERF claimed): Documented methodology for ERF calculation (ISO/IEC 30134-6). Supporting data for energy inputs and reused heat. Reports including ERF values. Evidence of EED reporting.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of ERF calculation procedures, source data for total energy and reused heat, and reported ERF values. Verification of consistency with ISO/IEC standard and EED reporting requirements if applicable.",
        "nonconformity": {
          "major": "Waste heat is reused but ERF is not calculated or reported according to ISO/IEC 30134-6, especially if required for EED compliance.",
          "minor": "ERF is calculated but methodology deviates from ISO/IEC 30134-6, or source data for reused heat is not accurately metered or documented."
        },
        "majorNonconformity": "If waste heat reuse is implemented (as per Practices 5.7.1, 5.7.2, 5.7.3) and EED reporting is applicable, failure to report the Energy Reuse Factor (ERF) calculated in accordance with ISO/IEC 30134-6.",
        "minorNonconformity": "ERF is reported, but the calculation methodology or data inputs (e.g., amount of reused heat, primary energy offset) are not fully aligned with the ISO/IEC 30134-6 standard, or the reporting is inconsistent.",
        "whatThisMeans": "If your data centre reuses its waste heat (for example, to heat nearby buildings), your organization needs to report how much energy is being effectively reused using a standard metric called ERF (Energy Reuse Factor). This should be calculated according to the ISO/IEC 30134-6 standard. This helps to quantify and communicate how well your heat reuse efforts are working and is also a requirement for the EU Energy Efficiency Directive (EED).",
        "complianceRequires": "- Where waste heat reuse is implemented (as per Practices 5.7.1, 5.7.2, 5.7.3), implement reporting of the Energy Reuse Factor (ERF).\n- Calculate ERF in accordance with ISO/IEC 30134-6 ('Data centres - Key performance indicators - Part 6: Energy Reuse Factor (ERF)').\n- This metric quantifies the effectiveness of waste heat reuse initiatives by comparing the amount of reused energy to the total energy consumed by the data centre.\n- Understand that ERF reporting is explicitly noted as being required for EU Commission EED (Energy Efficiency Directive) reporting.",
        "expertTip": "Ensure accurate and verifiable metering of the reused heat energy delivered to the recipient (e.g., using heat meters on the hot water/air loop). Clearly document the type of primary energy being offset by the reused heat (e.g., natural gas, electricity from grid) and its conversion factor to correctly calculate ERF as per the standard.",
        "recommendedEvidence": [
          "- Documented procedure for calculating ERF, referencing ISO/IEC 30134-6.\n- Data inputs for ERF calculation: total data centre energy consumption (from Practice 9.1.1), metered quantity of reused heat energy, and documentation of the primary energy source offset by the reused heat.\n- Examples of ERF calculations and reported values.\n- If applicable, copies of submissions for EU EED reporting that include ERF data."
        ],
        "recommendedActions": [
          "- If waste heat reuse is implemented, establish a robust methodology for calculating ERF in strict accordance with ISO/IEC 30134-6.\n- Install accurate metering to measure the amount of heat energy effectively reused externally.\n- Clearly define and document the primary energy source being offset by the reused heat.\n- Regularly calculate, track, and report ERF to stakeholders and for regulatory compliance (EED).\n- Use ERF metrics to evaluate and improve the performance of heat reuse systems."
        ],
        "sampleResponseTemplate": "For our [Facility Name] which implements [waste heat reuse for X], we calculate and report the Energy Reuse Factor (ERF) [annually/quarterly] as per ISO/IEC 30134-6. In [Reporting Period], our ERF was [Value], indicating that [Y]% of our total energy input was effectively reused, offsetting [Type of primary energy]. This data is used for our EED reporting. The calculation methodology is detailed in '[ERF Reporting Procedure ID]'.",
        "humanInputAnswer": "We implemented a waste heat reuse system for our 'CityNorth DC' in 2024, supplying low-grade heat to an adjacent office complex. We calculate ERF according to ISO/IEC 30134-6 on a monthly basis. For March 2025, the ERF was 0.08. This calculation uses metered heat delivered to the office (via heat meters) and our total DC energy consumption. This ERF data is included in our EED reporting submission.",
        "llmGeneratedAnswer": "Yes, as our 'EcoHub Facility' incorporates a waste heat reuse system supplying a local greenhouse (Practice 5.7.1), we report the Energy Reuse Factor (ERF) in accordance with ISO/IEC 30134-6. The ERF is calculated quarterly based on metered heat energy exported to the greenhouse and the total energy consumption of the data centre. For Q1 2025, the ERF was [Calculated Value, e.g., 0.15]. This metric is documented in our 'Quarterly Sustainability Metrics Report' and is a component of our annual EU Energy Efficiency Directive (EED) reporting (Ref: 'EED_Report_EcoHub_2025.pdf').",
        "badExampleAnswer": "We send some heat to a nearby building, but we don't have a specific way to measure how much energy that saves them or report it as ERF."
      },
      "question_160": {
        "id": 160,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented reporting of the Carbon Emission Reduction (CER) from energy reuse initiatives, in accordance with ISO/IEC 30134-6 Appendix A? Is this done to quantify the reduction in carbon emissions achieved by offsetting primary energy consumption elsewhere through data centre waste heat reuse, and is it understood that while not explicitly stated as required for EED in this guide, it provides valuable environmental performance data?",
        "simplifiedQuestion": "If you reuse waste heat, do you report the Carbon Emission Reduction (CER) using ISO/IEC 30134-6 Appendix A to show CO2 savings?",
        "reference": "2025 CoC Guide 9.3.9",
        "priority": "Low (Value: 1, Expected: Optional Practice)",
        "stepSection": "step_18",
        "recommendationTitle": "CER Reporting",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Methodology for calculating Carbon Emission Reduction (CER) from energy reuse, aligned with ISO/IEC 30134-6 Appendix A. Data inputs for CER (reused heat energy, carbon intensity of offset primary energy). Reports showing calculated CER values."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Optional (unless confirmed for EED): Methodology and calculation of Carbon Emission Reduction (CER) from waste heat reuse, per ISO/IEC 30134-6 Appendix A. Reports showing CER values.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of CER calculation procedures, source data for reused heat and carbon intensity of offset fuel, and reported CER values.",
        "nonconformity": {
          "major": null,
          "minor": "Waste heat is reused and ERF is reported, but the associated Carbon Emission Reduction (CER) is not calculated or reported, missing an opportunity to quantify direct carbon savings (if practice is adopted or EED requires)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued or required for EED: CER is reported, but the calculation methodology or carbon intensity factors used are not aligned with ISO/IEC 30134-6 Appendix A guidelines or are not transparently documented.",
        "whatThisMeans": "If your data centre reuses its waste heat, in addition to reporting how much energy is reused (ERF), your organization should also consider calculating and reporting the Carbon Emission Reduction (CER) that results from this. This is detailed in an appendix of the same standard (ISO/IEC 30134-6 Appendix A) and shows how much CO2 emission is avoided because your reused heat replaced some other primary energy source (like natural gas for heating). The guide notes this is required for EU EED reporting.",
        "complianceRequires": "- Where waste heat reuse is implemented, consider or implement reporting of the Carbon Emission Reduction (CER) achieved.\n- Calculate CER in accordance with the informative Appendix A of ISO/IEC 30134-6 ('Data centres - Key performance indicators - Part 6: Energy Reuse Factor (ERF)').\n- This metric quantifies the reduction in carbon emissions resulting from the reused data centre energy offsetting primary energy consumption elsewhere.\n- Understand that CER reporting is noted as being required for EU Commission EED (Energy Efficiency Directive) reporting, providing valuable environmental performance data beyond just energy reuse quantity.",
        "expertTip": "To calculate CER accurately, you need reliable data on the carbon intensity (e.g., kg CO2e/kWh) of the primary energy source that is being offset by your reused heat. Use official or widely recognized emission factors for the specific fuel or energy source being displaced in the recipient's application.",
        "recommendedEvidence": [
          "- Documented procedure for calculating CER from energy reuse, referencing ISO/IEC 30134-6 Appendix A.\n- Data inputs for CER calculation: amount of reused heat energy (from ERF calculation), and the carbon emission factor of the primary energy source being offset (e.g., natural gas, heating oil, grid electricity used for heating).\n- Examples of CER calculations and reported values (e.g., tonnes of CO2e avoided per year).\n- If applicable, copies of submissions for EU EED reporting that include CER data."
        ],
        "recommendedActions": [
          "- If waste heat reuse is implemented, establish a methodology for calculating Carbon Emission Reduction (CER) based on ISO/IEC 30134-6 Appendix A.\n- Identify the primary energy source(s) offset by the reused heat and determine their appropriate carbon emission factors.\n- Regularly calculate and report CER alongside ERF and other sustainability metrics.\n- Ensure CER reporting meets any requirements of the EU Energy Efficiency Directive."
        ],
        "sampleResponseTemplate": "For our waste heat reuse initiatives, we calculate the Carbon Emission Reduction (CER) annually using the methodology in ISO/IEC 30134-6 Appendix A. In [Reporting Period], by reusing [X MWh_thermal] of heat which offset [e.g., natural gas for building heating], we achieved a CER of [Y tonnes CO2e]. This is included in our EED reporting and detailed in '[Sustainability Report Annex Z]'.",
        "humanInputAnswer": "In conjunction with our ERF reporting for the 'EcoHub Data Centre', we calculate the Carbon Emission Reduction (CER) as per ISO/IEC 30134-6 Appendix A. For 2024, the reused heat (1,200 MWh_thermal) supplied to the adjacent greenhouse offset natural gas heating, resulting in an estimated CER of 240 tonnes of CO2e (using official government emission factors for natural gas). This is part of our EED data submission.",
        "llmGeneratedAnswer": "Yes, complementing our Energy Reuse Factor (ERF) reporting for facilities with heat reuse, we also calculate and report the Carbon Emission Reduction (CER). This is performed in accordance with the guidance in ISO/IEC 30134-6 Appendix A. The CER quantifies the avoided carbon emissions due to our waste heat offsetting the consumption of primary energy sources (e.g., natural gas for heating) by the heat recipient. For the 'EcoPark Data Centre' in 2024, the reported CER was [Calculated Value, e.g., 180 tonnes CO2e], based on the volume of heat reused and the carbon intensity of the displaced heating fuel. This metric is included in our EU EED reporting and highlights the broader environmental benefits of our heat reuse initiatives.",
        "badExampleAnswer": "We know reusing heat is good for the environment generally, but we don't do specific CO2 reduction calculations for it."
      },
      "question_161": {
        "id": 161,
        "old_question_text": null,
        "questionText": "Has the organization implemented reporting of Carbon Usage Effectiveness (CUE) in accordance with ISO/IEC 30134-8? Is this done to quantify the carbon emissions associated with the data centre's energy consumption, and is it understood that CUE reporting is required for EU Commission EED (Energy Efficiency Directive) reporting?",
        "simplifiedQuestion": "Do you report Carbon Usage Effectiveness (CUE) using ISO/IEC 30134-8 to show CO2 emissions from energy use, especially for EED reporting?",
        "reference": "2025 CoC Guide 9.3.10",
        "priority": "Low (Value: 1, Expected: Entire Data Centre, NOTE: Required for EU Commission EED reporting)",
        "stepSection": "step_18",
        "recommendationTitle": "CUE Reporting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Methodology for calculating Carbon Usage Effectiveness (CUE) aligned with ISO/IEC 30134-8. Data inputs for CUE (total DC energy, carbon emission factors for energy sources). Reports showing calculated CUE values. Evidence of EED reporting including CUE."
        },
        "docRequired": "Yes, if EED reporting applicable or CUE claimed",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory (if EED reporting applicable or CUE claimed): Documented methodology for CUE calculation (ISO/IEC 30134-8). Supporting data on energy consumption and relevant carbon emission factors. Reports including CUE values. Evidence of EED reporting.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of CUE calculation procedures, source data for energy consumption and emission factors, and reported CUE values. Verification of consistency with ISO/IEC standard and EED reporting requirements if applicable.",
        "nonconformity": {
          "major": "CUE is not calculated or reported, especially if required for EED compliance, despite the availability of energy consumption data and emission factors.",
          "minor": "CUE is calculated but methodology deviates from ISO/IEC 30134-8, or source data (emission factors) is outdated, inaccurate, or not specific enough."
        },
        "majorNonconformity": "Failure to report Carbon Usage Effectiveness (CUE) calculated in accordance with ISO/IEC 30134-8, particularly if required for EU Commission EED reporting.",
        "minorNonconformity": "CUE is reported, but the calculation methodology or the carbon emission factors used for energy sources are not fully aligned with ISO/IEC 30134-8 guidelines or are not transparently documented.",
        "whatThisMeans": "Your organization should measure and report the Carbon Usage Effectiveness (CUE) of your data centre. This metric, defined in ISO/IEC 30134-8, quantifies the carbon dioxide emissions produced per unit of IT work or energy. It helps understand the carbon footprint related to your data centre's energy use. Reporting CUE is also a requirement for the EU Energy Efficiency Directive (EED).",
        "complianceRequires": "- Implement reporting of Carbon Usage Effectiveness (CUE).\n- Calculate CUE in accordance with ISO/IEC 30134-8 ('Data centres - Key performance indicators - Part 8: Carbon usage effectiveness (CUE)').\n- This metric quantifies the carbon emissions associated with the data centre's energy consumption relative to its IT infrastructure energy or workload.\n- Understand that CUE reporting is explicitly noted as being required for EU Commission EED (Energy Efficiency Directive) reporting.",
        "expertTip": "Ensure you use up-to-date and accurate carbon emission factors for your electricity sources (grid mix, specific renewable contracts, on-site generation). Differentiate between location-based and market-based emission factors if appropriate for your reporting context. CUE is effectively PUE multiplied by the carbon intensity of your energy source(s).",
        "recommendedEvidence": [
          "- Documented procedure for calculating CUE, referencing ISO/IEC 30134-8.\n- Data inputs for CUE calculation: total data centre energy consumption (from Practice 9.1.1), and official or recognized carbon emission factors for all energy sources used by the data centre (e.g., kg CO2e/kWh for grid electricity, kg CO2e/litre for diesel).\n- Examples of CUE calculations and reported values.\n- If applicable, copies of submissions for EU EED reporting that include CUE data."
        ],
        "recommendedActions": [
          "- Establish a robust methodology for calculating CUE in strict accordance with ISO/IEC 30134-8.\n- Identify and document the appropriate carbon emission factors for all energy sources consumed by the data centre.\n- Regularly calculate, track, and report CUE to stakeholders and for regulatory compliance (EED).\n- Use CUE metrics, in conjunction with PUE and REF, to drive strategies for reducing the carbon footprint of data centre operations (e.g., improving energy efficiency, increasing renewable energy use)."
        ],
        "sampleResponseTemplate": "We calculate and report Carbon Usage Effectiveness (CUE) [annually/quarterly] in accordance with ISO/IEC 30134-8. Our CUE for [Reporting Period] was [Value kgCO2e/kWh_IT_Equipment_Energy]. This is based on our total energy consumption and the [specific grid emission factor / renewable energy mix details]. This data is included in our EED reporting (Ref: '[EED Report 202X]'). The methodology is detailed in '[CUE Reporting Procedure ID]'.",
        "humanInputAnswer": "Our CUE is calculated annually as per ISO/IEC 30134-8. For 2024, it was 0.25 kgCO2e/kWh_ITE. This uses our total data centre energy consumption, our PUE, and the official carbon intensity factor for the national grid mix where our facility is located, adjusted for our certified renewable energy purchases. This is a key metric in our EED submission and corporate sustainability report.",
        "llmGeneratedAnswer": "Yes, Carbon Usage Effectiveness (CUE) is reported annually by our organization, adhering to the ISO/IEC 30134-8 standard. The calculation utilizes the total energy consumption data of the data centre and the applicable carbon emission factors for our energy sources (including grid electricity with consideration for Guarantees of Origin, and any on-site generation). For the calendar year 2024, our CUE was [Calculated Value, e.g., 0.31 kgCO2e/kWh of IT equipment energy]. This metric is crucial for tracking our decarbonization progress and is included in our mandatory EU Energy Efficiency Directive (EED) reporting, as documented in 'Annual_Environmental_Metrics_Report_AEMR-2024.pdf'.",
        "badExampleAnswer": "We know our PUE, but we don't calculate CUE separately as it seems too complicated to get the carbon factors right."
      },
      "question_162": {
        "id": 162,
        "old_question_text": null,
        "questionText": "Where water is used for cooling or other significant purposes within the data centre (as per Practice 8.3.3 'Metering of water consumption'), has the organization implemented reporting of Water Usage Effectiveness (WUE) in accordance with ISO/IEC 30134-9 or EN 50600-4-9? Is this done to quantify the efficiency of water use, and is it understood that WUE reporting is required for EU Commission EED (Energy Efficiency Directive) reporting?",
        "simplifiedQuestion": "If your DC uses water for cooling, do you report Water Usage Effectiveness (WUE) using ISO/IEC 30134-9 or EN 50600-4-9, especially for EED reporting?",
        "reference": "2025 CoC Guide 9.3.11",
        "priority": "Low (Value: 1, Expected: Entire Data Centre, NOTE: Required for EU Commission EED reporting)",
        "stepSection": "step_18",
        "recommendationTitle": "WUE Reporting",
        "document_requirements": {
          "required": true,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv"
          ],
          "description": "Methodology for calculating Water Usage Effectiveness (WUE) aligned with ISO/IEC 30134-9 or EN 50600-4-9. Data inputs for WUE (total DC water consumption for cooling/operations, IT equipment energy). Reports showing calculated WUE values. Evidence of EED reporting including WUE."
        },
        "docRequired": "Yes, if water used for cooling AND EED reporting applicable/WUE claimed",
        "acceptedFiles": "pdf, docx, xlsx, csv",
        "documentDescriptor": "Mandatory (if water used for cooling AND EED reporting applicable/WUE claimed): Documented methodology for WUE calculation (ISO/IEC 30134-9 / EN 50600-4-9). Supporting data on water and IT energy consumption. Reports including WUE values. Evidence of EED reporting.",
        "verification_method": [
          "Document Review",
          "Data Analysis"
        ],
        "verificationMethods": "Review of WUE calculation procedures, source data for water consumption (from 8.3.3) and IT energy (from 9.1.2), and reported WUE values. Verification of consistency with ISO/IEC or EN standards and EED reporting requirements if applicable.",
        "nonconformity": {
          "major": "Significant water is used for cooling, but WUE is not calculated or reported according to ISO/IEC 30134-9 or EN 50600-4-9, especially if required for EED compliance.",
          "minor": "WUE is calculated but methodology deviates from standards, or source data (water consumption, IT energy) is inaccurate or not consistently measured."
        },
        "majorNonconformity": "If water is used for cooling or other significant purposes, failure to report Water Usage Effectiveness (WUE) calculated in accordance with ISO/IEC 30134-9 or EN 50600-4-9, particularly if required for EU Commission EED reporting.",
        "minorNonconformity": "WUE is reported, but the calculation methodology or data inputs (e.g., total data centre water consumption, IT equipment energy) are not fully aligned with the relevant ISO/IEC or EN standards, or the scope of reporting is incomplete.",
        "whatThisMeans": "If your data centre uses a significant amount of water, especially for cooling (like in cooling towers or adiabatic systems \u2013 see Practice 8.3.3 for metering), your organization needs to report its Water Usage Effectiveness (WUE). This metric is defined in standards like ISO/IEC 30134-9 or EN 50600-4-9 and shows how efficiently your data centre uses water in relation to the energy consumed by its IT equipment. Reporting WUE is also a requirement for the EU Energy Efficiency Directive (EED).",
        "complianceRequires": "- Where water is used for cooling or other significant purposes within the data centre (as per Practice 8.3.3), implement reporting of Water Usage Effectiveness (WUE).\n- Calculate WUE in accordance with recognized standards, such as ISO/IEC 30134-9 ('Data centres - Key performance indicators - Part 9: Water Usage Effectiveness (WUE)') or EN 50600-4-9 ('Data centre facilities and infrastructures - Part 4-9: Key Performance Indicators - Water Usage Effectiveness').\n- This metric quantifies the efficiency of water use in the data centre, typically relating water consumption to IT equipment energy consumption.\n- Understand that WUE reporting is explicitly noted as being required for EU Commission EED (Energy Efficiency Directive) reporting.",
        "expertTip": "Ensure accurate metering of all water consumed by the data centre that is attributable to its operation, particularly make-up water for cooling towers and adiabatic systems, as well as any water used for on-site generation if applicable. Clearly define the boundaries for both water and IT energy measurements according to the chosen standard (ISO/IEC or EN).",
        "recommendedEvidence": [
          "- Documented procedure for calculating WUE, referencing ISO/IEC 30134-9 or EN 50600-4-9.\n- Data inputs for WUE calculation: total data centre water consumption (Liters) for the reporting period (from Practice 8.3.3), and total IT equipment energy consumption (kWh) for the same period (from Practice 9.1.2).\n- Examples of WUE calculations and reported values (e.g., L/kWh_IT_Equipment_Energy).\n- If applicable, copies of submissions for EU EED reporting that include WUE data."
        ],
        "recommendedActions": [
          "- If significant water is used for cooling, establish a robust methodology for calculating WUE in strict accordance with ISO/IEC 30134-9 or EN 50600-4-9.\n- Ensure accurate and consistent metering of all relevant water consumption and IT equipment energy consumption (as per Practices 8.3.3 and 9.1.2).\n- Regularly calculate, track, and report WUE to stakeholders and for regulatory compliance (EED).\n- Use WUE metrics to identify opportunities for water conservation and to evaluate the effectiveness of water-saving initiatives."
        ],
        "sampleResponseTemplate": "As our facility utilizes [evaporative cooling towers / adiabatic cooling], we calculate and report Water Usage Effectiveness (WUE) [annually/quarterly] in accordance with [ISO/IEC 30134-9 / EN 50600-4-9]. Our WUE for [Reporting Period] was [Value L/kWh_ITE]. This is based on metered water consumption (from [Meter ID for 8.3.3]) and IT equipment energy (from [Meter ID for 9.1.2]). This data is included in our EED reporting (Ref: '[EED Report 202X]').",
        "humanInputAnswer": "Our data centre uses cooling towers, and we report WUE annually as per ISO/IEC 30134-9. For 2024, our WUE was 1.75 L/kWh of IT equipment energy. This calculation uses total site water consumption for cooling (metered via main make-up lines) and total IT equipment energy (from PDU-level metering). This information is a component of our EED submission and is detailed in our annual 'Environmental Performance Statement'.",
        "llmGeneratedAnswer": "Yes, as our primary cooling methodology involves evaporative cooling towers, our organization reports Water Usage Effectiveness (WUE). The WUE is calculated quarterly in accordance with ISO/IEC 30134-9, utilizing data from our comprehensive water metering system (Practice 8.3.3) for total data centre water consumption and our IT energy consumption figures (Practice 9.1.2). For Q1 2025, the WUE was recorded as [Calculated Value, e.g., 1.9 L/kWh_ITE]. This metric is integral to our water conservation efforts and is included in our mandatory reporting for the EU Energy Efficiency Directive (EED), as documented in 'Environmental_Metrics_Reporting_Protocol_EMRP-2025.pdf'.",
        "badExampleAnswer": "We use water in our cooling towers, but we don't measure exactly how much or calculate WUE. We just try not to waste it."
      },
      "question_163": {
        "id": 163,
        "old_question_text": null,
        "questionText": "Has the organization considered or developed a management dashboard that presents business-relevant information related to the data centre's energy consumption, cost, and potential carbon footprint in a format easily understandable by non-technical business managers? Does this dashboard translate technical metrics (like PUE, kWh) into business terms (e.g., energy cost per business transaction, carbon footprint per service, comparison against budget or targets) to better inform strategic business decisions and demonstrate the value of energy efficiency efforts?",
        "simplifiedQuestion": "Do you have a dashboard for business managers showing data centre energy use, cost, and carbon footprint in simple business terms (e.g., cost per transaction, CO2 per service)?",
        "reference": "2025 CoC Guide 9.3.12",
        "priority": "Low (Value: 1, Expected: Optional Practice)",
        "stepSection": "step_18",
        "recommendationTitle": "Business relevant dashboard",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "screenshots",
            "url"
          ],
          "description": "Screenshots or descriptions of any business-relevant energy/sustainability dashboards. Examples of how technical data is translated into business metrics. Information on the target audience and usage of such dashboards."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, jpg, screenshots, url",
        "documentDescriptor": "Optional: If implemented: Examples or screenshots of dashboards translating technical data centre energy/carbon metrics into business-relevant terms for non-technical managers. Documentation of the KPIs used and their calculation.",
        "verification_method": [
          "System Demonstration",
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Demonstration of the business-relevant dashboard. Review of KPI definitions and data sources. Interviews with business managers who use or receive information from such dashboards.",
        "nonconformity": {
          "major": null,
          "minor": "Energy and environmental data is reported in purely technical terms, without translation into business-relevant metrics for non-technical management, hindering strategic engagement and decision-making (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: While technical dashboards may exist (per 9.3.2 or 9.3.3), there is no dedicated effort or dashboard to translate key energy consumption, cost, and carbon footprint data into easily understandable, business-relevant metrics for non-technical managers.",
        "whatThisMeans": "To help business managers (who might not be technical experts) understand the importance of data centre energy use, your organization should think about creating a special dashboard. This dashboard would show information about the data centre's energy consumption, its cost, and its carbon footprint in ways that are easy for business managers to grasp. For example, it might show 'energy cost per customer transaction' or 'carbon footprint per business service,' and compare these against goals or budgets. This helps them make better strategic decisions and see the value of energy efficiency.",
        "complianceRequires": "- Consider or develop a management dashboard that presents business-relevant information related to data centre energy consumption, cost, and potential carbon footprint.\n- Design this dashboard in a format that is easily understandable by non-technical business managers.\n- Translate technical metrics (e.g., kWh, PUE, CUE) into business-relevant terms, such as:\n    - Energy cost per business transaction or per user.\n    - Carbon footprint per service or per business unit.\n    - Comparison of energy performance against budget, targets, or business output.\n- The aim is to better inform strategic business decisions and demonstrate the business value of energy efficiency initiatives.",
        "expertTip": "Work with business stakeholders to identify which business-relevant metrics would be most meaningful and actionable for them. Start with a few simple, high-impact metrics and iterate based on feedback. Ensure clear definitions and transparent methodologies for calculating these business-linked KPIs.",
        "recommendedEvidence": [
          "- Screenshots or a live demonstration of the business-relevant dashboard.\n- Definitions of the specific business-relevant KPIs displayed and how they are calculated from technical data.\n- Examples of how this dashboard is used in management reporting or strategic decision-making processes.\n- Feedback from business managers on the utility of the dashboard."
        ],
        "recommendedActions": [
          "- Identify key business metrics that can be correlated with data centre energy consumption, cost, or carbon footprint.\n- Develop methodologies to translate technical data (kWh, PUE, CUE, etc.) into these business-relevant terms.\n- Design and implement a dashboard or reporting format that presents this information clearly and concisely to non-technical managers.\n- Regularly share and discuss these business-relevant energy/sustainability metrics with senior management and business unit leaders."
        ],
        "sampleResponseTemplate": "We have developed a '[Quarterly Business Sustainability Dashboard]' which translates data centre energy metrics into business terms. For example, it shows '[Energy cost as a % of revenue for Service X / CO2 emissions per 1000 active users]' and tracks this against targets. This dashboard is presented to [e.g., the Executive Leadership Team / Business Unit Heads]. A sample view is attached. (Ref: '[Dashboard Spec ID]').",
        "humanInputAnswer": "Our 'Executive Sustainability Dashboard', updated monthly, includes a section on 'Data Centre Business Impact'. This translates our total DC energy spend and carbon footprint (from CUE) into 'Energy Cost per Processed Transaction' and 'Carbon Footprint per Customer Account'. It also tracks these against quarterly targets. This dashboard is reviewed by our CEO and business unit VPs, helping to contextualize energy efficiency efforts in business terms.",
        "llmGeneratedAnswer": "Yes, our organization has developed a 'Business Operations & Sustainability Dashboard' which includes a module dedicated to data centre performance, aimed at non-technical senior management. This module, as detailed in 'BOSD_KPI_Definitions.pdf', translates core technical metrics like PUE, total energy consumption (kWh), and CUE into business-relevant terms such as 'IT Energy Cost per Unit of Business Output [e.g., per $1M revenue or per key transaction type]' and 'Carbon Footprint per Service Category'. It also tracks performance against budgeted energy costs and our corporate carbon reduction targets. This dashboard is reviewed monthly by the executive committee to inform strategic resource allocation and sustainability investment decisions.",
        "badExampleAnswer": "We send the PUE numbers to our CIO. It's up to them to understand what it means for the business."
      },
      "question_164": {
        "id": 164,
        "old_question_text": null,
        "questionText": "Has the organization implemented reporting on the utilization of its server hardware (e.g., CPU, memory, disk, network interface utilization)? Is this data regularly reviewed by IT management to identify underutilized servers, inform capacity planning, support consolidation efforts (Practice 4.3.4), and potentially calculate IT Equipment Utilisation for servers (ITEUsv - Practice 9.3.7)?",
        "simplifiedQuestion": "Do you report on server hardware utilization (CPU, memory, etc.) to find underused servers and inform planning/consolidation?",
        "reference": "2025 CoC Guide 9.4.1",
        "priority": "Medium (Value: 3, Expected: Optional Practice)",
        "stepSection": "step_18",
        "recommendationTitle": "Server Utilisation",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv",
            "screenshots"
          ],
          "description": "Server utilization monitoring reports (CPU, memory, disk, network). Procedures for reviewing utilization data. Examples of how this data informs capacity planning, consolidation, or ITEUsv calculation."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, csv, screenshots",
        "documentDescriptor": "Optional: If implemented: Reports showing server hardware utilization (CPU, memory, disk, network). Documentation of review processes and actions taken based on this data (e.g., consolidation, ITEUsv input).",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "Interview"
        ],
        "verificationMethods": "Review of server utilization reports and related capacity/consolidation planning documents. Interviews with IT operations and capacity managers.",
        "nonconformity": {
          "major": null,
          "minor": "Server utilization is not systematically monitored or reported, or if data is collected, it's not effectively used to identify optimization opportunities (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Server utilization is monitored, but reporting is inconsistent, lacks key metrics (CPU, memory, disk, network), or the data is not regularly reviewed by IT management to drive actions like consolidation or inform ITEUsv calculations.",
        "whatThisMeans": "Your organization should monitor and create reports on how much your server hardware is actually being used \u2013 this includes looking at CPU usage, memory usage, disk activity, and network traffic for each server. IT managers should regularly review this information to find servers that are not being used much, help plan for future capacity needs, support efforts to consolidate servers (as mentioned in Practice 4.3.4), and potentially feed into calculating overall server utilization metrics (like ITEUsv \u2013 Practice 9.3.7).",
        "complianceRequires": "- Implement reporting on the utilization of server hardware.\n- This reporting should include metrics such as CPU utilization, memory utilization, disk I/O, and network interface utilization.\n- Ensure this data is regularly reviewed by IT management.\n- Use the data to:\n    - Identify underutilized servers.\n    - Inform capacity planning.\n    - Support server consolidation initiatives (Practice 4.3.4).\n    - Potentially as an input for calculating IT Equipment Utilisation for servers (ITEUsv - Practice 9.3.7).",
        "expertTip": "Automate the collection of server utilization data using IT monitoring tools. Set thresholds for underutilization and overutilization to trigger alerts or reviews. Correlate utilization data with application performance to ensure that consolidation efforts do not negatively impact service levels.",
        "recommendedEvidence": [
          "- Examples of regular server utilization reports showing metrics like CPU, memory, disk, and network utilization for individual servers or groups of servers.\n- Documentation of the process for reviewing these reports and taking action (e.g., minutes from capacity review meetings, action plans for server consolidation).\n- Evidence that utilization data is used as input for capacity planning or ITEUsv calculations."
        ],
        "recommendedActions": [
          "- Implement comprehensive monitoring tools to collect utilization data (CPU, memory, disk, network) for all production servers.\n- Establish a regular reporting cycle for server utilization metrics, tailored to IT management needs.\n- Develop clear criteria for identifying underutilized servers and a process for evaluating them for consolidation, decommissioning, or workload migration.\n- Use utilization trends to improve the accuracy of capacity forecasting."
        ],
        "sampleResponseTemplate": "We generate weekly server utilization reports from [Monitoring Tool, e.g., VMware vROps / Zabbix], covering [CPU, memory, disk I/O, network I/O] for all production servers. These reports are reviewed by the [IT Operations Manager / Capacity Planning Team] to identify [underutilized servers (<X% CPU average for Y days) for consolidation (Practice 4.3.4) / servers approaching capacity limits]. This data also feeds into our ITEUsv calculations (Practice 9.3.7).",
        "humanInputAnswer": "Our 'Server Performance & Utilization Monitoring SOP (SPUM-001)' mandates daily collection of CPU, memory, disk I/O, and network utilization for all physical and virtual servers via SolarWinds. Monthly reports are generated for IT management, highlighting servers averaging below 20% CPU utilization or above 80% on any key resource. This data directly informed our Q1 2025 server consolidation project, where 15 underutilized physical servers were P2V'd. It is also a primary input for our ITEUsv reporting.",
        "llmGeneratedAnswer": "Yes, our organization has implemented comprehensive reporting on server hardware utilization. Our [IT Monitoring Platform, e.g., Datadog] continuously collects metrics including CPU utilization (average and peak), memory usage, disk I/O rates, and network interface throughput for all production servers. As per our 'IT Resource Management Policy (ITRMP-2025, Section 5.1)', utilization reports are generated weekly and reviewed by IT Operations and Capacity Planning teams. This data is pivotal for identifying underutilized assets for consolidation (Practice 4.3.4), proactive capacity planning, and provides essential input for our ITEUsv calculations (Practice 9.3.7). For example, the Q1 2025 utilization review flagged 12 servers for potential decommissioning due to sustained low utilization.",
        "badExampleAnswer": "We know if servers are getting slow or running out of space because users complain. We don't have detailed utilization reports for everything."
      },
      "question_165": {
        "id": 165,
        "old_question_text": null,
        "questionText": "Has the organization implemented reporting on the utilization of its core network equipment (e.g., routers, switches, firewalls), including metrics such as port utilization, bandwidth consumption, and device CPU/memory load? Is this data regularly reviewed by IT management to identify underutilized network assets, optimize network topology, inform capacity planning, and potentially reduce the energy consumption of the network infrastructure (e.g., by powering down unused ports or devices where feasible and safe)?",
        "simplifiedQuestion": "Do you report on core network equipment utilization (ports, bandwidth, device load) to find underused gear, optimize your network, and potentially save energy?",
        "reference": "2025 CoC Guide 9.4.2",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_18",
        "recommendationTitle": "Network Utilisation",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv",
            "screenshots"
          ],
          "description": "Network utilization monitoring reports (port usage, bandwidth, device CPU/memory). Procedures for reviewing this data. Examples of how data informs network optimization, capacity planning, or energy reduction efforts (e.g., port shutdown policies)."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, csv, screenshots",
        "documentDescriptor": "Optional: If implemented: Reports showing network equipment utilization (port status/traffic, device load). Documentation of review processes and actions based on this data (e.g., network topology changes, decommissioning of underused switches, port power management).",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "Interview"
        ],
        "verificationMethods": "Review of network utilization reports and capacity planning documents. Interviews with network operations and planning teams.",
        "nonconformity": {
          "major": null,
          "minor": "Network utilization is not systematically monitored or reported, or if data is collected, it's not effectively used to identify optimization or energy-saving opportunities (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Network equipment utilization (port, bandwidth, device load) is not comprehensively monitored or reported, or the collected data is not regularly reviewed by IT management to drive network optimization, capacity planning, or energy conservation measures (e.g., identifying and powering down unused ports/devices).",
        "whatThisMeans": "Your organization should monitor and create reports on how much your main network equipment (like routers, switches, firewalls) is being used. This includes looking at how many ports are active, how much data is flowing (bandwidth), and how busy the devices themselves are (CPU/memory load). IT managers should regularly review this to find network gear that's not being used much, to optimize the network layout, plan for future needs, and potentially save energy by switching off unused ports or entire devices if it's safe to do so.",
        "complianceRequires": "- Implement reporting on the utilization of core network equipment (routers, switches, firewalls).\n- Include metrics such as port utilization (active vs. inactive ports), bandwidth consumption (average and peak), and device CPU/memory load.\n- Ensure this data is regularly reviewed by IT management.\n- Use the data to:\n    - Identify underutilized network assets.\n    - Optimize network topology for efficiency and performance.\n    - Inform network capacity planning.\n    - Potentially reduce the energy consumption of the network infrastructure (e.g., by implementing policies to power down unused ports or decommission redundant/underutilized devices where operationally feasible and safe).",
        "expertTip": "Utilize Network Performance Monitoring (NPM) tools that can automatically discover network devices, collect utilization data via SNMP or other protocols, and provide dashboards and reports. Implement a policy for deactivating or 'shutting down' unused switch ports to save energy, and regularly audit for and decommission completely unused network devices.",
        "recommendedEvidence": [
          "- Examples of regular network utilization reports showing metrics like port status/utilization, bandwidth throughput, and device (CPU/memory) load for core network equipment.\n- Documentation of the process for reviewing these reports and taking action (e.g., network reconfigurations, decommissioning of unused switches, implementation of port power-saving features).\n- Network capacity planning documents that incorporate utilization data.\n- Any policies related to managing unused network ports or devices to save energy."
        ],
        "recommendedActions": [
          "- Implement network monitoring tools to collect utilization data for all core network equipment.\n- Establish a regular reporting cycle for network utilization metrics.\n- Develop clear criteria for identifying underutilized network devices or ports.\n- Create and implement procedures for optimizing network topology, decommissioning unused equipment, and managing port status for energy efficiency where possible.\n- Use utilization trends to improve network capacity forecasting."
        ],
        "sampleResponseTemplate": "We use [NPM Tool, e.g., SolarWinds NPM / PRTG Network Monitor] to generate [weekly/monthly] reports on core network equipment utilization, including [port utilization on our core switches, bandwidth on internet links, CPU load on firewalls]. These reports are reviewed by the [Network Operations Team / IT Infrastructure Manager] to identify [e.g., 'underutilized switches for potential decommissioning', 'congested links requiring upgrade']. We have a policy to [administratively disable unused switch ports].",
        "humanInputAnswer": "Our network operations centre (NOC) uses LibreNMS to monitor all core switches and routers. Monthly utilization reports are generated, covering per-port bandwidth, error rates, and device CPU/memory load (Ref: 'NetOps_Monthly_Report_Template.pdf'). These reports help us identify chronically underused ports, which are then administratively shut down as per 'Port Management Policy NET-POL-007'. Last quarter, this led to disabling ~200 unused 1GbE ports across access switches, contributing to minor energy savings.",
        "llmGeneratedAnswer": "Yes, our organization monitors and reports on the utilization of its core network equipment. Using [Network Monitoring System, e.g., Cisco DNA Center/Juniper Mist], we collect data on port utilization, aggregate bandwidth consumption, and device (router/switch/firewall) CPU and memory load at 15-minute intervals. This information is compiled into a 'Monthly Network Performance & Capacity Report' (example: 'NetReport_2025-04.pdf'), reviewed by the Network Engineering team. This review identifies underutilized assets for potential consolidation or decommissioning, informs network topology optimizations, and supports proactive capacity planning. For instance, analysis of port utilization data in Q1 2025 led to the consolidation of two edge switches, reducing overall power draw.",
        "badExampleAnswer": "Our network seems to be working fine. We only look at it if there are performance problems or if we need to add more connections."
      }
    }
  },
  "step_19": {
    "description": "2025 Guide Section 9: Monitoring (Practices 9.4.3-9.4.4 from Sec 9.4 'IT Reporting')",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Monitoring_4"
    ],
    "critical_questions": [],
    "questions": {
      "question_166": {
        "id": 166,
        "old_question_text": null,
        "questionText": "Has the organization implemented reporting on the utilization of its storage hardware (e.g., capacity utilization, IOPS performance, throughput, cache hit rates)? Is this data regularly reviewed by IT management to identify underutilized storage assets, optimize data placement (tiering), inform capacity planning, support consolidation efforts (Practice 4.3.4), and ensure that storage performance is not a bottleneck while minimizing energy consumption (e.g., by powering down unused arrays or shelves where feasible, referencing Practice 4.4.6 on reducing storage volume)?",
        "simplifiedQuestion": "Do you report on storage hardware utilization (capacity, performance) to find underused gear, optimize data placement, and plan capacity, aiming to minimize energy use?",
        "reference": "2025 CoC Guide 9.4.3",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_19",
        "recommendationTitle": "Storage Utilisation",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "xlsx",
            "csv",
            "screenshots"
          ],
          "description": "Storage utilization monitoring reports (capacity, IOPS, throughput, cache hits). Procedures for reviewing this data. Examples of how data informs storage tiering, capacity planning, consolidation, or energy reduction (e.g., powering down unused arrays)."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, xlsx, csv, screenshots",
        "documentDescriptor": "Optional: If implemented: Reports showing storage hardware utilization (capacity, performance metrics). Documentation of review processes and actions based on this data (e.g., data migration for tiering, consolidation of arrays, decommissioning of unused storage).",
        "verification_method": [
          "Document Review",
          "System Data Analysis",
          "Interview"
        ],
        "verificationMethods": "Review of storage utilization reports and capacity/tiering planning documents. Interviews with storage administrators and IT operations managers.",
        "nonconformity": {
          "major": null,
          "minor": "Storage utilization is not systematically monitored or reported, or if data is collected, it's not effectively used to identify optimization or energy-saving opportunities like tiering, consolidation, or decommissioning (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: Storage hardware utilization (capacity, performance aspects like IOPS, throughput) is not comprehensively monitored or reported, or the collected data is not regularly reviewed by IT management to drive data placement optimization (tiering), capacity planning, consolidation efforts, or energy conservation measures (e.g., powering down unused arrays/shelves).",
        "whatThisMeans": "Your organization should monitor and create reports on how much your storage hardware is being used. This includes looking at how much storage space is filled (capacity utilization), how busy it is with read/write operations (IOPS performance, throughput), and how effectively its cache is working. IT managers should regularly review this information to find storage systems that are not being used much, to decide where to place data for best efficiency (tiering), plan for future storage needs, support efforts to consolidate storage systems (Practice 4.3.4), and make sure storage isn't a performance bottleneck, all while trying to use as little energy as possible (e.g., by switching off unused storage arrays or shelves if safe, as linked to Practice 4.4.6).",
        "complianceRequires": "- Implement reporting on the utilization of storage hardware.\n- Include metrics such as capacity utilization (e.g., % allocated, % used), IOPS performance, data throughput, and cache hit rates where available.\n- Ensure this data is regularly reviewed by IT management.\n- Use the data to:\n    - Identify underutilized storage assets or stranded capacity.\n    - Optimize data placement strategies (e.g., storage tiering - Practice 4.4.3).\n    - Inform storage capacity planning and forecasting.\n    - Support storage consolidation initiatives (Practice 4.3.4).\n    - Ensure storage performance meets service requirements while minimizing energy consumption (e.g., by identifying opportunities to power down unused arrays or shelves, aligning with Practice 4.4.6 on reducing total storage volume where appropriate).",
        "expertTip": "Utilize storage resource management (SRM) tools that can provide comprehensive insights into capacity, performance, and utilization across heterogeneous storage environments. Implement automated storage tiering where possible to move data to the most cost-effective and energy-efficient storage type based on access patterns. Regularly reclaim orphaned or unallocated storage.",
        "recommendedEvidence": [
          "- Examples of regular storage utilization reports showing metrics like capacity usage (total, per tier, per array), IOPS, throughput, and cache hit rates.\n- Documentation of the process for reviewing these reports and taking action (e.g., data migration, storage consolidation, decommissioning of old arrays).\n- Storage capacity planning documents that incorporate utilization data and trends.\n- Policies related to data tiering or managing unused storage capacity for energy efficiency."
        ],
        "recommendedActions": [
          "- Implement storage monitoring tools to collect comprehensive utilization data (capacity and performance) for all significant storage assets.\n- Establish a regular reporting cycle for storage utilization metrics.\n- Develop clear criteria for identifying underutilized storage capacity or arrays.\n- Create and implement procedures for optimizing data placement (tiering), consolidating storage platforms, and decommissioning unused storage systems or components.\n- Use utilization trends to improve storage capacity forecasting and procurement decisions."
        ],
        "sampleResponseTemplate": "We use [SRM Tool, e.g., Dell EMC ViPR SRM / NetApp OnCommand Insight] to generate [weekly/monthly] reports on storage utilization, covering [capacity usage (%), IOPS, throughput, cache hit %] for our [SAN/NAS arrays]. These are reviewed by the [Storage Team / IT Infrastructure Manager] to [e.g., 'identify candidates for data tiering to lower-cost/lower-power storage', 'plan capacity upgrades', 'initiate consolidation of older arrays']. This data supports Practices 4.3.4 and 4.4.6.",
        "humanInputAnswer": "Our storage team uses Pure1 Meta (for Pure Storage arrays) and Dell EMC CloudIQ to monitor capacity utilization, IOPS, latency, and bandwidth for our primary storage systems. Monthly reports are generated (Ref: 'Storage_Perf_Util_Report_Template.docx') and reviewed to ensure capacity remains above 20% free and performance meets SLAs. Data is used to plan data migration to our active archive tier (NetApp StorageGRID) for less active data, thereby optimizing performance-tier utilization and energy. Underutilized LUNs are reclaimed quarterly.",
        "llmGeneratedAnswer": "Yes, our organization has implemented comprehensive reporting on storage hardware utilization. Our [Storage Resource Management (SRM) tool, e.g., IBM Storage Insights] provides daily and monthly reports on key metrics, including:\n- Capacity utilization (allocated vs. actual used, thin provisioning savings) for each storage array.\n- IOPS and throughput performance against defined service tiers.\n- Cache hit rates and controller utilization.\nThis data, as outlined in our 'Storage Performance & Capacity Management Protocol (SPCMP-2025)', is reviewed weekly by the storage administration team and monthly by IT management. It informs our data tiering strategy (Practice 4.4.3), capacity planning, identifies candidates for storage consolidation (Practice 4.3.4), and helps us ensure efficient use of power by right-sizing environments and decommissioning underused arrays (supporting Practice 4.4.6). For example, Q1 2025 review led to the planned consolidation of two older arrays onto a single new, more power-efficient system.",
        "badExampleAnswer": "We add more storage when users run out of space. We check performance if there are complaints."
      },
      "question_167": {
        "id": 167,
        "old_question_text": null,
        "questionText": "Has the organization considered or implemented a dashboard for the IT department that illustrates business-relevant information concerning IT services? Does this include metrics such as the number of transactions or users per kWh of IT equipment energy, or the cost and carbon footprint (if available) attributed to specific IT services or business units, to help the IT department understand and communicate its efficiency in business terms?",
        "simplifiedQuestion": "Do you have a dashboard for the IT department showing business-relevant metrics like transactions/users per kWh, or energy cost/carbon footprint per IT service/business unit?",
        "reference": "2025 CoC Guide 9.4.4",
        "priority": "Low (Value: 2, Expected: Optional Practice)",
        "stepSection": "step_19",
        "recommendationTitle": "Business relevant dashboard",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "jpg",
            "screenshots",
            "url"
          ],
          "description": "Screenshots or descriptions of any IT department dashboards showing business-relevant energy/sustainability metrics. Examples of how IT technical data is correlated with business metrics (transactions, users, services). Information on how the IT department uses this dashboard."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, jpg, screenshots, url",
        "documentDescriptor": "Optional: If implemented: Examples or screenshots of dashboards used by the IT department that correlate IT energy/carbon metrics with business-relevant units (e.g., transactions/kWh, cost/service). Documentation of KPIs and data sources.",
        "verification_method": [
          "System Demonstration",
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Demonstration of the IT department's business-relevant dashboard. Review of KPI definitions and how business and IT data are correlated. Interviews with IT managers and analysts who use the dashboard.",
        "nonconformity": {
          "major": null,
          "minor": "IT department reports primarily on technical IT metrics without translating them into business-relevant efficiency or sustainability terms, hindering communication of value to the business (if practice is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If pursued: No dashboard or reporting mechanism exists within the IT department to correlate its energy consumption, cost, or carbon footprint with business-relevant metrics (e.g., transactions, users, services), thus limiting the IT department's ability to demonstrate and improve its efficiency in business terms.",
        "whatThisMeans": "The IT department itself should consider having a dashboard that shows how its operations relate to the business in terms of energy and cost. This is different from the high-level business dashboard in 9.3.12. For the IT team, this could mean tracking things like the number of business transactions or active users per kilowatt-hour of IT energy used, or the energy cost and carbon footprint associated with providing specific IT services or supporting particular business units. This helps the IT department understand its own efficiency from a business perspective and communicate it effectively.",
        "complianceRequires": "- Consider or implement a dashboard specifically for the IT department.\n- This dashboard should illustrate business-relevant information concerning the IT services it provides and the resources consumed.\n- Include metrics such as:\n    - Number of transactions or users per kWh of IT equipment energy.\n    - Energy cost attributed to specific IT services or supporting specific business units.\n    - Carbon footprint (if available and calculated) attributed to specific IT services or business units.\n- The aim is to help the IT department understand its efficiency in business terms and communicate this to relevant stakeholders, including potentially feeding into the broader business dashboard (Practice 9.3.12).",
        "expertTip": "Start by identifying 2-3 key IT services and the primary business metrics associated with them (e.g., for an e-commerce platform: orders processed; for a CRM: active users). Then, work to allocate IT equipment energy consumption (from Practice 9.1.2 or 9.1.9) to these services. This allows for the calculation of insightful KPIs like 'energy per order' or 'energy per active CRM user'.",
        "recommendedEvidence": [
          "- Screenshots or a live demonstration of the IT department's business-relevant dashboard.\n- Definitions of the specific business-IT correlated KPIs displayed (e.g., transactions/kWh, energy cost/service, CO2/user).\n- Explanation of how data from IT energy monitoring and business systems is combined to calculate these metrics.\n- Examples of how the IT department uses this dashboard for internal performance tracking, optimization, or reporting to business units."
        ],
        "recommendedActions": [
          "- Identify key IT services and the business metrics that define their output or usage.\n- Develop a methodology to attribute IT equipment energy consumption, operational costs, and (if available) carbon footprint to these specific IT services or the business units they support.\n- Implement a dashboard or reporting mechanism within the IT department to visualize these business-relevant efficiency and sustainability metrics.\n- Use these metrics to identify opportunities for improving the energy efficiency of service delivery and to demonstrate the IT department's contribution to overall business and sustainability goals."
        ],
        "sampleResponseTemplate": "Our IT Operations team uses an internal dashboard (built on [Platform, e.g., Splunk/Kibana]) that correlates IT metrics with business outputs. For example, we track '[kWh per 1000 e-commerce transactions]' and '[average server energy cost per virtual desktop session]'. This dashboard, updated [daily/weekly], helps us identify how application performance and infrastructure efficiency impact business-relevant costs and resource use. A sample view is provided.",
        "humanInputAnswer": "The IT Infrastructure team maintains a 'Service Efficiency Dashboard' using Microsoft Power BI. It pulls data from our VMware monitoring (VM utilization), ServiceNow (service catalog and business unit mapping), and our PDU-level energy monitoring. Key metrics displayed include 'Average kWh per active Virtual Desktop Infrastructure (VDI) user per month' and 'Energy cost for supporting the Enterprise Resource Planning (ERP) service'. This helps us demonstrate IT's energy stewardship to business unit liaisons.",
        "llmGeneratedAnswer": "Yes, our IT department utilizes a 'Service Delivery Efficiency Dashboard'. This dashboard, developed in-house using [BI Tool Name] and refreshed weekly, integrates data from our IT service management platform, application performance monitoring tools, and data centre energy monitoring systems. It presents business-relevant metrics such as 'Energy Cost per [Key Business Metric, e.g., Processed Insurance Claim]' and 'Carbon Footprint (estimated) per [Key Service, e.g., Online Banking Platform User Session]'. This enables the IT department to track and communicate its operational efficiency in terms directly relatable to business value and supports informed discussions on resource optimization for specific services. (Example Dashboard View: 'IT_Business_Metrics_Dashboard.pdf')",
        "badExampleAnswer": "We provide the business with the total IT energy bill. They can figure out how it relates to their activities if they want to."
      }
    }
  },
  "step_20": {
    "description": "2025 Guide Section 11: Items under consideration (Practices 11.1 through 11.4)",
    "upload_required": true,
    "reviewers": [
      "AuditorGroup_Future_Considerations"
    ],
    "critical_questions": [],
    "questions": {
      "question_168": {
        "id": 168,
        "old_question_text": null,
        "questionText": "Is the organization aware of and actively monitoring the ongoing 'Items under consideration' for the EU Code of Conduct, specifically regarding 'Further development of software efficiency definitions'? Does the organization proactively consider how future standardized metrics or definitions for software energy efficiency might impact its software selection (Practice 4.2.4), in-house development (Practice 4.2.5), and procurement of outsourced software development (Practice 4.2.6)? Is there an internal process or assigned responsibility for tracking such developments and preparing for the potential adoption of new software efficiency standards or metrics when they become established?",
        "simplifiedQuestion": "Are you actively monitoring the development of standardized software energy efficiency definitions (as noted in CoC Section 11.1) and preparing to use them in your software lifecycle practices?",
        "reference": "2025 CoC Guide 11.1",
        "priority": "Low (Value: Not specified in guide table, Expected: Not specified - Implied future consideration and awareness)",
        "stepSection": "step_20",
        "recommendationTitle": "Further development of software efficiency definitions",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "md",
            "pptx"
          ],
          "description": "Internal strategy documents, technology watch reports, or meeting minutes demonstrating awareness and discussion of evolving software efficiency definitions. Notes on potential impacts and preparedness for adopting future standards."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, md, pptx",
        "documentDescriptor": "Optional: Documentation showing proactive monitoring of software efficiency definition developments and strategic consideration of their future impact on software procurement, development, and contractual agreements.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of any internal documentation related to tracking emerging IT sustainability standards. Interviews with IT strategists, software development leads, and procurement managers.",
        "nonconformity": {
          "major": null,
          "minor": "The organization shows no awareness or proactive consideration of the evolving landscape of software efficiency definitions and their potential future impact on its established software lifecycle practices (if practice of monitoring Section 11 items is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If active monitoring of CoC developments is claimed: Lack of evidence of systematically tracking, discussing, or planning for the integration of emerging standardized software efficiency definitions into relevant internal processes (software selection, development, outsourcing).",
        "whatThisMeans": "The EU Code of Conduct (Section 11.1) highlights that defining and measuring software energy efficiency is an area of ongoing development. Your organization should stay informed about this progress. This involves thinking about how new, standardized methods for defining or measuring software efficiency could influence your future decisions on buying software (Practice 4.2.4), developing it internally (Practice 4.2.5), or contracting out its development (Practice 4.2.6). Being prepared to adopt these new standards when they mature is key.",
        "complianceRequires": "- Maintain awareness that 'Further development of software efficiency definitions' is an active item under consideration for the EU Code of Conduct.\n- Proactively monitor progress from industry bodies, research institutions, and standardization organizations (e.g., ISO/IEC) in developing practical and robust metrics for software energy efficiency.\n- Internally discuss and assess how future standardized definitions or metrics could be integrated into existing lifecycle practices for software (selection, in-house development, outsourced development).\n- Foster a state of preparedness to evaluate and potentially adopt recognized software efficiency standards or metrics as they become established and offer clear benefits.",
        "expertTip": "Designate an individual or team (e.g., within IT architecture, sustainability, or R&D) to track advancements in software efficiency metrics and green software engineering principles. Encourage participation in relevant industry forums or working groups to stay at the forefront of these developments.",
        "recommendedEvidence": [
          "- Internal reports or presentations from technology watch activities that cover software efficiency definitions.\n- Records of discussions (e.g., meeting minutes from strategy or architecture reviews) about adapting software practices to upcoming efficiency standards.\n- Subscriptions to or participation in relevant standards bodies or industry groups (e.g., Green Software Foundation, ISO/IEC JTC 1/SC 39).\n- Roadmaps or internal plans showing consideration for future software efficiency metric adoption."
        ],
        "recommendedActions": [
          "- Formally assign responsibility for monitoring the evolution of software efficiency definitions and related standards.\n- Schedule periodic internal briefings or reviews on the status of these developments and their potential organizational impact.\n- Start preliminary discussions on how current software development guidelines and procurement criteria might need to evolve.\n- Encourage software development teams to explore and experiment with green software engineering principles."
        ],
        "sampleResponseTemplate": "Our [e.g., IT Strategy Committee / Sustainability Office] actively monitors progress on software efficiency definitions (CoC Section 11.1) through [e.g., industry publications, participation in X forum]. We have an internal working group that [e.g., meets quarterly] to assess how emerging standards could refine our existing software selection (4.2.4) and development (4.2.5) best practices. We aim to be early adopters of relevant ISO/IEC standards in this domain once finalized.",
        "humanInputAnswer": "Our 'Emerging Technologies Council' has a standing agenda item to review progress on software efficiency metrics, including developments noted in CoC 11.1. We presented a white paper internally in Q1 2025 (Ref: 'SoftwareEfficiency_FutureMetrics_WP.pdf') on how upcoming ISO standards could impact our custom development lifecycle and vendor assessment criteria. We are also piloting Green Software Foundation principles in one of our new development projects.",
        "llmGeneratedAnswer": "Yes, our organization is actively tracking the 'Further development of software efficiency definitions' as highlighted in CoC Section 11.1. Our [e.g., Office of the CTO / Sustainability Research Team] is tasked with monitoring advancements from key standardization bodies and industry initiatives like the Green Software Foundation. Findings are reviewed biannually, and their potential impact on our internal software development guidelines (Practice 4.2.5) and third-party software procurement criteria (Practice 4.2.4) is assessed. We are committed to integrating robust, standardized software efficiency metrics into our processes as they become industry-accepted and practically implementable.",
        "badExampleAnswer": "We just focus on making sure our software works as required. We haven't specifically tracked external developments on how to define its energy efficiency."
      },
      "question_169": {
        "id": 169,
        "old_question_text": null,
        "questionText": "Is the organization aware of and actively monitoring potential future revisions or tightening of requirements within the 'EU Eco Design Directive regarding idle state power' for servers and data storage products (e.g., Lot 9 or its successors), as noted in CoC Section 11.2? Does the organization have a process to evaluate how such changes would impact its IT equipment procurement strategies (referencing Practice 4.1.10) and ensure timely adoption of any new or stricter mandatory idle power consumption limits for servers and storage systems?",
        "simplifiedQuestion": "Are you monitoring potential EU Eco Design changes for server/storage idle power (CoC Section 11.2) and ready to update your IT purchasing to meet new rules?",
        "reference": "2025 CoC Guide 11.2",
        "priority": "Low (Value: Not specified, Expected: Not specified - Implied future consideration)",
        "stepSection": "step_20",
        "recommendationTitle": "EU Eco Design Directive idle state power",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "md"
          ],
          "description": "Process documentation for tracking relevant regulatory changes (e.g., EU Eco Design). Internal communications or reports discussing potential future Eco Design requirements for server/storage idle power. Plans for adapting IT procurement policies if new regulations are enacted."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, md",
        "documentDescriptor": "Optional: Documentation showing a process for monitoring upcoming changes to the EU Eco Design Directive concerning server/storage idle power. Evidence of consideration for adapting IT procurement to meet potential future stricter requirements.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of any internal regulatory watch processes or documentation related to IT hardware environmental compliance. Interviews with IT procurement managers and legal/compliance personnel.",
        "nonconformity": {
          "major": null,
          "minor": "The organization does not have a process to track or prepare for potential future changes in EU Eco Design requirements for server/storage idle power, potentially risking non-compliance or missing opportunities to adopt more efficient hardware early (if practice of monitoring Section 11 items is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If active monitoring of CoC developments is claimed: Lack of a proactive approach to tracking and preparing for potential stricter EU Eco Design Directive requirements on server and storage idle state power, thereby not future-proofing IT procurement strategies (related to Practice 4.1.10).",
        "whatThisMeans": "The EU Eco Design Directive sets energy efficiency rules for products, including servers and storage (Lot 9). These rules, especially those concerning how much power equipment uses when idle, might become stricter in the future (as noted in CoC Section 11.2). Your organization needs to keep an eye on these potential changes and be ready to update your IT purchasing requirements to ensure any new or tougher idle power limits are met. This builds on your existing efforts to buy Eco Design compliant hardware (Practice 4.1.10).",
        "complianceRequires": "- Maintain awareness that 'EU Eco Design Directive idle state power' for servers and data storage products is an area subject to potential future revisions and tightening of requirements.\n- Actively monitor communications and proposals from the European Commission regarding updates to relevant Eco Design regulations (e.g., Lot 9 for servers and data storage, or subsequent regulatory packages).\n- Establish a process to evaluate the impact of any announced or anticipated changes on current IT equipment procurement strategies (which should already align with Practice 4.1.10).\n- Ensure preparedness to adopt and incorporate any new or stricter mandatory idle power consumption limits into procurement specifications for servers and storage systems in a timely manner.",
        "expertTip": "Subscribe to official EU regulatory update services and engage with IT hardware vendors about their product roadmaps concerning compliance with current and anticipated Eco Design requirements, particularly for idle power. Low idle power is critical as many servers have significant periods of low utilization.",
        "recommendedEvidence": [
          "- Documentation of a process for monitoring regulatory changes related to the EU Eco Design Directive for IT equipment.\n- Internal communications (e.g., briefings, newsletters) or reports discussing potential future changes to idle power requirements.\n- IT procurement policy or strategy documents that acknowledge the need to adapt to evolving Eco Design standards.\n- Records of discussions with IT vendors regarding future Eco Design compliance, especially for idle state power."
        ],
        "recommendedActions": [
          "- Assign responsibility within the organization (e.g., procurement, legal, IT strategy) for tracking updates to the EU Eco Design Directive relevant to data centre IT equipment.\n- Establish a mechanism for reviewing proposed changes and assessing their impact on IT procurement and operations.\n- Develop a plan for timely adoption of new Eco Design requirements into procurement specifications once they are enacted.\n- Continuously emphasize the importance of low idle power consumption when selecting new servers and storage systems."
        ],
        "sampleResponseTemplate": "Our [Legal/Compliance/IT Procurement Team] monitors updates to the EU Eco Design Directive, including potential changes to idle state power requirements for servers and storage (CoC Section 11.2), via [Source, e.g., 'official EU publications', 'industry association briefings']. We have a process to review such updates [annually/as they arise] and incorporate necessary changes into our IT Hardware Procurement Standard ([Doc ID]), which already mandates current Eco Design compliance (Practice 4.1.10).",
        "humanInputAnswer": "Our 'Regulatory Compliance Watch' team actively monitors updates to the EU Eco Design Directive, specifically tracking Lot 9 developments for server and storage idle state power. Any significant proposed changes are reviewed by our IT Hardware Standards Committee, which then updates our 'Global IT Procurement Policy (GIPP-001)' accordingly to ensure future compliance. This process ensures we are prepared beyond current requirements (Practice 4.1.10).",
        "llmGeneratedAnswer": "Yes, our organization is aware of and monitors considerations regarding 'EU Eco Design Directive idle state power' for servers and storage, as highlighted in Section 11.2 of the Code of Conduct. Our [Procurement and Sustainability Department] subscribes to regulatory update services and actively tracks proposals for revisions to Lot 9 and related Eco Design measures. An internal 'Eco Design Impact Assessment' (Ref: 'EDIA_ServersStorage_2025.pdf') is conducted annually to evaluate how potential stricter idle power limits would affect our IT equipment procurement specifications (currently aligned with Practice 4.1.10) and to ensure our vendor engagement strategy includes these forward-looking considerations.",
        "badExampleAnswer": "We just buy Lot 9 compliant servers when the vendors tell us that's the current rule. We don't look into what might change in the future."
      },
      "question_170": {
        "id": 170,
        "old_question_text": null,
        "questionText": "Is the organization aware of and actively monitoring developments related to the 'Use of Hydrogen or other alternative energy sources and storage' for data centre power, as noted in CoC Section 11.3? Does this include evaluating the potential future viability, environmental impact (including sourcing of hydrogen - e.g., green, blue, grey), and technical feasibility of such technologies (e.g., hydrogen fuel cells for primary or backup power, other novel energy storage like flow batteries or kinetic storage) for its data centre portfolio, even if widespread adoption is not yet current practice? Is there an internal process for assessing these emerging technologies against criteria such as reliability, scalability, cost, and sustainability for potential future adoption?",
        "simplifiedQuestion": "Are you monitoring developments and evaluating the future potential of using hydrogen or other novel alternative energy sources and advanced storage technologies for your data centres (CoC Section 11.3)?",
        "reference": "2025 CoC Guide 11.3",
        "priority": "Low (Value: Not specified in guide table, Expected: Not specified - Implied future consideration and awareness)",
        "stepSection": "step_20",
        "recommendationTitle": "Use of Hydrogen or other alternative energy sources and storage",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "md",
            "pptx"
          ],
          "description": "Internal research papers, technology watch reports, feasibility assessments, or strategy documents discussing the potential of hydrogen or other novel alternative energy sources/storage for data centres. Records of participation in industry forums or pilot projects related to these technologies, including evaluation criteria."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, md, pptx",
        "documentDescriptor": "Optional: Documentation showing that the organization is actively monitoring, evaluating, and strategizing around the potential of hydrogen fuel cells, advanced energy storage, or other emerging alternative energy sources for data centre applications, including lifecycle environmental impact considerations.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of any internal research, strategy documents, or technology tracking related to hydrogen and alternative energy/storage for data centres. Interviews with personnel responsible for long-term technology strategy, R&D, or sustainability.",
        "nonconformity": {
          "major": null,
          "minor": "The organization is not actively monitoring or evaluating emerging alternative energy sources like hydrogen or advanced energy storage for potential future data centre application, despite its listing as an item under consideration and relevance to long-term sustainability goals (if practice of monitoring Section 11 items is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If active monitoring of CoC developments is claimed: No evidence of systematic tracking, research, internal discussion, or strategic evaluation regarding the potential future use of hydrogen (considering its production source) or other novel alternative energy sources and storage solutions for data centres, including their environmental and technical feasibility.",
        "whatThisMeans": "The EU Code of Conduct (Section 11.3) indicates that using hydrogen and other new alternative energy sources or advanced storage methods for data centres is an important area of ongoing development. Your organization should be actively aware of and monitoring these advancements. This includes assessing the future practicality, environmental implications (e.g., whether hydrogen is 'green', 'blue', or 'grey'), and technical aspects of technologies like hydrogen fuel cells (for main or backup power) or new types of energy storage (e.g., flow batteries, kinetic storage). Even if these aren't mainstream yet, considering them for your future data centre plans is recommended.",
        "complianceRequires": "- Maintain awareness that 'Use of Hydrogen or other alternative energy sources and storage' is an item under active consideration and development (CoC Section 11.3).\n- Actively monitor advancements in relevant technologies, such as:\n    - Hydrogen fuel cells (considering hydrogen sourcing - green, blue, grey - for primary, backup, or supplementary power).\n    - Innovative energy storage solutions beyond traditional batteries (e.g., flow batteries, kinetic storage, compressed air energy storage).\n    - Other emerging alternative energy sources suitable for data centre applications.\n- Evaluate the potential long-term viability, full lifecycle environmental impact, technical feasibility, scalability, reliability, and economic implications of adopting such technologies within the data centre portfolio.\n- Consider establishing internal criteria and processes for assessing these emerging technologies for potential future adoption.",
        "expertTip": "When evaluating hydrogen, focus on the end-to-end carbon intensity based on its production method ('green' hydrogen from renewables is preferred). For energy storage, assess not just capacity but also round-trip efficiency, degradation over time, and suitability for data centre load profiles (e.g., long-duration vs. short-duration power backup/grid services).",
        "recommendedEvidence": [
          "- Internal technology watch reports, research summaries, or presentations discussing hydrogen and other alternative energy/storage technologies for data centres, including lifecycle assessments.\n- Documented criteria for evaluating emerging energy technologies.\n- Records of attendance at industry conferences, webinars, or working groups focused on these topics.\n- Preliminary feasibility assessments or white papers exploring the potential application of these technologies within the organization's data centre strategy.\n- If involved in pilot projects for such technologies, provide relevant documentation."
        ],
        "recommendedActions": [
          "- Assign responsibility for monitoring technological advancements and market readiness of hydrogen and other alternative energy sources and storage solutions relevant to data centres.\n- Develop a framework for evaluating emerging energy technologies based on technical, economic, environmental, and reliability criteria.\n- Periodically review and discuss the potential of these technologies in strategic planning sessions for future data centre energy resilience and sustainability.\n- Engage with technology vendors, research institutions, and industry consortia to stay informed about developments and potential pilot opportunities."
        ],
        "sampleResponseTemplate": "Our [Innovation/Sustainability/R&D Team] actively monitors developments in alternative energy, including green hydrogen fuel cells and advanced energy storage solutions like [e.g., flow batteries] (CoC Section 11.3), as part of our '[Future Tech Roadmap ID]'. We assess these against criteria including [e.g., TCO, carbon impact, scalability, reliability]. We are currently tracking [X specific research/pilot programs] and have a [Year] target to conduct a feasibility study for [Specific Application, e.g., 'hydrogen fuel cells for backup at edge sites'].",
        "humanInputAnswer": "Our 'Advanced Energy Research Group' tracks developments in hydrogen (especially green hydrogen pathways) and next-generation storage technologies like iron-air batteries, as per CoC 11.3. We have an internal 'Emerging Energy Tech Assessment Framework' (EETF-001) used to evaluate these for future DC application. While widespread deployment is not yet viable for us, we presented a paper on 'Hydrogen for DC Peaking Power' to our strategy board in Q1 2025, and we are a member of the 'Hydrogen for Data Centres Alliance'.",
        "llmGeneratedAnswer": "Yes, our organization's [e.g., Technology Scouting Team/Chief Sustainability Office] is tasked with actively monitoring and evaluating the 'Use of Hydrogen or other alternative energy sources and storage' for data centres, as outlined in CoC Section 11.3. Our 'Future Energy Systems Watchlist (FESW-2025.Q1)' details ongoing assessments of green hydrogen fuel cells for baseload and backup power, alongside emerging long-duration storage technologies such as flow batteries. Evaluations include technical feasibility, lifecycle carbon impact (prioritizing green hydrogen), scalability, and projected economic viability. While no large-scale implementations are current, this research informs our long-term energy strategy and readiness for adopting proven, sustainable alternatives beyond existing solutions.",
        "badExampleAnswer": "We use diesel for backup and lithium-ion for UPS. We're not looking at hydrogen or other things as they seem too experimental and not ready for data centres."
      },
      "question_171": {
        "id": 171,
        "old_question_text": null,
        "questionText": "Is the organization aware that 'Direct Liquid Cooling (DLC)' is an area being actively investigated for broader use cases and is monitoring advancements in this field, as noted in CoC Section 11.4? Does this involve tracking developments in various DLC technologies (e.g., direct-to-chip, immersion - single-phase/two-phase), supporting infrastructure requirements, standardization efforts, and potential for wider integration into data centre designs to manage increasing IT heat densities and enhance energy efficiency and heat reuse opportunities (complementing existing Practice 5.6.1 on current DLC use)?",
        "simplifiedQuestion": "Are you actively monitoring advancements and broader applications of Direct Liquid Cooling (DLC) for future data centre designs, beyond any current use (CoC Section 11.4)?",
        "reference": "2025 CoC Guide 11.4",
        "priority": "Low (Value: Not specified, Expected: Not specified - Implied future consideration and awareness)",
        "stepSection": "step_20",
        "recommendationTitle": "Direct Liquid Cooling",
        "document_requirements": {
          "required": false,
          "accepted_types": [
            "pdf",
            "docx",
            "md",
            "pptx"
          ],
          "description": "Internal research, technology watch reports, or strategy documents discussing advancements in DLC, broader use cases, standardization efforts, and potential for future widespread integration. Notes on how current DLC practices (5.6.1) might evolve."
        },
        "docRequired": "No",
        "acceptedFiles": "pdf, docx, md, pptx",
        "documentDescriptor": "Optional: Documentation indicating active monitoring of Direct Liquid Cooling advancements, standardization, and potential for broader future applications in data centres, beyond specific current implementations outlined in Practice 5.6.1.",
        "verification_method": [
          "Document Review",
          "Interview"
        ],
        "verificationMethods": "Review of internal technology roadmaps, research on advanced cooling, or strategy documents. Interviews with data centre architects, cooling specialists, and innovation teams.",
        "nonconformity": {
          "major": null,
          "minor": "The organization is not actively monitoring or evaluating advancements in Direct Liquid Cooling for potential broader future application or for managing next-generation high-density IT, despite its listing as an item under active investigation (if practice of monitoring Section 11 items is adopted)."
        },
        "majorNonconformity": null,
        "minorNonconformity": "If active monitoring of CoC developments is claimed: No evidence of tracking, research, or internal discussion regarding the evolution of Direct Liquid Cooling technologies (e.g., immersion, advanced direct-to-chip) and their potential for wider deployment or for addressing future ultra-high IT densities, beyond any current specific applications (Practice 5.6.1).",
        "whatThisMeans": "The EU Code of Conduct (Section 11.4) emphasizes that Direct Liquid Cooling (DLC) is being actively explored for much wider use than its current, often specialized, applications. While Practice 5.6.1 covers what you might already be doing with DLC, this item means your organization should be keeping up with new developments in DLC technologies (like different types of direct-to-chip or immersion cooling), how they integrate with data centre infrastructure, and any new standards. This is important for planning how you might use DLC more broadly in future data centres to cool very high-power IT gear, improve energy efficiency, and better enable heat reuse.",
        "complianceRequires": "- Maintain awareness that 'Direct Liquid Cooling (DLC)' is an area of active investigation by organizations for broader use cases beyond current deployments (as also covered by Practice 5.6.1).\n- Actively monitor advancements in various DLC technologies (e.g., direct-to-chip cold plates, single-phase and two-phase immersion cooling), their supporting infrastructure requirements (e.g., Coolant Distribution Units, heat rejection systems), and relevant standardization efforts.\n- Evaluate the potential for wider integration of DLC into future data centre designs or retrofits, especially for managing increasing IT equipment heat densities and further enhancing energy efficiency and heat reuse capabilities.\n- Understand that information and best practices for DLC are expected to evolve, potentially leading to expanded guidance in future Code of Conduct revisions.",
        "expertTip": "Engage with industry consortia and research groups focused on advanced cooling and liquid cooling technologies (e.g., Open Compute Project - OCP, ASHRAE TC9.9). Consider establishing a small testbed or lab environment to evaluate emerging DLC solutions and gain hands-on experience with their operational aspects before considering larger-scale deployments.",
        "recommendedEvidence": [
          "- Internal technology watch reports or presentations specifically focusing on advancements in Direct Liquid Cooling technologies, their TCO, and integration challenges/opportunities for broader deployment.\n- Records of attendance at industry events, working groups, or collaborations related to liquid cooling.\n- Preliminary feasibility assessments or white papers exploring the potential for expanded use of DLC within the organization's data centre portfolio.\n- If involved in pilot projects for new or broader DLC applications, provide relevant documentation."
        ],
        "recommendedActions": [
          "- Assign responsibility for tracking and evaluating advancements in Direct Liquid Cooling technologies and standards.\n- Foster internal expertise or seek external consultancy on DLC design, implementation, and operation.\n- When planning for future IT deployments with very high power densities (e.g., AI/ML clusters, next-gen CPUs/GPUs), proactively evaluate various DLC options as primary cooling solutions.\n- Consider developing a phased strategy for DLC adoption, starting with pilot projects and gradually expanding as the technology matures and business needs dictate."
        ],
        "sampleResponseTemplate": "Our [Data Centre Design Team / Emerging Technologies Group] actively monitors advancements in Direct Liquid Cooling (CoC Section 11.4), including [e.g., 'two-phase immersion solutions', 'standardized manifold designs for direct-to-chip']. We are currently [e.g., 'evaluating DLC for our next-gen GPU platform design / participating in an OCP liquid cooling workstream']. This complements our existing use of DLC for [specific application mentioned in 5.6.1], with a view to broader adoption where TCO and density requirements justify it.",
        "humanInputAnswer": "Our 'Advanced Cooling Solutions Taskforce' tracks developments in Direct Liquid Cooling as per CoC 11.4, focusing on broader applications beyond our current HPC DLC (Practice 5.6.1). We are currently assessing next-generation single-phase and two-phase immersion cooling technologies for potential future high-density general compute deployments. A report on 'DLC Future Roadmap' (Ref: ACST_DLC_Roadmap_2025.pdf') was presented to our IT strategy committee in Q1 2025.",
        "llmGeneratedAnswer": "Yes, our organization is keenly aware that Direct Liquid Cooling (DLC) is an area of active investigation for broader use, as highlighted in CoC Section 11.4. Beyond our current specific implementations (Practice 5.6.1), our [Data Centre Innovation Team/R&D Department] maintains an ongoing assessment of evolving DLC technologies, including full immersion and advanced direct-to-chip solutions. This involves tracking industry standardization efforts (e.g., OCP, ASHRAE) and evaluating the potential for wider integration into our standard data centre designs, particularly for future generations of high-TDP IT equipment. Our 'Next-Gen DC Cooling Roadmap (NGCR-2025)' includes milestones for piloting and potentially adopting more pervasive DLC strategies.",
        "badExampleAnswer": "We use liquid cooling where we absolutely have to for some HPC gear, but we don't see it becoming mainstream for general servers anytime soon."
      }
    }
  }
}